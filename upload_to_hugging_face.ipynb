{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4816a3e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Iskandar\\Documents\\GitHub\\Tubes-NLP-PaperMatch\\hf_env\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mempersiapkan upload model ke: iskandarmrp/distilbert-lora-paper-topic-classification\n",
      "Repo Model siap.\n",
      "Mengupload adapter LoRA dari './hf_finetune_results_lora_baru/distilbert/lora_model'...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "adapter_model.safetensors: 100%|██████████| 3.61M/3.61M [00:02<00:00, 1.41MB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Upload Model BERHASIL!\n",
      "Lihat model Anda di sini: https://huggingface.co/iskandarmrp/distilbert-lora-paper-topic-classification\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from huggingface_hub import HfApi, create_repo\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "HF_TOKEN = os.getenv(\"HF_TOKEN\")\n",
    "\n",
    "MODEL_REPO_NAME = \"distilbert-lora-topic-classification\"\n",
    "FULL_MODEL_ID = f\"iskandarmrp/distilbert-lora-paper-topic-classification\"\n",
    "\n",
    "MODEL_PATH = \"./hf_finetune_results_lora_baru/distilbert/lora_model\"\n",
    "REPO_TYPE = \"model\"\n",
    "\n",
    "def upload_model():\n",
    "    if not HF_TOKEN:\n",
    "        print(\"Error: HF_TOKEN tidak ditemukan di .env\")\n",
    "        return\n",
    "\n",
    "    print(f\"Mempersiapkan upload model ke: {FULL_MODEL_ID}\")\n",
    "    \n",
    "    api = HfApi(token=HF_TOKEN)\n",
    "\n",
    "    if not os.path.exists(MODEL_PATH):\n",
    "        print(f\"Error: Folder model tidak ditemukan di '{MODEL_PATH}'\")\n",
    "        print(\"   Pastikan Anda sudah menjalankan training dan folder tersebut ada.\")\n",
    "        return\n",
    "\n",
    "    try:\n",
    "        create_repo(\n",
    "            repo_id=FULL_MODEL_ID, \n",
    "            token=HF_TOKEN, \n",
    "            repo_type=REPO_TYPE,\n",
    "            exist_ok=True,\n",
    "            private=False \n",
    "        )\n",
    "        print(\"Repo Model siap.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Info Repo: {e}\")\n",
    "\n",
    "    print(f\"Mengupload adapter LoRA dari '{MODEL_PATH}'...\")\n",
    "    \n",
    "    try:\n",
    "        api.upload_folder(\n",
    "            folder_path=MODEL_PATH,\n",
    "            repo_id=FULL_MODEL_ID,\n",
    "            repo_type=REPO_TYPE,\n",
    "            path_in_repo=\".\", \n",
    "            ignore_patterns=[\".git\", \"__pycache__\", \"checkpoint-*\"]\n",
    "        )\n",
    "        print(\"Upload Model BERHASIL!\")\n",
    "        print(f\"Lihat model Anda di sini: https://huggingface.co/{FULL_MODEL_ID}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Upload Gagal: {e}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    upload_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "eff939af",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Iskandar\\Documents\\GitHub\\Tubes-NLP-PaperMatch\\hf_env\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mempersiapkan upload model ke: iskandarmrp/llama-3.2-1b-related-works-generation\n",
      "Repo Model siap.\n",
      "Mengupload model dari 'summarization/related_works_generation_model'...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Upload 2 LFS files:   0%|          | 0/2 [00:00<?, ?it/s]\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "tokenizer.json: 100%|██████████| 17.2M/17.2M [00:06<00:00, 2.77MB/s]\n",
      "model.safetensors: 100%|██████████| 1.55G/1.55G [03:18<00:00, 7.83MB/s]\n",
      "Upload 2 LFS files: 100%|██████████| 2/2 [03:19<00:00, 99.52s/it] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Upload Model BERHASIL!\n",
      "Lihat model Anda di sini: https://huggingface.co/iskandarmrp/llama-3.2-1b-related-works-generation\n",
      "\n",
      "CARA PAKAI (INFERENCE):\n",
      "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
      "tokenizer = AutoTokenizer.from_pretrained('iskandarmrp/llama-3.2-1b-related-works-generation')\n",
      "model = AutoModelForCausalLM.from_pretrained('iskandarmrp/llama-3.2-1b-related-works-generation', device_map='auto')\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from huggingface_hub import HfApi, create_repo\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "HF_TOKEN = os.getenv(\"HF_TOKEN\")\n",
    "USERNAME = \"iskandarmrp\"\n",
    "\n",
    "MODEL_REPO_NAME = \"llama-3.2-1b-related-works-generation\"\n",
    "FULL_MODEL_ID = f\"{USERNAME}/{MODEL_REPO_NAME}\"\n",
    "\n",
    "MODEL_PATH = \"summarization/related_works_generation_model\" \n",
    "REPO_TYPE = \"model\"\n",
    "\n",
    "def upload_model():\n",
    "    if not HF_TOKEN:\n",
    "        print(\"Error: HF_TOKEN tidak ditemukan di .env\")\n",
    "        return\n",
    "\n",
    "    print(f\"Mempersiapkan upload model ke: {FULL_MODEL_ID}\")\n",
    "    \n",
    "    api = HfApi(token=HF_TOKEN)\n",
    "\n",
    "    if not os.path.exists(MODEL_PATH):\n",
    "        print(f\"Error: Folder model tidak ditemukan di '{MODEL_PATH}'\")\n",
    "        print(\"   Pastikan proses 'save_pretrained' di kode training sudah selesai.\")\n",
    "        return\n",
    "\n",
    "    try:\n",
    "        create_repo(\n",
    "            repo_id=FULL_MODEL_ID, \n",
    "            token=HF_TOKEN, \n",
    "            repo_type=REPO_TYPE,\n",
    "            exist_ok=True,\n",
    "            private=False \n",
    "        )\n",
    "        print(\"Repo Model siap.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Info Repo: {e}\")\n",
    "\n",
    "    print(f\"Mengupload model dari '{MODEL_PATH}'...\")\n",
    "    \n",
    "    try:\n",
    "        api.upload_folder(\n",
    "            folder_path=MODEL_PATH,\n",
    "            repo_id=FULL_MODEL_ID,\n",
    "            repo_type=REPO_TYPE,\n",
    "            path_in_repo=\".\", \n",
    "            ignore_patterns=[\".git\", \"__pycache__\", \"checkpoint-*\", \"*.ipynb_checkpoints\"]\n",
    "        )\n",
    "        print(\"Upload Model BERHASIL!\")\n",
    "        print(f\"Lihat model Anda di sini: https://huggingface.co/{FULL_MODEL_ID}\")\n",
    "        \n",
    "        print(\"\\nCARA PAKAI (INFERENCE):\")\n",
    "        print(f\"from transformers import AutoTokenizer, AutoModelForCausalLM\")\n",
    "        print(f\"tokenizer = AutoTokenizer.from_pretrained('{FULL_MODEL_ID}')\")\n",
    "        print(f\"model = AutoModelForCausalLM.from_pretrained('{FULL_MODEL_ID}', device_map='auto')\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Upload Gagal: {e}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    upload_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "80a64d4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Iskandar\\Documents\\GitHub\\Tubes-NLP-PaperMatch\\hf_env\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Repo 'iskandarmrp/nlp-papermatch-dataset' siap.\n",
      "\n",
      "Mengupload './train_raw_dataset' ke folder 'raw_data/train'...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Upload 0 LFS files: 0it [00:00, ?it/s]\n",
      "No files have been modified since last commit. Skipping to prevent empty commit.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sukses upload raw_data/train!\n",
      "\n",
      "Mengupload './test_raw_dataset' ke folder 'raw_data/test'...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Upload 0 LFS files: 0it [00:00, ?it/s]\n",
      "No files have been modified since last commit. Skipping to prevent empty commit.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sukses upload raw_data/test!\n",
      "\n",
      "Mengupload './topic_classification/train_dataset' ke folder 'topic_classification/train'...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Upload 0 LFS files: 0it [00:00, ?it/s]\n",
      "No files have been modified since last commit. Skipping to prevent empty commit.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sukses upload topic_classification/train!\n",
      "\n",
      "Mengupload './topic_classification/test_dataset' ke folder 'topic_classification/test'...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Upload 0 LFS files: 0it [00:00, ?it/s]\n",
      "No files have been modified since last commit. Skipping to prevent empty commit.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sukses upload topic_classification/test!\n",
      "\n",
      "Mengupload './processed_multixscience_data' ke folder 'processed_multixscience_data'...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Upload 0 LFS files: 0it [00:00, ?it/s]\n",
      "No files have been modified since last commit. Skipping to prevent empty commit.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sukses upload processed_multixscience_data!\n",
      "\n",
      "Mengupload './chroma_db' ke folder 'chroma_db'...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "data_level0.bin:   0%|          | 0.00/144M [00:00<?, ?B/s]\n",
      "\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "data_level0.bin:   0%|          | 164k/144M [00:00<01:32, 1.55MB/s]\n",
      "\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "data_level0.bin:   0%|          | 639k/144M [00:00<00:49, 2.92MB/s]\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "data_level0.bin:   1%|          | 934k/144M [00:00<02:23, 999kB/s] \n",
      "data_level0.bin:   1%|          | 1.11M/144M [00:00<02:10, 1.10MB/s]\n",
      "data_level0.bin:   1%|          | 1.43M/144M [00:01<01:41, 1.41MB/s]\n",
      "data_level0.bin:   1%|          | 1.74M/144M [00:01<01:21, 1.74MB/s]\n",
      "header.bin: 100%|██████████| 100/100 [00:01<00:00, 82.7B/s]\n",
      "data_level0.bin:   1%|▏         | 2.15M/144M [00:01<01:05, 2.18MB/s]\n",
      "data_level0.bin:   2%|▏         | 2.51M/144M [00:01<00:56, 2.50MB/s]\n",
      "data_level0.bin:   2%|▏         | 3.18M/144M [00:01<00:55, 2.52MB/s]\n",
      "\u001b[A\n",
      "length.bin: 100%|██████████| 344k/344k [00:01<00:00, 181kB/s] 8MB/s]\n",
      "link_lists.bin: 100%|██████████| 735k/735k [00:02<00:00, 353kB/s] s]\n",
      "index_metadata.pickle: 100%|██████████| 7.91M/7.91M [00:03<00:00, 2.41MB/s]\n",
      "data_level0.bin: 100%|██████████| 144M/144M [00:32<00:00, 4.47MB/s] \n",
      "\n",
      "\n",
      "\n",
      "chroma.sqlite3: 100%|██████████| 480M/480M [01:17<00:00, 6.16MB/s]\n",
      "\n",
      "\n",
      "\n",
      "Upload 6 LFS files: 100%|██████████| 6/6 [01:20<00:00, 13.39s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sukses upload chroma_db!\n",
      "\n",
      "Semua proses selesai! Cek di: https://huggingface.co/datasets/iskandarmrp/nlp-papermatch-dataset/tree/main\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from huggingface_hub import HfApi, create_repo\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "HF_TOKEN = os.getenv(\"HF_TOKEN\")\n",
    "REPO_ID = \"iskandarmrp/nlp-papermatch-dataset\"\n",
    "REPO_TYPE = \"dataset\"\n",
    "\n",
    "uploads = [\n",
    "    (\"./train_raw_dataset\", \"raw_data/train\"),\n",
    "    (\"./test_raw_dataset\", \"raw_data/test\"),\n",
    "    (\"./topic_classification/train_dataset\", \"topic_classification/train\"),\n",
    "    (\"./topic_classification/test_dataset\", \"topic_classification/test\"),\n",
    "    (\"./processed_multixscience_data\", \"processed_multixscience_data\"),\n",
    "    (\"./chroma_db\", \"chroma_db\")\n",
    "]\n",
    "\n",
    "def upload_datasets():\n",
    "    if not HF_TOKEN or not REPO_ID:\n",
    "        print(\"Error: Pastikan HF_TOKEN dan REPO_ID ada\")\n",
    "        return\n",
    "\n",
    "    api = HfApi(token=HF_TOKEN)\n",
    "\n",
    "    try:\n",
    "        create_repo(\n",
    "            repo_id=REPO_ID, \n",
    "            token=HF_TOKEN, \n",
    "            repo_type=REPO_TYPE,\n",
    "            exist_ok=True,\n",
    "            private=False \n",
    "        )\n",
    "        print(f\"Repo '{REPO_ID}' siap.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Info Repo: {e}\")\n",
    "\n",
    "    for local_path, target_path in uploads:\n",
    "        if not os.path.exists(local_path):\n",
    "            print(f\"Folder lokal tidak ditemukan: {local_path} (Dilewati)\")\n",
    "            continue\n",
    "\n",
    "        print(f\"\\nMengupload '{local_path}' ke folder '{target_path}'...\")\n",
    "        \n",
    "        try:\n",
    "            api.upload_folder(\n",
    "                folder_path=local_path,\n",
    "                repo_id=REPO_ID,\n",
    "                repo_type=REPO_TYPE,\n",
    "                path_in_repo=target_path,\n",
    "                ignore_patterns=[\".git\", \"__pycache__\", \".DS_Store\", \"*.lock\"]\n",
    "            )\n",
    "            print(f\"Sukses upload {target_path}!\")\n",
    "        except Exception as e:\n",
    "            print(f\"Gagal upload {target_path}: {e}\")\n",
    "\n",
    "    print(f\"\\nSemua proses selesai! Cek di: https://huggingface.co/datasets/{REPO_ID}/tree/main\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    upload_datasets()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d931023d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hf_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
