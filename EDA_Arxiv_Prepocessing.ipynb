{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "252b4a37",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Iskandar\\Documents\\GitHub\\Tubes-NLP-PaperMatch\\hf_env\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "# Login using e.g. `huggingface-cli login` to access this dataset\n",
    "ds = load_dataset(\"kilian-group/arxiv-classifier\", \"default\")\n",
    "\n",
    "df = ds[\"train\"].to_pandas()\n",
    "\n",
    "df_clean = df.dropna(subset=['abstract', 'title']).copy()\n",
    "df_clean = df_clean[df_clean['abstract'].apply(len) > 50] # Hapus abstrak terlalu pendek"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bb5db442",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Iskandar\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\Iskandar\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\Iskandar\\AppData\\Roaming\\nltk_data...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. Melakukan Split Data (Stratified)...\n",
      "   Data Train: 86941\n",
      "   Data Val  : 21736\n",
      "\n",
      "2. Memproses Data TRAIN...\n",
      "   > Membersihkan teks...\n",
      "3. Memproses Data VAL...\n",
      "   > Membersihkan teks...\n",
      "\n",
      "4. Menyiapkan Label & Bobot...\n",
      "\n",
      "✅ PIPELINE SELESAI!\n",
      "Label Mapping: {'astro-ph': np.int64(0), 'cond-mat': np.int64(1), 'cs': np.int64(2), 'econ': np.int64(3), 'eess': np.int64(4), 'gr-qc': np.int64(5), 'hep-ex': np.int64(6), 'hep-lat': np.int64(7), 'hep-ph': np.int64(8), 'hep-th': np.int64(9), 'math': np.int64(10), 'math-ph': np.int64(11), 'nlin': np.int64(12), 'nucl-ex': np.int64(13), 'nucl-th': np.int64(14), 'physics': np.int64(15), 'q-bio': np.int64(16), 'q-fin': np.int64(17), 'quant-ph': np.int64(18), 'stat': np.int64(19)}\n",
      "\n",
      "Contoh Class Weights (Train Data):\n",
      "  - astro-ph: 0.9725\n",
      "  - cond-mat: 0.7554\n",
      "  - cs: 0.1949\n",
      "  - econ: 12.6736\n",
      "  - eess: 1.8128\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# --- 1. SETUP & RESOURCE ---\n",
    "# Download resource NLTK (Hanya jika belum ada)\n",
    "try:\n",
    "    nltk.data.find('corpora/stopwords')\n",
    "    nltk.data.find('corpora/wordnet')\n",
    "except LookupError:\n",
    "    nltk.download('stopwords')\n",
    "    nltk.download('wordnet')\n",
    "    nltk.download('omw-1.4')\n",
    "\n",
    "# Inisialisasi tools cleaning\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "custom_stopwords = set([\n",
    "    'based', 'proposed', 'using', 'paper', 'data', 'results', 'method', \n",
    "    'model', 'approach', 'analysis', 'study', 'performance', 'new', \n",
    "    'presented', 'show', 'demonstrate', 'investigate'\n",
    "])\n",
    "stop_words = set(stopwords.words('english')) | custom_stopwords\n",
    "\n",
    "# --- 2. DEFINISI FUNGSI ---\n",
    "\n",
    "def clean_text_classifier(text):\n",
    "    \"\"\"\n",
    "    Fungsi cleaning khusus untuk Classifier (Router).\n",
    "    Fokus: Hapus noise, pertahankan sinyal matematika ([EQ]).\n",
    "    \"\"\"\n",
    "    if not isinstance(text, str): return \"\"\n",
    "    \n",
    "    # 1. Ganti rumus LaTeX ($...$) dengan token khusus [EQ]\n",
    "    text = re.sub(r'\\$.*?\\$', ' [EQ] ', text)\n",
    "    \n",
    "    # 2. Lowercase\n",
    "    text = text.lower()\n",
    "    \n",
    "    # 3. Hapus karakter non-alfabet (kecuali token [EQ] dan spasi)\n",
    "    text = re.sub(r'[^a-z\\s\\[\\]]', ' ', text)\n",
    "    \n",
    "    # 4. Tokenisasi, Stopwords, & Lemmatization\n",
    "    words = text.split()\n",
    "    cleaned_words = [\n",
    "        lemmatizer.lemmatize(w) \n",
    "        for w in words \n",
    "        if w not in stop_words and len(w) > 2\n",
    "    ]\n",
    "    \n",
    "    return \" \".join(cleaned_words)\n",
    "\n",
    "def preprocess_batch(df):\n",
    "    \"\"\"\n",
    "    Wrapper untuk menerapkan preprocessing ke dataframe (Train atau Test).\n",
    "    \"\"\"\n",
    "    df = df.copy()\n",
    "    \n",
    "    # A. FEATURE ENGINEERING (Pada Raw Text)\n",
    "    # Hitung fitur sebelum teks dibersihkan/hilang\n",
    "    df['num_latex'] = df['abstract'].astype(str).apply(lambda x: x.count('$'))\n",
    "    df['text_len'] = df['abstract'].astype(str).apply(len)\n",
    "    \n",
    "    # B. TEXT CLEANING\n",
    "    print(\"   > Membersihkan teks...\")\n",
    "    df['clean_abstract'] = df['abstract'].astype(str).apply(clean_text_classifier)\n",
    "    \n",
    "    # C. FITUR GABUNGAN (Text + Hand-crafted Features)\n",
    "    # Menambahkan sinyal 'high_math_density' jika banyak rumus\n",
    "    df['final_text'] = df['clean_abstract'] + \\\n",
    "                       np.where(df['num_latex'] > 5, ' high_math_density', '')\n",
    "    \n",
    "    return df\n",
    "\n",
    "# --- 3. EKSEKUSI PIPELINE (SPLIT DULU!) ---\n",
    "\n",
    "# Asumsi: df_clean adalah dataframe awal Anda yang sudah membuang baris null\n",
    "# df_clean = ... (Load data Anda di sini)\n",
    "\n",
    "print(\"1. Melakukan Split Data (Stratified)...\")\n",
    "# Kita split RAW data dulu untuk mencegah kebocoran informasi\n",
    "X_raw = df_clean.drop(columns=['field']) # Fitur mentah\n",
    "y_raw = df_clean['field']               # Label mentah\n",
    "\n",
    "X_train_raw, X_val_raw, y_train_raw, y_val_raw = train_test_split(\n",
    "    X_raw, \n",
    "    y_raw,\n",
    "    test_size=0.2, \n",
    "    stratify=y_raw, # Menjaga proporsi kelas\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "print(f\"   Data Train: {len(X_train_raw)}\")\n",
    "print(f\"   Data Val  : {len(X_val_raw)}\")\n",
    "\n",
    "# --- 4. TERAPKAN PREPROCESSING TERPISAH ---\n",
    "\n",
    "print(\"\\n2. Memproses Data TRAIN...\")\n",
    "X_train_processed = preprocess_batch(X_train_raw)\n",
    "\n",
    "print(\"3. Memproses Data VAL...\")\n",
    "X_val_processed = preprocess_batch(X_val_raw)\n",
    "\n",
    "# --- 5. LABEL ENCODING & CLASS WEIGHTS ---\n",
    "\n",
    "print(\"\\n4. Menyiapkan Label & Bobot...\")\n",
    "le = LabelEncoder()\n",
    "\n",
    "# Fit hanya pada TRAIN, lalu transform ke keduanya\n",
    "y_train_enc = le.fit_transform(y_train_raw)\n",
    "y_val_enc = le.transform(y_val_raw) # Gunakan mapping yang sama dengan train\n",
    "\n",
    "# Simpan mapping\n",
    "label_mapping = dict(zip(le.classes_, le.transform(le.classes_)))\n",
    "\n",
    "# Hitung Class Weights (Hanya dari data TRAIN)\n",
    "class_weights = compute_class_weight(\n",
    "    class_weight='balanced',\n",
    "    classes=np.unique(y_train_enc),\n",
    "    y=y_train_enc\n",
    ")\n",
    "class_weight_dict = dict(zip(np.unique(y_train_enc), class_weights))\n",
    "\n",
    "print(\"\\n✅ PIPELINE SELESAI!\")\n",
    "print(\"Label Mapping:\", label_mapping)\n",
    "print(\"\\nContoh Class Weights (Train Data):\")\n",
    "for label_name in list(label_mapping.keys())[:5]:\n",
    "    idx = label_mapping[label_name]\n",
    "    print(f\"  - {label_name}: {class_weight_dict[idx]:.4f}\")\n",
    "\n",
    "# --- OUTPUT DATA SIAP PAKAI ---\n",
    "# X_train_processed['final_text'] -> Input Teks untuk Model\n",
    "# X_train_processed['num_latex']  -> Input Numerik Tambahan (jika pakai arsitektur hybrid)\n",
    "# y_train_enc                     -> Target Label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "440a1d1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memori sudah dibersihkan.\n"
     ]
    }
   ],
   "source": [
    "import gc\n",
    "import pandas as pd\n",
    "\n",
    "try:\n",
    "    del df_train_raw\n",
    "    del df_test_raw\n",
    "    del dataset_train_raw\n",
    "    del dataset_test_raw\n",
    "except NameError:\n",
    "    pass\n",
    "\n",
    "gc.collect()\n",
    "\n",
    "print(\"Memori sudah dibersihkan.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "8523d16f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating train split: 86941 examples [00:19, 4403.71 examples/s]\n",
      "Generating test split: 21736 examples [00:03, 5682.99 examples/s]\n",
      "Saving the dataset (10/10 shards): 100%|██████████| 86941/86941 [00:13<00:00, 6634.73 examples/s]\n",
      "Saving the dataset (3/3 shards): 100%|██████████| 21736/21736 [00:05<00:00, 4002.57 examples/s]\n"
     ]
    }
   ],
   "source": [
    "from datasets import Dataset\n",
    "\n",
    "df_train_raw = pd.concat(\n",
    "    [\n",
    "        X_train_processed.reset_index(drop=True),\n",
    "        pd.Series(y_train_enc, name=\"label\")\n",
    "    ],\n",
    "    axis=1\n",
    ")\n",
    "\n",
    "df_test_raw = pd.concat(\n",
    "    [\n",
    "        X_val_processed.reset_index(drop=True),\n",
    "        pd.Series(y_val_enc, name=\"label\")\n",
    "    ],\n",
    "    axis=1\n",
    ")\n",
    "\n",
    "df_train_raw.to_parquet(\"train.parquet\", index=False)\n",
    "df_test_raw.to_parquet(\"test.parquet\", index=False)\n",
    "\n",
    "dataset_raw = load_dataset(\"parquet\", data_files={\n",
    "    \"train\": \"train.parquet\", \n",
    "    \"test\": \"test.parquet\"\n",
    "})\n",
    "\n",
    "dataset_train_raw = dataset_raw[\"train\"]\n",
    "dataset_test_raw = dataset_raw[\"test\"]\n",
    "\n",
    "dataset_train_raw.save_to_disk(\"./train_raw_dataset\")\n",
    "dataset_test_raw.save_to_disk(\"./test_raw_dataset\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8ca99a32",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving the dataset (1/1 shards): 100%|██████████| 86941/86941 [00:00<00:00, 1609978.83 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 21736/21736 [00:00<00:00, 1358821.21 examples/s]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df_concat_train = pd.concat(\n",
    "    [\n",
    "        X_train_processed['final_text'].reset_index(drop=True),\n",
    "        pd.Series(y_train_enc, name=\"label\")\n",
    "    ],\n",
    "    axis=1\n",
    ")\n",
    "\n",
    "# X_train_processed['final_text'] -> Input Teks untuk Model\n",
    "# df_concat_train.to_csv('./topic_classification/topic_classification_train.csv')\n",
    "df_concat_test = pd.concat(\n",
    "    [\n",
    "        X_val_processed['final_text'].reset_index(drop=True),\n",
    "        pd.Series(y_val_enc, name=\"label\")\n",
    "    ],\n",
    "    axis=1\n",
    ")\n",
    "# df_concat_test.to_csv('./topic_classification/topic_classification_test.csv')\n",
    "\n",
    "from datasets import Dataset\n",
    "\n",
    "dataset_train = Dataset.from_pandas(df_concat_train)\n",
    "dataset_test = Dataset.from_pandas(df_concat_test)\n",
    "\n",
    "\n",
    "dataset_train.save_to_disk(\"./topic_classification/train_dataset\")\n",
    "dataset_test.save_to_disk(\"./topic_classification/test_dataset\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79891f72",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "label_mapping_py = {str(k): int(v) for k, v in label_mapping.items()}\n",
    "\n",
    "with open(\"label_mapping.json\", \"w\") as f:\n",
    "    json.dump(label_mapping_py, f, indent=4)\n",
    "\n",
    "\n",
    "class_weight_dict_py = {str(k): float(v) for k, v in class_weight_dict.items()}\n",
    "\n",
    "with open(\"class_weight.json\", \"w\") as f:\n",
    "    json.dump(class_weight_dict_py, f, indent=4)"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 3,
   "id": "fa98e322",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'y_train_enc' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 6\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msklearn\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mutils\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mclass_weight\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m compute_class_weight\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnumpy\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnp\u001b[39;00m\n\u001b[32m      4\u001b[39m class_weights = compute_class_weight(\n\u001b[32m      5\u001b[39m     class_weight=\u001b[33m'\u001b[39m\u001b[33mbalanced\u001b[39m\u001b[33m'\u001b[39m,\n\u001b[32m----> \u001b[39m\u001b[32m6\u001b[39m     classes=np.unique(\u001b[43my_train_enc\u001b[49m),\n\u001b[32m      7\u001b[39m     y=y_train_enc\n\u001b[32m      8\u001b[39m )\n\u001b[32m      9\u001b[39m class_weight_dict = \u001b[38;5;28mdict\u001b[39m(\u001b[38;5;28mzip\u001b[39m(np.unique(y_train_enc), class_weights))\n",
      "\u001b[31mNameError\u001b[39m: name 'y_train_enc' is not defined"
     ]
    }
   ],
   "source": [
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "import numpy as np\n",
    "\n",
    "class_weights = compute_class_weight(\n",
    "    class_weight='balanced',\n",
    "    classes=np.unique(y_train_enc),\n",
    "    y=y_train_enc\n",
    ")\n",
    "class_weight_dict = dict(zip(np.unique(y_train_enc), class_weights))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1c50f5be",
=======
   "execution_count": 10,
   "id": "d9db552a",
>>>>>>> 4a281e6a8e77def85afd423bdcd03342545e68bb
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
<<<<<<< HEAD
      "    features: ['input_text', 'related_work', '__index_level_0__'],\n",
      "    num_rows: 23496\n",
=======
      "    features: ['final_text', 'label'],\n",
      "    num_rows: 86941\n",
>>>>>>> 4a281e6a8e77def85afd423bdcd03342545e68bb
      "})\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_from_disk\n",
<<<<<<< HEAD
    "\n",
    "ds = load_from_disk(\"processed_multixscience_data\")\n",
    "print(ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c9466a02",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'We solve the subgraph isomorphism problem in planar graphs in linear time, for any pattern of constant size. Our results are based on a technique of partitioning the planar graph into pieces of small tree-width, and applying dynamic programming within each piece. The same methods can be used to solve other planar graph problems including connectivity, diameter, girth, induced subgraph isomorphism, and shortest paths. <doc-sep> @cite_0 It is well known that any planar graph contains at most O(n) complete subgraphs. We extend this to an exact characterization: G occurs O(n) times as a subgraph of any planar graph, if and only if G is three-connected. We generalize these results to similarly characterize certain other minor-closed families of graphs; in particular, G occurs O(n) times as a'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds['input_text'][0]"
=======
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "import numpy as np\n",
    "import json\n",
    "\n",
    "\n",
    "# Load dataset IMDB\n",
    "dataset = load_from_disk(\"./topic_classification/train_dataset\")\n",
    "y_train_enc = dataset['label']\n",
    "\n",
    "# Lihat split yang tersedia\n",
    "print(dataset)  # train, test, unsupervised\n",
    "\n",
    "class_weights = compute_class_weight(\n",
    "    class_weight='balanced',\n",
    "    classes=np.unique(y_train_enc),\n",
    "    y=y_train_enc\n",
    ")\n",
    "class_weight_dict = dict(zip(np.unique(y_train_enc), class_weights))\n",
    "\n",
    "class_weight_dict_py = {str(k): float(v) for k, v in class_weight_dict.items()}\n",
    "\n",
    "with open(\"class_weight.json\", \"w\") as f:\n",
    "    json.dump(class_weight_dict_py, f, indent=4)\n"
>>>>>>> 4a281e6a8e77def85afd423bdcd03342545e68bb
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
<<<<<<< HEAD
   "id": "3f087aef",
=======
   "id": "86e98985",
>>>>>>> 4a281e6a8e77def85afd423bdcd03342545e68bb
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hf_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
