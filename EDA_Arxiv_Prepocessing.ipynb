{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "252b4a37",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "# Login using e.g. `huggingface-cli login` to access this dataset\n",
    "ds = load_dataset(\"kilian-group/arxiv-classifier\", \"default\")\n",
    "\n",
    "df = ds[\"train\"].to_pandas()\n",
    "\n",
    "df_clean = df.dropna(subset=['abstract', 'title']).copy()\n",
    "df_clean = df_clean[df_clean['abstract'].apply(len) > 50] # Hapus abstrak terlalu pendek"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bb5db442",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Lenovo\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\Lenovo\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\Lenovo\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. Melakukan Split Data (Stratified)...\n",
      "   Data Train: 86941\n",
      "   Data Val  : 21736\n",
      "\n",
      "2. Memproses Data TRAIN...\n",
      "   > Membersihkan teks...\n",
      "3. Memproses Data VAL...\n",
      "   > Membersihkan teks...\n",
      "\n",
      "4. Menyiapkan Label & Bobot...\n",
      "\n",
      "✅ PIPELINE SELESAI!\n",
      "Label Mapping: {'astro-ph': np.int64(0), 'cond-mat': np.int64(1), 'cs': np.int64(2), 'econ': np.int64(3), 'eess': np.int64(4), 'gr-qc': np.int64(5), 'hep-ex': np.int64(6), 'hep-lat': np.int64(7), 'hep-ph': np.int64(8), 'hep-th': np.int64(9), 'math': np.int64(10), 'math-ph': np.int64(11), 'nlin': np.int64(12), 'nucl-ex': np.int64(13), 'nucl-th': np.int64(14), 'physics': np.int64(15), 'q-bio': np.int64(16), 'q-fin': np.int64(17), 'quant-ph': np.int64(18), 'stat': np.int64(19)}\n",
      "\n",
      "Contoh Class Weights (Train Data):\n",
      "  - astro-ph: 0.9725\n",
      "  - cond-mat: 0.7554\n",
      "  - cs: 0.1949\n",
      "  - econ: 12.6736\n",
      "  - eess: 1.8128\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# --- 1. SETUP & RESOURCE ---\n",
    "# Download resource NLTK (Hanya jika belum ada)\n",
    "try:\n",
    "    nltk.data.find('corpora/stopwords')\n",
    "    nltk.data.find('corpora/wordnet')\n",
    "except LookupError:\n",
    "    nltk.download('stopwords')\n",
    "    nltk.download('wordnet')\n",
    "    nltk.download('omw-1.4')\n",
    "\n",
    "# Inisialisasi tools cleaning\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "custom_stopwords = set([\n",
    "    'based', 'proposed', 'using', 'paper', 'data', 'results', 'method', \n",
    "    'model', 'approach', 'analysis', 'study', 'performance', 'new', \n",
    "    'presented', 'show', 'demonstrate', 'investigate'\n",
    "])\n",
    "stop_words = set(stopwords.words('english')) | custom_stopwords\n",
    "\n",
    "# --- 2. DEFINISI FUNGSI ---\n",
    "\n",
    "def clean_text_classifier(text):\n",
    "    \"\"\"\n",
    "    Fungsi cleaning khusus untuk Classifier (Router).\n",
    "    Fokus: Hapus noise, pertahankan sinyal matematika ([EQ]).\n",
    "    \"\"\"\n",
    "    if not isinstance(text, str): return \"\"\n",
    "    \n",
    "    # 1. Ganti rumus LaTeX ($...$) dengan token khusus [EQ]\n",
    "    text = re.sub(r'\\$.*?\\$', ' [EQ] ', text)\n",
    "    \n",
    "    # 2. Lowercase\n",
    "    text = text.lower()\n",
    "    \n",
    "    # 3. Hapus karakter non-alfabet (kecuali token [EQ] dan spasi)\n",
    "    text = re.sub(r'[^a-z\\s\\[\\]]', ' ', text)\n",
    "    \n",
    "    # 4. Tokenisasi, Stopwords, & Lemmatization\n",
    "    words = text.split()\n",
    "    cleaned_words = [\n",
    "        lemmatizer.lemmatize(w) \n",
    "        for w in words \n",
    "        if w not in stop_words and len(w) > 2\n",
    "    ]\n",
    "    \n",
    "    return \" \".join(cleaned_words)\n",
    "\n",
    "def preprocess_batch(df):\n",
    "    \"\"\"\n",
    "    Wrapper untuk menerapkan preprocessing ke dataframe (Train atau Test).\n",
    "    \"\"\"\n",
    "    df = df.copy()\n",
    "    \n",
    "    # A. FEATURE ENGINEERING (Pada Raw Text)\n",
    "    # Hitung fitur sebelum teks dibersihkan/hilang\n",
    "    df['num_latex'] = df['abstract'].astype(str).apply(lambda x: x.count('$'))\n",
    "    df['text_len'] = df['abstract'].astype(str).apply(len)\n",
    "    \n",
    "    # B. TEXT CLEANING\n",
    "    print(\"   > Membersihkan teks...\")\n",
    "    df['clean_abstract'] = df['abstract'].astype(str).apply(clean_text_classifier)\n",
    "    \n",
    "    # C. FITUR GABUNGAN (Text + Hand-crafted Features)\n",
    "    # Menambahkan sinyal 'high_math_density' jika banyak rumus\n",
    "    df['final_text'] = df['clean_abstract'] + \\\n",
    "                       np.where(df['num_latex'] > 5, ' high_math_density', '')\n",
    "    \n",
    "    return df\n",
    "\n",
    "# --- 3. EKSEKUSI PIPELINE (SPLIT DULU!) ---\n",
    "\n",
    "# Asumsi: df_clean adalah dataframe awal Anda yang sudah membuang baris null\n",
    "# df_clean = ... (Load data Anda di sini)\n",
    "\n",
    "print(\"1. Melakukan Split Data (Stratified)...\")\n",
    "# Kita split RAW data dulu untuk mencegah kebocoran informasi\n",
    "X_raw = df_clean.drop(columns=['field']) # Fitur mentah\n",
    "y_raw = df_clean['field']               # Label mentah\n",
    "\n",
    "X_train_raw, X_val_raw, y_train_raw, y_val_raw = train_test_split(\n",
    "    X_raw, \n",
    "    y_raw,\n",
    "    test_size=0.2, \n",
    "    stratify=y_raw, # Menjaga proporsi kelas\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "print(f\"   Data Train: {len(X_train_raw)}\")\n",
    "print(f\"   Data Val  : {len(X_val_raw)}\")\n",
    "\n",
    "# --- 4. TERAPKAN PREPROCESSING TERPISAH ---\n",
    "\n",
    "print(\"\\n2. Memproses Data TRAIN...\")\n",
    "X_train_processed = preprocess_batch(X_train_raw)\n",
    "\n",
    "print(\"3. Memproses Data VAL...\")\n",
    "X_val_processed = preprocess_batch(X_val_raw)\n",
    "\n",
    "# --- 5. LABEL ENCODING & CLASS WEIGHTS ---\n",
    "\n",
    "print(\"\\n4. Menyiapkan Label & Bobot...\")\n",
    "le = LabelEncoder()\n",
    "\n",
    "# Fit hanya pada TRAIN, lalu transform ke keduanya\n",
    "y_train_enc = le.fit_transform(y_train_raw)\n",
    "y_val_enc = le.transform(y_val_raw) # Gunakan mapping yang sama dengan train\n",
    "\n",
    "# Simpan mapping\n",
    "label_mapping = dict(zip(le.classes_, le.transform(le.classes_)))\n",
    "\n",
    "# Hitung Class Weights (Hanya dari data TRAIN)\n",
    "class_weights = compute_class_weight(\n",
    "    class_weight='balanced',\n",
    "    classes=np.unique(y_train_enc),\n",
    "    y=y_train_enc\n",
    ")\n",
    "class_weight_dict = dict(zip(np.unique(y_train_enc), class_weights))\n",
    "\n",
    "print(\"\\n✅ PIPELINE SELESAI!\")\n",
    "print(\"Label Mapping:\", label_mapping)\n",
    "print(\"\\nContoh Class Weights (Train Data):\")\n",
    "for label_name in list(label_mapping.keys())[:5]:\n",
    "    idx = label_mapping[label_name]\n",
    "    print(f\"  - {label_name}: {class_weight_dict[idx]:.4f}\")\n",
    "\n",
    "# --- OUTPUT DATA SIAP PAKAI ---\n",
    "# X_train_processed['final_text'] -> Input Teks untuk Model\n",
    "# X_train_processed['num_latex']  -> Input Numerik Tambahan (jika pakai arsitektur hybrid)\n",
    "# y_train_enc                     -> Target Label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8523d16f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The history saving thread hit an unexpected error (OperationalError('database or disk is full')).History will not be written to the database.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[8]\u001b[39m\u001b[32m, line 19\u001b[39m\n\u001b[32m     17\u001b[39m buffer_train = BytesIO()\n\u001b[32m     18\u001b[39m df_train = pd.concat([X_train_raw, y_train_raw], axis=\u001b[32m1\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m19\u001b[39m \u001b[43mdf_train\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbuffer_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindex\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m     20\u001b[39m buffer_train.seek(\u001b[32m0\u001b[39m)\n\u001b[32m     22\u001b[39m api.upload_file(\n\u001b[32m     23\u001b[39m     path_or_fileobj=buffer_train,\n\u001b[32m     24\u001b[39m     path_in_repo=\u001b[33m\"\u001b[39m\u001b[33mraw_train.csv\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m     25\u001b[39m     repo_id=repo_id,\n\u001b[32m     26\u001b[39m     repo_type=\u001b[33m\"\u001b[39m\u001b[33mdataset\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     27\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Lenovo\\Documents\\S2\\NLP\\Tubes-NLP-PaperMatch\\.venv\\Lib\\site-packages\\pandas\\util\\_decorators.py:333\u001b[39m, in \u001b[36mdeprecate_nonkeyword_arguments.<locals>.decorate.<locals>.wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    327\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(args) > num_allow_args:\n\u001b[32m    328\u001b[39m     warnings.warn(\n\u001b[32m    329\u001b[39m         msg.format(arguments=_format_argument_list(allow_args)),\n\u001b[32m    330\u001b[39m         \u001b[38;5;167;01mFutureWarning\u001b[39;00m,\n\u001b[32m    331\u001b[39m         stacklevel=find_stack_level(),\n\u001b[32m    332\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m333\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Lenovo\\Documents\\S2\\NLP\\Tubes-NLP-PaperMatch\\.venv\\Lib\\site-packages\\pandas\\core\\generic.py:3989\u001b[39m, in \u001b[36mNDFrame.to_csv\u001b[39m\u001b[34m(self, path_or_buf, sep, na_rep, float_format, columns, header, index, index_label, mode, encoding, compression, quoting, quotechar, lineterminator, chunksize, date_format, doublequote, escapechar, decimal, errors, storage_options)\u001b[39m\n\u001b[32m   3978\u001b[39m df = \u001b[38;5;28mself\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m, ABCDataFrame) \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m.to_frame()\n\u001b[32m   3980\u001b[39m formatter = DataFrameFormatter(\n\u001b[32m   3981\u001b[39m     frame=df,\n\u001b[32m   3982\u001b[39m     header=header,\n\u001b[32m   (...)\u001b[39m\u001b[32m   3986\u001b[39m     decimal=decimal,\n\u001b[32m   3987\u001b[39m )\n\u001b[32m-> \u001b[39m\u001b[32m3989\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mDataFrameRenderer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mformatter\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto_csv\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   3990\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpath_or_buf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3991\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlineterminator\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlineterminator\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3992\u001b[39m \u001b[43m    \u001b[49m\u001b[43msep\u001b[49m\u001b[43m=\u001b[49m\u001b[43msep\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3993\u001b[39m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[43m=\u001b[49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3994\u001b[39m \u001b[43m    \u001b[49m\u001b[43merrors\u001b[49m\u001b[43m=\u001b[49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3995\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcompression\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3996\u001b[39m \u001b[43m    \u001b[49m\u001b[43mquoting\u001b[49m\u001b[43m=\u001b[49m\u001b[43mquoting\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3997\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3998\u001b[39m \u001b[43m    \u001b[49m\u001b[43mindex_label\u001b[49m\u001b[43m=\u001b[49m\u001b[43mindex_label\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3999\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4000\u001b[39m \u001b[43m    \u001b[49m\u001b[43mchunksize\u001b[49m\u001b[43m=\u001b[49m\u001b[43mchunksize\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4001\u001b[39m \u001b[43m    \u001b[49m\u001b[43mquotechar\u001b[49m\u001b[43m=\u001b[49m\u001b[43mquotechar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4002\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdate_format\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdate_format\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4003\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdoublequote\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdoublequote\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4004\u001b[39m \u001b[43m    \u001b[49m\u001b[43mescapechar\u001b[49m\u001b[43m=\u001b[49m\u001b[43mescapechar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4005\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4006\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Lenovo\\Documents\\S2\\NLP\\Tubes-NLP-PaperMatch\\.venv\\Lib\\site-packages\\pandas\\io\\formats\\format.py:1014\u001b[39m, in \u001b[36mDataFrameRenderer.to_csv\u001b[39m\u001b[34m(self, path_or_buf, encoding, sep, columns, index_label, mode, compression, quoting, quotechar, lineterminator, chunksize, date_format, doublequote, escapechar, errors, storage_options)\u001b[39m\n\u001b[32m    993\u001b[39m     created_buffer = \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[32m    995\u001b[39m csv_formatter = CSVFormatter(\n\u001b[32m    996\u001b[39m     path_or_buf=path_or_buf,\n\u001b[32m    997\u001b[39m     lineterminator=lineterminator,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1012\u001b[39m     formatter=\u001b[38;5;28mself\u001b[39m.fmt,\n\u001b[32m   1013\u001b[39m )\n\u001b[32m-> \u001b[39m\u001b[32m1014\u001b[39m \u001b[43mcsv_formatter\u001b[49m\u001b[43m.\u001b[49m\u001b[43msave\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1016\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m created_buffer:\n\u001b[32m   1017\u001b[39m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(path_or_buf, StringIO)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Lenovo\\Documents\\S2\\NLP\\Tubes-NLP-PaperMatch\\.venv\\Lib\\site-packages\\pandas\\io\\formats\\csvs.py:270\u001b[39m, in \u001b[36mCSVFormatter.save\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    251\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m get_handle(\n\u001b[32m    252\u001b[39m     \u001b[38;5;28mself\u001b[39m.filepath_or_buffer,\n\u001b[32m    253\u001b[39m     \u001b[38;5;28mself\u001b[39m.mode,\n\u001b[32m   (...)\u001b[39m\u001b[32m    258\u001b[39m ) \u001b[38;5;28;01mas\u001b[39;00m handles:\n\u001b[32m    259\u001b[39m     \u001b[38;5;66;03m# Note: self.encoding is irrelevant here\u001b[39;00m\n\u001b[32m    260\u001b[39m     \u001b[38;5;28mself\u001b[39m.writer = csvlib.writer(\n\u001b[32m    261\u001b[39m         handles.handle,\n\u001b[32m    262\u001b[39m         lineterminator=\u001b[38;5;28mself\u001b[39m.lineterminator,\n\u001b[32m   (...)\u001b[39m\u001b[32m    267\u001b[39m         quotechar=\u001b[38;5;28mself\u001b[39m.quotechar,\n\u001b[32m    268\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m270\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_save\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Lenovo\\Documents\\S2\\NLP\\Tubes-NLP-PaperMatch\\.venv\\Lib\\site-packages\\pandas\\io\\formats\\csvs.py:275\u001b[39m, in \u001b[36mCSVFormatter._save\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    273\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._need_to_save_header:\n\u001b[32m    274\u001b[39m     \u001b[38;5;28mself\u001b[39m._save_header()\n\u001b[32m--> \u001b[39m\u001b[32m275\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_save_body\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Lenovo\\Documents\\S2\\NLP\\Tubes-NLP-PaperMatch\\.venv\\Lib\\site-packages\\pandas\\io\\formats\\csvs.py:313\u001b[39m, in \u001b[36mCSVFormatter._save_body\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    311\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m start_i >= end_i:\n\u001b[32m    312\u001b[39m     \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m313\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_save_chunk\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstart_i\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mend_i\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Lenovo\\Documents\\S2\\NLP\\Tubes-NLP-PaperMatch\\.venv\\Lib\\site-packages\\pandas\\io\\formats\\csvs.py:324\u001b[39m, in \u001b[36mCSVFormatter._save_chunk\u001b[39m\u001b[34m(self, start_i, end_i)\u001b[39m\n\u001b[32m    321\u001b[39m data = \u001b[38;5;28mlist\u001b[39m(res._iter_column_arrays())\n\u001b[32m    323\u001b[39m ix = \u001b[38;5;28mself\u001b[39m.data_index[slicer]._get_values_for_csv(**\u001b[38;5;28mself\u001b[39m._number_format)\n\u001b[32m--> \u001b[39m\u001b[32m324\u001b[39m \u001b[43mlibwriters\u001b[49m\u001b[43m.\u001b[49m\u001b[43mwrite_csv_rows\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    325\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    326\u001b[39m \u001b[43m    \u001b[49m\u001b[43mix\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    327\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mnlevels\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    328\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcols\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    329\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mwriter\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    330\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mpandas/_libs/writers.pyx:73\u001b[39m, in \u001b[36mpandas._libs.writers.write_csv_rows\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m<frozen codecs>:331\u001b[39m, in \u001b[36mreset\u001b[39m\u001b[34m(self)\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m<frozen codecs>:279\u001b[39m, in \u001b[36mreset\u001b[39m\u001b[34m(self)\u001b[39m\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "from huggingface_hub import HfApi\n",
    "import pandas as pd\n",
    "from io import BytesIO\n",
    "\n",
    "api = HfApi()\n",
    "\n",
    "# create csv\n",
    "# df_train = pd.concat([X_train_raw, y_train_raw], axis=1)\n",
    "# df_train.to_csv(\"raw_train.csv\", index=False)\n",
    "\n",
    "# df_test = pd.concat([X_val_raw, y_val_raw], axis=1)\n",
    "# df_test.to_csv(\"raw_test.csv\", index=False)\n",
    "\n",
    "repo_id = \"ALAN43/NLP-PAPERMATCH\"\n",
    "\n",
    "# --- TRAIN ---\n",
    "buffer_train = BytesIO()\n",
    "df_train = pd.concat([X_train_raw, y_train_raw], axis=1)\n",
    "df_train.to_csv(buffer_train, index=False)\n",
    "buffer_train.seek(0)\n",
    "\n",
    "api.upload_file(\n",
    "    path_or_fileobj=buffer_train,\n",
    "    path_in_repo=\"raw_train.csv\",\n",
    "    repo_id=repo_id,\n",
    "    repo_type=\"dataset\"\n",
    ")\n",
    "\n",
    "# --- TEST ---\n",
    "buffer_test = BytesIO()\n",
    "df_test = pd.concat([X_val_raw, y_val_raw], axis=1)\n",
    "df_test.to_csv(buffer_test, index=False)\n",
    "buffer_test.seek(0)\n",
    "\n",
    "api.upload_file(\n",
    "    path_or_fileobj=buffer_test,\n",
    "    path_in_repo=\"raw_test.csv\",\n",
    "    repo_id=repo_id,\n",
    "    repo_type=\"dataset\"\n",
    ")\n",
    "\n",
    "print(\"Uploaded directly without saving any local files!\")\n",
    "\n",
    "print(\"Upload complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ca99a32",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_concat_train = pd.concat([X_train_processed['final_text'], y_train_enc], axis=1)\n",
    "# # X_train_processed['final_text'] -> Input Teks untuk Model\n",
    "# df_concat_train.to_csv('./topic_classification/topic_classification_train.csv')\n",
    "# df_concat_test = pd.concat([X_val_processed['final_text'], y_val_enc], axis=1)\n",
    "# df_concat_test.to_csv('./topic_classification/topic_classification_test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79891f72",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "with open(\"label_mapping.json\", \"w\") as f:\n",
    "    json.dump(label_mapping, f, indent=4)\n",
    "\n",
    "\n",
    "with open(\"class_weight.json\", \"w\") as f:\n",
    "    json.dump(label_mapping, f, indent=4)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
