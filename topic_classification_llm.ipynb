{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "11997c8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Iskandar\\Documents\\GitHub\\Tubes-NLP-PaperMatch\\hf_env\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset Train loaded: Dataset({\n",
      "    features: ['final_text', 'label'],\n",
      "    num_rows: 86941\n",
      "})\n",
      "Dataset Test loaded: Dataset({\n",
      "    features: ['final_text', 'label'],\n",
      "    num_rows: 21736\n",
      "})\n",
      "Label Mapping loaded: {'astro-ph': 0, 'cond-mat': 1, 'cs': 2, 'econ': 3, 'eess': 4, 'gr-qc': 5, 'hep-ex': 6, 'hep-lat': 7, 'hep-ph': 8, 'hep-th': 9, 'math': 10, 'math-ph': 11, 'nlin': 12, 'nucl-ex': 13, 'nucl-th': 14, 'physics': 15, 'q-bio': 16, 'q-fin': 17, 'quant-ph': 18, 'stat': 19}\n",
      "Class Weights loaded (re-casted): {0: 0.9724944071588367, 1: 0.7553518679409209, 2: 0.19485633600788918, 3: 12.673615160349854, 4: 1.8127814845704755, 5: 6.792265625, 6: 6.792265625, 7: 6.792265625, 8: 6.813557993730408, 9: 6.792265625, 10: 0.2343423180592992, 11: 6.792265625, 12: 1.552517857142857, 13: 6.802895148669797, 14: 6.792265625, 15: 0.3266003005259204, 16: 0.8681945276612743, 17: 1.1803013847407005, 18: 6.802895148669797, 19: 1.69806640625}\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from datasets import load_from_disk\n",
    "\n",
    "dataset_train_loaded = load_from_disk(\"./topic_classification/train_dataset\")\n",
    "dataset_test_loaded = load_from_disk(\"./topic_classification/test_dataset\")\n",
    "\n",
    "print(\"Dataset Train loaded:\", dataset_train_loaded)\n",
    "print(\"Dataset Test loaded:\", dataset_test_loaded)\n",
    "\n",
    "with open(\"label_mapping.json\", \"r\") as f:\n",
    "    label_mapping_loaded = json.load(f)\n",
    "\n",
    "print(\"Label Mapping loaded:\", label_mapping_loaded)\n",
    "\n",
    "with open(\"class_weight.json\", \"r\") as f:\n",
    "    class_weight_loaded = json.load(f)\n",
    "\n",
    "class_weight_dict = {int(k): v for k, v in class_weight_loaded.items()}\n",
    "\n",
    "print(\"Class Weights loaded (re-casted):\", class_weight_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1c3f7345",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "5 Data Pertama (Train)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>final_text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ante forecast outcome interpreted counterfactu...</td>\n",
       "      <td>17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>gaming customizing individual character create...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>framework european research project meteomet l...</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>software product quality defined feature chara...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>optimizing communication imperative large scal...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          final_text  label\n",
       "0  ante forecast outcome interpreted counterfactu...     17\n",
       "1  gaming customizing individual character create...      2\n",
       "2  framework european research project meteomet l...     15\n",
       "3  software product quality defined feature chara...      2\n",
       "4  optimizing communication imperative large scal...      2"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "5 Data Pertama (Test)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>final_text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>work propose use dropout bayesian estimator in...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>given simple polygon [eq] consisting [eq] vert...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>pattern stored within pre trained deep neural ...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ride sharing service gaining popularity crucia...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>perform experiment phase simulation ring netwo...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          final_text  label\n",
       "0  work propose use dropout bayesian estimator in...      4\n",
       "1  given simple polygon [eq] consisting [eq] vert...      2\n",
       "2  pattern stored within pre trained deep neural ...      2\n",
       "3  ride sharing service gaining popularity crucia...      2\n",
       "4  perform experiment phase simulation ring netwo...      1"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "print(\"\\n5 Data Pertama (Train)\")\n",
    "df_sample_train = pd.DataFrame(dataset_train_loaded[:5])\n",
    "display(df_sample_train)\n",
    "\n",
    "print(\"\\n5 Data Pertama (Test)\")\n",
    "df_sample_test = pd.DataFrame(dataset_test_loaded[:5])\n",
    "display(df_sample_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "339c3123",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tipe kolom label sekarang: ClassLabel(names=['0', '1', '2', '3', '4', '5', '6', '7', '8', '9', '10', '11', '12', '13', '14', '15', '16', '17', '18', '19'])\n",
      "==============================\n",
      "Original Train: 86941\n",
      "New Train (80%): 69552\n",
      "New Val   (20%): 17389\n",
      "Original Test : 21736\n",
      "==============================\n"
     ]
    }
   ],
   "source": [
    "from datasets import ClassLabel\n",
    "\n",
    "num_classes = len(label_mapping_loaded)\n",
    "\n",
    "dataset_train_loaded = dataset_train_loaded.cast_column(\n",
    "    \"label\", \n",
    "    ClassLabel(num_classes=num_classes)\n",
    ")\n",
    "\n",
    "print(\"Tipe kolom label sekarang:\", dataset_train_loaded.features['label'])\n",
    "\n",
    "split_result = dataset_train_loaded.train_test_split(\n",
    "    test_size=0.2, \n",
    "    seed=42, \n",
    "    stratify_by_column=\"label\"\n",
    ")\n",
    "\n",
    "dataset_train_final = split_result['train']\n",
    "dataset_val_final = split_result['test']\n",
    "\n",
    "print(\"=\"*30)\n",
    "print(f\"Original Train: {len(dataset_train_loaded)}\")\n",
    "print(f\"New Train (80%): {len(dataset_train_final)}\")\n",
    "print(f\"New Val   (20%): {len(dataset_val_final)}\")\n",
    "print(f\"Original Test : {len(dataset_test_loaded)}\")\n",
    "print(\"=\"*30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3fe94949",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\Iskandar\\Documents\\GitHub\\Tubes-NLP-PaperMatch\\hf_env\\Lib\\site-packages\\tf_keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import transformers\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, Trainer, TrainingArguments\n",
    "from peft import LoraConfig, get_peft_model, TaskType\n",
    "import torch, os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c5af7101",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer_map = {\n",
    "    \"bert\": \"bert-base-uncased\",\n",
    "    \"roberta\": \"roberta-base\",\n",
    "    \"distilbert\": \"distilbert-base-uncased\"\n",
    "}\n",
    "\n",
    "tokenizers = {k: AutoTokenizer.from_pretrained(v) for k, v in tokenizer_map.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2425157e",
   "metadata": {},
   "outputs": [],
   "source": [
    "target_modules_map = {\n",
    "    \"bert\": [\"query\", \"value\"],\n",
    "    \"roberta\": [\"query\", \"value\"],\n",
    "    \"distilbert\": [\"q_lin\", \"v_lin\"]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8b608183",
   "metadata": {},
   "outputs": [],
   "source": [
    "models_to_train = {\n",
    "    \"bert\": \"bert-base-uncased\",\n",
    "    \"roberta\": \"roberta-base\",\n",
    "    \"distilbert\": \"distilbert-base-uncased\",\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e7935cf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "weights_list = [class_weight_loaded[str(i)] for i in range(num_classes)]\n",
    "weights_tensor = torch.tensor(weights_list, dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ebbbd3cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weights Tensor loaded: tensor([ 0.9725,  0.7554,  0.1949, 12.6736,  1.8128,  6.7923,  6.7923,  6.7923,\n",
      "         6.8136,  6.7923,  0.2343,  6.7923,  1.5525,  6.8029,  6.7923,  0.3266,\n",
      "         0.8682,  1.1803,  6.8029,  1.6981])\n"
     ]
    }
   ],
   "source": [
    "print(\"Weights Tensor loaded:\", weights_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "39ca1bce",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "\n",
    "class WeightedTrainer(Trainer):\n",
    "    def __init__(self, class_weights, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.class_weights = class_weights\n",
    "    \n",
    "    def compute_loss(self, model, inputs, return_outputs=False, **kwargs):\n",
    "        labels = inputs.get(\"labels\")\n",
    "        outputs = model(**inputs)\n",
    "        logits = outputs.get(\"logits\")\n",
    "        \n",
    "        if self.class_weights.device != model.device:\n",
    "            self.class_weights = self.class_weights.to(model.device)\n",
    "            \n",
    "        loss_fct = nn.CrossEntropyLoss(weight=self.class_weights)\n",
    "        loss = loss_fct(logits.view(-1, self.model.config.num_labels), labels.view(-1))\n",
    "        \n",
    "        return (loss, outputs) if return_outputs else loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "48239135",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "    acc = accuracy_score(labels, predictions)\n",
    "    f1 = f1_score(labels, predictions, average='weighted')\n",
    "    return {\"accuracy\": acc, \"f1\": f1}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "884c2fa9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "Fine-tuning bert with LoRA (bert-base-uncased)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "C:\\Users\\Iskandar\\AppData\\Local\\Temp\\ipykernel_37028\\586253147.py:82: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 605,204 || all params: 110,102,824 || trainable%: 0.5497\n",
      "Tokenizing datasets...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='60858' max='65205' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [60858/65205 2:59:28 < 12:49, 5.65 it/s, Epoch 14/15]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.715500</td>\n",
       "      <td>0.749630</td>\n",
       "      <td>0.754845</td>\n",
       "      <td>0.739696</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.750500</td>\n",
       "      <td>0.678268</td>\n",
       "      <td>0.777388</td>\n",
       "      <td>0.769090</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.621500</td>\n",
       "      <td>0.660733</td>\n",
       "      <td>0.778883</td>\n",
       "      <td>0.776652</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.638300</td>\n",
       "      <td>0.642162</td>\n",
       "      <td>0.787107</td>\n",
       "      <td>0.776400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.567400</td>\n",
       "      <td>0.622501</td>\n",
       "      <td>0.793893</td>\n",
       "      <td>0.788006</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.540000</td>\n",
       "      <td>0.628467</td>\n",
       "      <td>0.791305</td>\n",
       "      <td>0.781334</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.553300</td>\n",
       "      <td>0.634377</td>\n",
       "      <td>0.789350</td>\n",
       "      <td>0.787166</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.567500</td>\n",
       "      <td>0.640683</td>\n",
       "      <td>0.797228</td>\n",
       "      <td>0.792306</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.444100</td>\n",
       "      <td>0.652289</td>\n",
       "      <td>0.791420</td>\n",
       "      <td>0.789576</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.441500</td>\n",
       "      <td>0.653185</td>\n",
       "      <td>0.799471</td>\n",
       "      <td>0.795319</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>0.457000</td>\n",
       "      <td>0.666940</td>\n",
       "      <td>0.800104</td>\n",
       "      <td>0.793836</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>0.396400</td>\n",
       "      <td>0.682104</td>\n",
       "      <td>0.796653</td>\n",
       "      <td>0.794166</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>0.317900</td>\n",
       "      <td>0.687470</td>\n",
       "      <td>0.798551</td>\n",
       "      <td>0.795827</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>0.397900</td>\n",
       "      <td>0.694194</td>\n",
       "      <td>0.796998</td>\n",
       "      <td>0.794996</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1087' max='1087' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1087/1087 01:08]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval results for bert: {'eval_loss': 0.6669399738311768, 'eval_accuracy': 0.8001035137155673, 'eval_f1': 0.7938357584784669, 'eval_runtime': 69.0177, 'eval_samples_per_second': 251.95, 'eval_steps_per_second': 15.75, 'epoch': 14.0}\n",
      "LoRA model saved to: ./hf_finetune_results_lora_baru\\bert\\lora_model\n",
      "Memory cleaned. Moving to next model...\n",
      "\n",
      "\n",
      "============================================================\n",
      "Fine-tuning roberta with LoRA (roberta-base)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 1,195,796 || all params: 125,856,808 || trainable%: 0.9501\n",
      "Tokenizing datasets...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 69552/69552 [00:09<00:00, 7362.12 examples/s]\n",
      "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 17389/17389 [00:02<00:00, 7901.51 examples/s]\n",
      "C:\\Users\\Iskandar\\AppData\\Local\\Temp\\ipykernel_37028\\586253147.py:82: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='47817' max='65205' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [47817/65205 2:21:12 < 51:20, 5.64 it/s, Epoch 11/15]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.706400</td>\n",
       "      <td>0.706525</td>\n",
       "      <td>0.762033</td>\n",
       "      <td>0.751367</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.755300</td>\n",
       "      <td>0.657006</td>\n",
       "      <td>0.776985</td>\n",
       "      <td>0.770139</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.581600</td>\n",
       "      <td>0.658303</td>\n",
       "      <td>0.774628</td>\n",
       "      <td>0.774339</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.622900</td>\n",
       "      <td>0.628491</td>\n",
       "      <td>0.786934</td>\n",
       "      <td>0.780786</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.584200</td>\n",
       "      <td>0.602436</td>\n",
       "      <td>0.796020</td>\n",
       "      <td>0.790673</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.532600</td>\n",
       "      <td>0.611981</td>\n",
       "      <td>0.796135</td>\n",
       "      <td>0.788371</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.549500</td>\n",
       "      <td>0.615564</td>\n",
       "      <td>0.793490</td>\n",
       "      <td>0.791946</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.539500</td>\n",
       "      <td>0.618119</td>\n",
       "      <td>0.800391</td>\n",
       "      <td>0.796021</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.439200</td>\n",
       "      <td>0.629794</td>\n",
       "      <td>0.794525</td>\n",
       "      <td>0.793502</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.417600</td>\n",
       "      <td>0.623446</td>\n",
       "      <td>0.800219</td>\n",
       "      <td>0.797508</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>0.397000</td>\n",
       "      <td>0.647187</td>\n",
       "      <td>0.796596</td>\n",
       "      <td>0.792290</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1087' max='1087' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1087/1087 01:08]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval results for roberta: {'eval_loss': 0.6181185841560364, 'eval_accuracy': 0.8003910518143654, 'eval_f1': 0.7960207895554434, 'eval_runtime': 68.7845, 'eval_samples_per_second': 252.804, 'eval_steps_per_second': 15.803, 'epoch': 11.0}\n",
      "LoRA model saved to: ./hf_finetune_results_lora_baru\\roberta\\lora_model\n",
      "Memory cleaned. Moving to next model...\n",
      "\n",
      "\n",
      "============================================================\n",
      "Fine-tuning distilbert with LoRA (distilbert-base-uncased)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 900,884 || all params: 67,869,736 || trainable%: 1.3274\n",
      "Tokenizing datasets...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 69552/69552 [00:09<00:00, 6978.85 examples/s]\n",
      "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 17389/17389 [00:02<00:00, 7642.92 examples/s]\n",
      "C:\\Users\\Iskandar\\AppData\\Local\\Temp\\ipykernel_37028\\586253147.py:82: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='34776' max='65205' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [34776/65205 52:20 < 45:48, 11.07 it/s, Epoch 8/15]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.647500</td>\n",
       "      <td>0.680203</td>\n",
       "      <td>0.769222</td>\n",
       "      <td>0.761052</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.684200</td>\n",
       "      <td>0.628379</td>\n",
       "      <td>0.786244</td>\n",
       "      <td>0.779889</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.540700</td>\n",
       "      <td>0.624778</td>\n",
       "      <td>0.786934</td>\n",
       "      <td>0.785292</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.611500</td>\n",
       "      <td>0.597923</td>\n",
       "      <td>0.796135</td>\n",
       "      <td>0.788947</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.556300</td>\n",
       "      <td>0.586558</td>\n",
       "      <td>0.801886</td>\n",
       "      <td>0.797399</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.478600</td>\n",
       "      <td>0.602516</td>\n",
       "      <td>0.800391</td>\n",
       "      <td>0.793325</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.488500</td>\n",
       "      <td>0.609154</td>\n",
       "      <td>0.799701</td>\n",
       "      <td>0.795568</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.460000</td>\n",
       "      <td>0.625849</td>\n",
       "      <td>0.801081</td>\n",
       "      <td>0.797072</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1087' max='1087' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1087/1087 00:36]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval results for distilbert: {'eval_loss': 0.5865576267242432, 'eval_accuracy': 0.8018862499281155, 'eval_f1': 0.7973991076830566, 'eval_runtime': 36.9492, 'eval_samples_per_second': 470.619, 'eval_steps_per_second': 29.419, 'epoch': 8.0}\n",
      "LoRA model saved to: ./hf_finetune_results_lora_baru\\distilbert\\lora_model\n",
      "Memory cleaned. Moving to next model...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, Trainer, TrainingArguments, EarlyStoppingCallback, DataCollatorWithPadding\n",
    "from peft import LoraConfig, get_peft_model, TaskType\n",
    "import torch, os\n",
    "import gc\n",
    "\n",
    "MAX_LEN = 256\n",
    "BATCH_SIZE = 16\n",
    "TRAIN_EPOCHS = 15\n",
    "OUTPUT_BASE = \"./hf_finetune_results_lora_baru\"\n",
    "os.makedirs(OUTPUT_BASE, exist_ok=True)\n",
    "\n",
    "results = {}\n",
    "\n",
    "for shortname, model_name in models_to_train.items():\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Fine-tuning {shortname} with LoRA ({model_name})\")\n",
    "\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    \n",
    "    lora_config = LoraConfig(\n",
    "        task_type=TaskType.SEQ_CLS,\n",
    "        r=16,\n",
    "        lora_alpha=32,\n",
    "        lora_dropout=0.1,\n",
    "        bias=\"none\",\n",
    "        target_modules=target_modules_map[shortname] \n",
    "    )\n",
    "\n",
    "    num_labels = dataset_train_final.features['label'].num_classes\n",
    "    \n",
    "    model = AutoModelForSequenceClassification.from_pretrained(\n",
    "        model_name,\n",
    "        num_labels=num_labels\n",
    "    )\n",
    "\n",
    "    model = get_peft_model(model, lora_config)\n",
    "    model.print_trainable_parameters()\n",
    "\n",
    "    def tokenize_function(examples):\n",
    "        return tokenizer(examples[\"final_text\"], truncation=True, padding=\"max_length\", max_length=MAX_LEN)\n",
    "\n",
    "    print(\"Tokenizing datasets...\")\n",
    "    tokenized_train = dataset_train_final.map(tokenize_function, batched=True)\n",
    "    tokenized_val = dataset_val_final.map(tokenize_function, batched=True)\n",
    "\n",
    "    cols_to_keep = [\"input_ids\", \"attention_mask\", \"label\"]\n",
    "    tokenized_train = tokenized_train.remove_columns([c for c in tokenized_train.column_names if c not in cols_to_keep])\n",
    "    tokenized_val = tokenized_val.remove_columns([c for c in tokenized_val.column_names if c not in cols_to_keep])\n",
    "    \n",
    "    tokenized_train.set_format(\"torch\")\n",
    "    tokenized_val.set_format(\"torch\")\n",
    "\n",
    "    training_args = TrainingArguments(\n",
    "        output_dir=os.path.join(OUTPUT_BASE, shortname),\n",
    "        num_train_epochs=TRAIN_EPOCHS,\n",
    "        per_device_train_batch_size=BATCH_SIZE,\n",
    "        per_device_eval_batch_size=BATCH_SIZE,\n",
    "        eval_strategy=\"epoch\",\n",
    "        logging_strategy=\"steps\",\n",
    "        logging_steps=50,\n",
    "        save_strategy=\"epoch\",\n",
    "        learning_rate=2e-4,\n",
    "        fp16=torch.cuda.is_available(),\n",
    "        save_total_limit=2,\n",
    "        metric_for_best_model=\"accuracy\",\n",
    "        load_best_model_at_end=True,\n",
    "        greater_is_better=True,\n",
    "        report_to=[]\n",
    "    )\n",
    "\n",
    "    # trainer = WeightedTrainer(\n",
    "    #     class_weights=weights_tensor,\n",
    "    #     model=model,\n",
    "    #     args=training_args,\n",
    "    #     train_dataset=tokenized_train,\n",
    "    #     eval_dataset=tokenized_val,\n",
    "    #     tokenizer=tokenizer,\n",
    "    #     compute_metrics=compute_metrics,\n",
    "    #     callbacks=[EarlyStoppingCallback(early_stopping_patience=3)]\n",
    "    # )\n",
    "    \n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=tokenized_train,\n",
    "        eval_dataset=tokenized_val,\n",
    "        tokenizer=tokenizer,\n",
    "        compute_metrics=compute_metrics,\n",
    "        callbacks=[EarlyStoppingCallback(early_stopping_patience=3)]\n",
    "    )\n",
    "\n",
    "    trainer.train()\n",
    "    eval_res = trainer.evaluate()\n",
    "    print(f\"Eval results for {shortname}:\", eval_res)\n",
    "\n",
    "    save_dir = os.path.join(OUTPUT_BASE, shortname, \"lora_model\")\n",
    "    model.save_pretrained(save_dir)\n",
    "    tokenizer.save_pretrained(save_dir)\n",
    "    print(f\"LoRA model saved to: {save_dir}\")\n",
    "\n",
    "    results[shortname] = {\n",
    "        \"model_name\": model_name,\n",
    "        \"eval\": eval_res,\n",
    "        \"save_dir\": save_dir\n",
    "    }\n",
    "\n",
    "    del model\n",
    "    del trainer\n",
    "    del tokenizer\n",
    "\n",
    "    gc.collect()\n",
    "\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()\n",
    "        \n",
    "    print(f\"Memory cleaned. Moving to next model...\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e85a230",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Test Dataset...\n",
      "Number of labels identified: 20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Casting the dataset: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 21736/21736 [00:00<00:00, 724499.48 examples/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "ðŸ§ª TESTING MODEL: bert\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizing test data for bert...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 21736/21736 [00:03<00:00, 6746.94 examples/s]\n",
      "C:\\Users\\Iskandar\\AppData\\Local\\Temp\\ipykernel_37028\\3482534635.py:76: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running prediction on 21736 samples...\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ“Š RESULT FOR BERT:\n",
      "Accuracy: 0.8019\n",
      "F1 Score: 0.7956\n",
      "------------------------------\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    astro-ph       0.87      0.90      0.89      1117\n",
      "    cond-mat       0.76      0.78      0.77      1439\n",
      "          cs       0.82      0.89      0.85      5578\n",
      "        econ       0.35      0.16      0.22        86\n",
      "        eess       0.56      0.34      0.42       599\n",
      "       gr-qc       0.68      0.54      0.60       160\n",
      "      hep-ex       0.80      0.76      0.78       160\n",
      "     hep-lat       0.78      0.85      0.81       160\n",
      "      hep-ph       0.70      0.61      0.65       160\n",
      "      hep-th       0.68      0.58      0.63       160\n",
      "        math       0.88      0.90      0.89      4637\n",
      "     math-ph       0.37      0.12      0.19       160\n",
      "        nlin       0.68      0.66      0.67       700\n",
      "     nucl-ex       0.71      0.65      0.68       160\n",
      "     nucl-th       0.59      0.57      0.58       160\n",
      "     physics       0.78      0.73      0.76      3327\n",
      "       q-bio       0.75      0.82      0.78      1252\n",
      "       q-fin       0.89      0.93      0.91       921\n",
      "    quant-ph       0.62      0.44      0.52       160\n",
      "        stat       0.63      0.56      0.59       640\n",
      "\n",
      "    accuracy                           0.80     21736\n",
      "   macro avg       0.69      0.64      0.66     21736\n",
      "weighted avg       0.79      0.80      0.80     21736\n",
      "\n",
      "\n",
      "============================================================\n",
      "ðŸ§ª TESTING MODEL: roberta\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizing test data for roberta...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 21736/21736 [00:02<00:00, 8803.52 examples/s]\n",
      "C:\\Users\\Iskandar\\AppData\\Local\\Temp\\ipykernel_37028\\3482534635.py:76: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running prediction on 21736 samples...\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ“Š RESULT FOR ROBERTA:\n",
      "Accuracy: 0.7991\n",
      "F1 Score: 0.7950\n",
      "------------------------------\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    astro-ph       0.88      0.90      0.89      1117\n",
      "    cond-mat       0.75      0.77      0.76      1439\n",
      "          cs       0.83      0.87      0.85      5578\n",
      "        econ       0.54      0.29      0.38        86\n",
      "        eess       0.51      0.43      0.47       599\n",
      "       gr-qc       0.57      0.79      0.66       160\n",
      "      hep-ex       0.81      0.74      0.77       160\n",
      "     hep-lat       0.85      0.80      0.83       160\n",
      "      hep-ph       0.66      0.58      0.62       160\n",
      "      hep-th       0.71      0.59      0.64       160\n",
      "        math       0.88      0.90      0.89      4637\n",
      "     math-ph       0.35      0.09      0.14       160\n",
      "        nlin       0.64      0.70      0.67       700\n",
      "     nucl-ex       0.66      0.65      0.66       160\n",
      "     nucl-th       0.58      0.61      0.60       160\n",
      "     physics       0.75      0.73      0.74      3327\n",
      "       q-bio       0.80      0.76      0.78      1252\n",
      "       q-fin       0.89      0.92      0.91       921\n",
      "    quant-ph       0.57      0.51      0.54       160\n",
      "        stat       0.66      0.54      0.59       640\n",
      "\n",
      "    accuracy                           0.80     21736\n",
      "   macro avg       0.69      0.66      0.67     21736\n",
      "weighted avg       0.79      0.80      0.80     21736\n",
      "\n",
      "\n",
      "============================================================\n",
      "ðŸ§ª TESTING MODEL: distilbert\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizing test data for distilbert...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 21736/21736 [00:02<00:00, 8023.84 examples/s]\n",
      "C:\\Users\\Iskandar\\AppData\\Local\\Temp\\ipykernel_37028\\3482534635.py:76: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running prediction on 21736 samples...\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ“Š RESULT FOR DISTILBERT:\n",
      "Accuracy: 0.8026\n",
      "F1 Score: 0.7985\n",
      "------------------------------\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    astro-ph       0.89      0.91      0.90      1117\n",
      "    cond-mat       0.80      0.71      0.75      1439\n",
      "          cs       0.84      0.86      0.85      5578\n",
      "        econ       0.57      0.29      0.38        86\n",
      "        eess       0.55      0.42      0.48       599\n",
      "       gr-qc       0.62      0.63      0.63       160\n",
      "      hep-ex       0.72      0.82      0.76       160\n",
      "     hep-lat       0.81      0.78      0.79       160\n",
      "      hep-ph       0.58      0.69      0.63       160\n",
      "      hep-th       0.68      0.60      0.64       160\n",
      "        math       0.88      0.91      0.89      4637\n",
      "     math-ph       0.36      0.17      0.23       160\n",
      "        nlin       0.66      0.67      0.67       700\n",
      "     nucl-ex       0.68      0.58      0.63       160\n",
      "     nucl-th       0.71      0.51      0.59       160\n",
      "     physics       0.76      0.76      0.76      3327\n",
      "       q-bio       0.73      0.86      0.79      1252\n",
      "       q-fin       0.89      0.93      0.91       921\n",
      "    quant-ph       0.55      0.47      0.51       160\n",
      "        stat       0.64      0.54      0.59       640\n",
      "\n",
      "    accuracy                           0.80     21736\n",
      "   macro avg       0.70      0.66      0.67     21736\n",
      "weighted avg       0.80      0.80      0.80     21736\n",
      "\n",
      "\n",
      "âœ… All Testing Finished!\n",
      "{'bert': {'test_loss': 0.6599531769752502, 'test_model_preparation_time': 0.005, 'test_accuracy': 0.8018954729481045, 'test_f1': 0.7956174473138251, 'test_runtime': 101.0231, 'test_samples_per_second': 215.159, 'test_steps_per_second': 26.895}, 'roberta': {'test_loss': 0.6152756214141846, 'test_model_preparation_time': 0.005, 'test_accuracy': 0.7990890688259109, 'test_f1': 0.7950472803872359, 'test_runtime': 100.8701, 'test_samples_per_second': 215.485, 'test_steps_per_second': 26.936}, 'distilbert': {'test_loss': 0.5837743878364563, 'test_model_preparation_time': 0.002, 'test_accuracy': 0.8026315789473685, 'test_f1': 0.7985226290137144, 'test_runtime': 55.8018, 'test_samples_per_second': 389.521, 'test_steps_per_second': 48.69}}\n"
     ]
    }
   ],
   "source": [
    "# Test\n",
    "\n",
    "import torch\n",
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "import gc\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, Trainer\n",
    "from peft import PeftModel, PeftConfig\n",
    "from datasets import load_from_disk, ClassLabel\n",
    "from sklearn.metrics import accuracy_score, f1_score, classification_report\n",
    "\n",
    "OUTPUT_BASE = \"./hf_finetune_results_lora_baru\"\n",
    "MAX_LEN = 256\n",
    "BATCH_SIZE = 16\n",
    "models_to_test = [\"bert\", \"roberta\", \"distilbert\"]\n",
    "\n",
    "print(\"Loading Test Dataset...\")\n",
    "dataset_test_loaded = load_from_disk(\"./topic_classification/test_dataset\")\n",
    "\n",
    "with open(\"label_mapping.json\", \"r\") as f:\n",
    "    label_mapping = json.load(f)\n",
    "\n",
    "num_labels_fixed = len(label_mapping)\n",
    "print(f\"Number of labels identified: {num_labels_fixed}\")\n",
    "\n",
    "dataset_test_loaded = dataset_test_loaded.cast_column(\n",
    "    \"label\", \n",
    "    ClassLabel(num_classes=num_labels_fixed)\n",
    ")\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "    acc = accuracy_score(labels, predictions)\n",
    "    f1 = f1_score(labels, predictions, average='weighted')\n",
    "    return {\"accuracy\": acc, \"f1\": f1}\n",
    "\n",
    "test_results = {}\n",
    "\n",
    "for shortname in models_to_test:\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"TESTING MODEL: {shortname}\")\n",
    "    \n",
    "    adapter_path = os.path.join(OUTPUT_BASE, shortname, \"lora_model\")\n",
    "    \n",
    "    if not os.path.exists(adapter_path):\n",
    "        print(f\"Path not found: {adapter_path}\")\n",
    "        continue\n",
    "\n",
    "    peft_config = PeftConfig.from_pretrained(adapter_path)\n",
    "    tokenizer = AutoTokenizer.from_pretrained(adapter_path)\n",
    "    \n",
    "    base_model = AutoModelForSequenceClassification.from_pretrained(\n",
    "        peft_config.base_model_name_or_path,\n",
    "        num_labels=num_labels_fixed, \n",
    "        ignore_mismatched_sizes=True\n",
    "    )\n",
    "    \n",
    "    model = PeftModel.from_pretrained(base_model, adapter_path)\n",
    "    \n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "    \n",
    "    print(f\"Tokenizing test data for {shortname}...\")\n",
    "    def tokenize_fn(examples):\n",
    "        return tokenizer(examples[\"final_text\"], truncation=True, padding=\"max_length\", max_length=MAX_LEN)\n",
    "    \n",
    "    test_tokenized = dataset_test_loaded.map(tokenize_fn, batched=True)\n",
    "    \n",
    "    cols_to_keep = [\"input_ids\", \"attention_mask\", \"label\"]\n",
    "    test_tokenized = test_tokenized.remove_columns([c for c in test_tokenized.column_names if c not in cols_to_keep])\n",
    "    test_tokenized.set_format(\"torch\")\n",
    "    \n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        tokenizer=tokenizer,\n",
    "        compute_metrics=compute_metrics\n",
    "    )\n",
    "    \n",
    "    print(f\"Running prediction on {len(test_tokenized)} samples...\")\n",
    "    predictions_output = trainer.predict(test_tokenized)\n",
    "    \n",
    "    metrics = predictions_output.metrics\n",
    "    \n",
    "    y_preds = np.argmax(predictions_output.predictions, axis=1)\n",
    "    y_true = predictions_output.label_ids\n",
    "    \n",
    "    print(f\"\\nRESULT FOR {shortname.upper()}:\")\n",
    "    print(f\"Accuracy: {metrics['test_accuracy']:.4f}\")\n",
    "    print(f\"F1 Score: {metrics['test_f1']:.4f}\")\n",
    "    print(\"-\" * 30)\n",
    "    \n",
    "    target_names = [k for k, v in sorted(label_mapping.items(), key=lambda item: item[1])]\n",
    "    \n",
    "    print(\"Classification Report:\")\n",
    "    print(classification_report(y_true, y_preds, target_names=target_names))\n",
    "    \n",
    "    test_results[shortname] = metrics\n",
    "\n",
    "    del model\n",
    "    del base_model\n",
    "    del trainer\n",
    "    del tokenizer\n",
    "    gc.collect()\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "print(\"\\nAll Testing Finished!\")\n",
    "print(test_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b89749bc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hf_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
