{
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.11.13",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "kaggle": {
      "accelerator": "nvidiaTeslaT4",
      "dataSources": [
        {
          "sourceId": 13828328,
          "sourceType": "datasetVersion",
          "datasetId": 8806816
        }
      ],
      "dockerImageVersionId": 31193,
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook",
      "isGpuEnabled": true
    },
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "A100"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "605d584358264689b32070cf0f5db65b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_16518ef9c6c1458196461b5319484ba3",
              "IPY_MODEL_66f2cf58531f415ea10673fe81df6ed6",
              "IPY_MODEL_b55fe915d4574213b6c3a51f35d52d1e"
            ],
            "layout": "IPY_MODEL_cb6013415a51473cad21e8b232b0bc0c"
          }
        },
        "16518ef9c6c1458196461b5319484ba3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e84db35f998e49208cf29e0827964378",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_7636d9d22d28406b8ba3a4bb4c3dd8de",
            "value": "Loading‚Äácheckpoint‚Äáshards:‚Äá100%"
          }
        },
        "66f2cf58531f415ea10673fe81df6ed6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_cc88ee8ce13a402188915fa2c44668f8",
            "max": 2,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_122597ca979d46be9c3802357e44750e",
            "value": 2
          }
        },
        "b55fe915d4574213b6c3a51f35d52d1e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_5b91d28114ec410da131e983402ea13d",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_89a4fc9d50ec4424a4965e69c51db8a1",
            "value": "‚Äá2/2‚Äá[00:04&lt;00:00,‚Äá‚Äá1.99s/it]"
          }
        },
        "cb6013415a51473cad21e8b232b0bc0c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e84db35f998e49208cf29e0827964378": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7636d9d22d28406b8ba3a4bb4c3dd8de": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "cc88ee8ce13a402188915fa2c44668f8": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "122597ca979d46be9c3802357e44750e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "5b91d28114ec410da131e983402ea13d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "89a4fc9d50ec4424a4965e69c51db8a1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## Finetuning Llama-3 8B for Related Work Generation\n",
        "\n",
        "This notebook demonstrates the process of finetuning the Llama-3 8B model using Unsloth for the task of generating 'Related Work' sections in academic papers. The process includes data preparation, model loading, training, evaluation, and saving the finetuned model."
      ],
      "metadata": {
        "id": "QzRLhebmUsSe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1. Setup Environment and Install Dependencies\n",
        "\n",
        "Before we begin, we need to install the necessary libraries, including `unsloth` for efficient finetuning, `bitsandbytes` for 4-bit quantization, `transformers`, `trl`, `peft`, `accelerate` for model handling, and `evaluate`, `rouge_score`, `bert_score` for evaluation metrics."
      ],
      "metadata": {
        "id": "u5BbDKOZU0il"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# !pip install \"unsloth[colab-new] @ git+https://github.com/unslothai/unsloth.git\"\n",
        "# !pip install -U bitsandbytes\n",
        "# !pip install --no-deps \"xformers<0.0.27\" \"trl<0.9.0\" peft accelerate\n",
        "# !pip install -U transformers accelerate\n",
        "# !pip install evaluate rouge_score bert_score"
      ],
      "metadata": {
        "id": "eCC3nd1tNAu8"
      },
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip \"/content/processed_multixscience_data.zip\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MKWP7i6DE2DE",
        "outputId": "6c56a017-7265-4de1-89e8-f25640cab293"
      },
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Archive:  /content/processed_multixscience_data.zip\n",
            "replace processed_multixscience_data/data-00000-of-00001.arrow? [y]es, [n]o, [A]ll, [N]one, [r]ename: N\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from datasets import load_from_disk\n",
        "from transformers import AutoTokenizer, TrainingArguments, AutoModelForCausalLM\n",
        "import accelerate\n",
        "from trl import SFTTrainer\n",
        "from unsloth import FastLanguageModel\n",
        "import torch\n",
        "import evaluate\n",
        "from tqdm import tqdm\n",
        "import numpy as np"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-11-22T14:52:41.729641Z",
          "iopub.execute_input": "2025-11-22T14:52:41.730277Z",
          "iopub.status.idle": "2025-11-22T14:52:54.825976Z",
          "shell.execute_reply.started": "2025-11-22T14:52:41.730250Z",
          "shell.execute_reply": "2025-11-22T14:52:54.825293Z"
        },
        "id": "l34f2Z5cEu6K"
      },
      "outputs": [],
      "execution_count": 47
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2. Load and Prepare Dataset\n",
        "\n",
        "We will load a pre-processed dataset from a ZIP archive, which contains academic paper data structured for our 'Related Work' generation task. The dataset is expected to have 'input_text' (abstract + references) and 'related_work' (target summary) columns."
      ],
      "metadata": {
        "id": "NIM7lbFYU60v"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data_path = \"/content/processed_multixscience_data\"\n",
        "\n",
        "print(f\"Memuat dataset dari {data_path}...\")\n",
        "ds = load_from_disk(data_path)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R1qnoeNVNV67",
        "outputId": "8f6c2cc7-0b16-4e1b-cf86-fa39fc9c96f4"
      },
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Memuat dataset dari /content/processed_multixscience_data...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3. Define Prompt Formatting Functions\n",
        "\n",
        "To prepare the dataset for instruction finetuning with Llama-3, we define a prompt template. This template structures the input (`input_text`) and output (`related_work`) into a conversational format suitable for the model, including system and user messages."
      ],
      "metadata": {
        "id": "gymyvFNiVCL-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "EOS_TOKEN = \"<|eot_id|>\"\n",
        "\n",
        "def format_prompt_llama3(examples):\n",
        "    # Ambil list data dari batch\n",
        "    inputs = examples[\"input_text\"]       # Context (Abstract + Refs)\n",
        "    outputs = examples[\"related_work\"]    # Ground Truth (Target)\n",
        "\n",
        "    prompts = []\n",
        "\n",
        "    # System Prompt: Instruksi peran untuk AI\n",
        "    system_msg = (\n",
        "        \"You are an academic writing assistant. \"\n",
        "        \"Write a 'Related Work' section based on the provided text. \"\n",
        "        \"The input contains the Current Abstract followed by References (marked with @cite_n). \"\n",
        "        \"Synthesize these references and highlight the novelty of the Current Abstract.\"\n",
        "    )\n",
        "\n",
        "    for input_text, output_text in zip(inputs, outputs):\n",
        "        text = f\"\"\"<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
        "\n",
        "{system_msg}<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
        "\n",
        "{input_text}<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
        "\n",
        "{output_text}{EOS_TOKEN}\"\"\"\n",
        "\n",
        "        prompts.append(text)\n",
        "\n",
        "    return { \"text\": prompts }\n",
        "\n",
        "def format_prompt_llama3_val(examples):\n",
        "    inputs = examples[\"input_text\"]\n",
        "    outputs = examples[\"related_work\"]\n",
        "\n",
        "    prompts = []\n",
        "\n",
        "    system_msg = (\n",
        "        \"You are an academic writing assistant. \"\n",
        "        \"Write a 'Related Work' section based on the provided text. \"\n",
        "        \"The input contains the Current Abstract followed by References (marked with @cite_n). \"\n",
        "        \"Synthesize these references and highlight the novelty of the Current Abstract.\"\n",
        "    )\n",
        "\n",
        "    for input_text, output_text in zip(inputs, outputs):\n",
        "        text = f\"\"\"<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
        "\n",
        "{system_msg}<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
        "\n",
        "{input_text}<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
        "\"\"\"\n",
        "        prompts.append(text)\n",
        "\n",
        "    return { \"text\": prompts }\n",
        "\n",
        "ds_train, ds_val = ds.train_test_split(test_size=0.2, shuffle=True, seed=3407).values()\n",
        "\n",
        "formatted_ds_train = ds_train.map(format_prompt_llama3, batched=True)\n",
        "formatted_ds_val = ds_val.map(format_prompt_llama3, batched=True)"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-11-22T14:52:54.827280Z",
          "iopub.execute_input": "2025-11-22T14:52:54.827888Z",
          "iopub.status.idle": "2025-11-22T14:52:56.999962Z",
          "shell.execute_reply.started": "2025-11-22T14:52:54.827861Z",
          "shell.execute_reply": "2025-11-22T14:52:56.999164Z"
        },
        "id": "tKVBmvlVEu6N"
      },
      "outputs": [],
      "execution_count": 49
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4. Load and Configure Llama-3 8B Model\n",
        "\n",
        "We load the `unsloth/llama-3-8b-bnb-4bit` model using Unsloth's `FastLanguageModel`. This leverages 4-bit quantization to reduce memory usage and enables efficient LoRA (Low-Rank Adaptation) for finetuning, significantly speeding up the training process."
      ],
      "metadata": {
        "id": "haNab0rkVHbQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "max_seq_length = 1024\n",
        "dtype = None\n",
        "load_in_4bit = True\n",
        "\n",
        "model, tokenizer = FastLanguageModel.from_pretrained(\n",
        "    model_name = \"unsloth/llama-3-8b-bnb-4bit\",\n",
        "    max_seq_length = max_seq_length,\n",
        "    dtype = dtype,\n",
        "    load_in_4bit = load_in_4bit,\n",
        ")\n",
        "\n",
        "target_modules = [\"q_proj\", \"v_proj\"]\n",
        "\n",
        "# Tambahkan Adapter LoRA (Agar model bisa belajar)\n",
        "model = FastLanguageModel.get_peft_model(\n",
        "    model,\n",
        "    r = 16,\n",
        "    target_modules = target_modules,\n",
        "    lora_alpha = 16,\n",
        "    lora_dropout = 0,\n",
        "    bias = \"none\",\n",
        "    use_gradient_checkpointing = \"unsloth\",\n",
        "    random_state = 3407,\n",
        ")\n",
        "\n",
        "print(\"Model Llama 3 8B (4-bit) siap di-train di Kaggle!\")"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-11-22T14:54:59.333275Z",
          "iopub.execute_input": "2025-11-22T14:54:59.333593Z",
          "iopub.status.idle": "2025-11-22T14:55:19.434759Z",
          "shell.execute_reply.started": "2025-11-22T14:54:59.333552Z",
          "shell.execute_reply": "2025-11-22T14:55:19.433587Z"
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rNYYbUwAEu6O",
        "outputId": "c388e3f7-de9e-40e2-8dd0-7ff737842ab7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "==((====))==  Unsloth 2025.11.3: Fast Llama patching. Transformers: 4.57.1.\n",
            "   \\\\   /|    NVIDIA A100-SXM4-80GB. Num GPUs = 1. Max memory: 79.318 GB. Platform: Linux.\n",
            "O^O/ \\_/ \\    Torch: 2.9.0+cu126. CUDA: 8.0. CUDA Toolkit: 12.6. Triton: 3.5.0\n",
            "\\        /    Bfloat16 = TRUE. FA [Xformers = None. FA2 = False]\n",
            " \"-____-\"     Free license: http://github.com/unslothai/unsloth\n",
            "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n",
            "Model Llama 3 8B (4-bit) siap di-train di Kaggle!\n"
          ]
        }
      ],
      "execution_count": 62
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Cek saved model"
      ],
      "metadata": {
        "id": "pmO9ToxZXnLO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define parameters needed for loading from_pretrained\n",
        "max_seq_length = 1024 # This should match the value used during initial model setup\n",
        "dtype = None # This should match the value used during initial model setup\n",
        "load_in_4bit = True # This should match the value used during initial model setup\n",
        "\n",
        "model_found = False\n",
        "try:\n",
        "  # Load save model path\n",
        "  model_path = \"/content/drive/MyDrive/related_works_generation_model\"\n",
        "\n",
        "  # Load the model using FastLanguageModel to ensure Unsloth's patches are applied\n",
        "  loaded_model, loaded_tokenizer = FastLanguageModel.from_pretrained(\n",
        "      model_name = model_path,\n",
        "      max_seq_length = max_seq_length,\n",
        "      dtype = dtype,\n",
        "      load_in_4bit = load_in_4bit,\n",
        "  )\n",
        "  model_found = True\n",
        "  print(f\"Pre-trained model found and loaded from {model_path}.\")\n",
        "except Exception as e:\n",
        "  print(f\"Model not found or error loading: {e}. Proceeding to training a new model...\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 170,
          "referenced_widgets": [
            "605d584358264689b32070cf0f5db65b",
            "16518ef9c6c1458196461b5319484ba3",
            "66f2cf58531f415ea10673fe81df6ed6",
            "b55fe915d4574213b6c3a51f35d52d1e",
            "cb6013415a51473cad21e8b232b0bc0c",
            "e84db35f998e49208cf29e0827964378",
            "7636d9d22d28406b8ba3a4bb4c3dd8de",
            "cc88ee8ce13a402188915fa2c44668f8",
            "122597ca979d46be9c3802357e44750e",
            "5b91d28114ec410da131e983402ea13d",
            "89a4fc9d50ec4424a4965e69c51db8a1"
          ]
        },
        "id": "nSEcxuUDXpWp",
        "outputId": "7d7a4d9f-1ccb-4a3f-97e8-600cce9fa5a1"
      },
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "==((====))==  Unsloth 2025.11.3: Fast Llama patching. Transformers: 4.57.1.\n",
            "   \\\\   /|    NVIDIA A100-SXM4-80GB. Num GPUs = 1. Max memory: 79.318 GB. Platform: Linux.\n",
            "O^O/ \\_/ \\    Torch: 2.9.0+cu126. CUDA: 8.0. CUDA Toolkit: 12.6. Triton: 3.5.0\n",
            "\\        /    Bfloat16 = TRUE. FA [Xformers = None. FA2 = False]\n",
            " \"-____-\"     Free license: http://github.com/unslothai/unsloth\n",
            "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "605d584358264689b32070cf0f5db65b"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Pre-trained model found and loaded from /content/drive/MyDrive/related_works_generation_model.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Training Arguments\n",
        "\n",
        "Di bagian ini, kita akan menentukan argumen pelatihan yang akan digunakan oleh `SFTTrainer`. Argumen ini mencakup ukuran batch, laju pembelajaran, jumlah epoch, strategi logging, dan lainnya. Tujuan utama adalah untuk mengkonfigurasi pelatihan yang efisien dan stabil untuk model bahasa besar (LLM)."
      ],
      "metadata": {
        "id": "O563mZ4SVOip"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "912d6daa",
        "outputId": "0cbf73e8-d992-4cd0-f298-3576cb608b4b"
      },
      "source": [
        "\n",
        "# Define Training Arguments\n",
        "training_args = TrainingArguments(\n",
        "    per_device_train_batch_size = 4,      # A100 kuat\n",
        "    gradient_accumulation_steps = 4,      # total effective batch = 16\n",
        "    warmup_steps = 50,\n",
        "    num_train_epochs = 1,\n",
        "    learning_rate = 2e-4,\n",
        "    fp16 = False,                         # jangan mix fp16 + bf16\n",
        "    bf16 = True,                          # A100 support, lebih stabil\n",
        "    logging_steps = 50,\n",
        "    optim = \"adamw_8bit\",\n",
        "    weight_decay = 0.01,\n",
        "    lr_scheduler_type = \"linear\",\n",
        "    seed = 3407,\n",
        "    output_dir = \"outputs\",\n",
        "    save_strategy=\"epoch\",\n",
        "    push_to_hub=False,\n",
        "    report_to=[],\n",
        ")\n",
        "\n",
        "\n",
        "# Trainer\n",
        "trainer = SFTTrainer(\n",
        "    model=model,\n",
        "    tokenizer=tokenizer,\n",
        "    train_dataset = formatted_ds_train if not model_found else formatted_ds_train.select(range(5)),\n",
        "    eval_dataset  = formatted_ds_val   if not model_found else formatted_ds_val.select(range(5)),\n",
        "    dataset_text_field=\"text\",\n",
        "    max_seq_length=max_seq_length,\n",
        "    args=training_args,\n",
        ")"
      ],
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "num_proc must be <= 5. Reducing num_proc to 5 for dataset of size 5.\n",
            "WARNING:datasets.arrow_dataset:num_proc must be <= 5. Reducing num_proc to 5 for dataset of size 5.\n",
            "num_proc must be <= 5. Reducing num_proc to 5 for dataset of size 5.\n",
            "WARNING:datasets.arrow_dataset:num_proc must be <= 5. Reducing num_proc to 5 for dataset of size 5.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "if model_found:\n",
        "  # If a pre-trained model was found and loaded with FastLanguageModel, use it directly.\n",
        "  # This means we skip the initial FastLanguageModel setup from cell rNYYbUwAEu6O\n",
        "  # and directly use the loaded model and tokenizer for evaluation.\n",
        "  print(\"Using loaded FastLanguageModel for evaluation. Skipping training.\")\n",
        "  model = loaded_model # Assign the loaded model to the 'model' variable for subsequent use\n",
        "  tokenizer = loaded_tokenizer\n",
        "else:\n",
        "  # If no pre-trained model was found, proceed with training the current model (Unsloth PEFT model)\n",
        "  # which was initialized in cell rNYYbUwAEu6O.\n",
        "  print(\"No pre-trained model found. Starting training of the newly configured model.\")\n",
        "  trainer.train()\n",
        "  # After training, the 'model' and 'tokenizer' variables from rNYYbUwAEu6O already hold the trained model."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eByq6d_7W6th",
        "outputId": "9500fede-1b4f-4389-f7d4-bdc15a2fd4be"
      },
      "execution_count": 65,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using loaded FastLanguageModel for evaluation. Skipping training.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 5. Evaluate Model Performance\n",
        "\n",
        "After training, we evaluate the model's performance on a held-out validation set. We generate predictions for a subset of the validation data and compare them against the original 'Related Work' sections using various metrics like ROUGE, BERTScore, and length analysis."
      ],
      "metadata": {
        "id": "e6C6zrgyVUHf"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fd83a4d5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4857c57f-b8db-4c43-9fb8-450118e6196b"
      },
      "source": [
        "import torch\n",
        "\n",
        "model.eval()\n",
        "print(\"Model set to evaluation mode.\")\n",
        "\n",
        "eval_dataset = formatted_ds_val.select(range(200))\n",
        "print(f\"Selected {len(eval_dataset)} samples for evaluation.\")\n",
        "\n",
        "# 1. Definisikan Config \"Obat Repetisi\"\n",
        "generation_kwargs = {\n",
        "    \"max_new_tokens\": 150,\n",
        "    \"min_length\": 60,\n",
        "    \"num_beams\": 1,\n",
        "    \"do_sample\": True,\n",
        "    \"temperature\": 0.1,\n",
        "    \"top_p\": 0.9,\n",
        "    \"no_repeat_ngram_size\": 3,\n",
        "    \"repetition_penalty\": 1.2,\n",
        "}\n",
        "\n",
        "# 3. Define the generation function\n",
        "def generate_related_work(input_text):\n",
        "    # Ensure pad_token_id is set for the tokenizer to prevent reorder_cache error\n",
        "    if tokenizer.pad_token_id is None:\n",
        "        tokenizer.pad_token_id = tokenizer.eos_token_id\n",
        "\n",
        "    inputs = tokenizer(input_text, return_tensors=\"pt\", add_special_tokens=False)\n",
        "    inputs = {k: v.to(model.device) for k, v in inputs.items()}\n",
        "\n",
        "  # 2. Jalankan Generate\n",
        "    with torch.no_grad():\n",
        "        outputs = model.generate(\n",
        "            **inputs,                         # Unpack input_ids & attention_mask\n",
        "            **generation_kwargs,              # Unpack config di atas\n",
        "            use_cache=True,\n",
        "            eos_token_id=tokenizer.eos_token_id,\n",
        "            pad_token_id=tokenizer.pad_token_id, # Gunakan pad token yang benar\n",
        "        )\n",
        "\n",
        "    generated_tokens = outputs[0][inputs[\"input_ids\"].shape[1]:]\n",
        "    decoded_output = tokenizer.decode(generated_tokens, skip_special_tokens=True)\n",
        "\n",
        "    return decoded_output.strip()\n",
        "\n",
        "predictions = []\n",
        "original_texts = []\n",
        "print(\"Generating predictions...\")\n",
        "for i, sample in enumerate(eval_dataset):\n",
        "    generated_text = generate_related_work(sample[\"input_text\"])\n",
        "    predictions.append(generated_text)\n",
        "    original_texts.append(sample[\"related_work\"])\n",
        "    if (i + 1) % 10 == 0:\n",
        "        print(f\"Generated prediction for {i + 1}/{len(eval_dataset)} samples.\")\n",
        "\n",
        "print(\"Prediction generation complete.\")\n",
        "print(f\"Generated {len(predictions)} predictions.\")\n",
        "\n",
        "print(\"\\n--- First Generated Prediction ---\")\n",
        "print(predictions[0])\n",
        "print(\"\\n--- First Original Related Work ---\")\n",
        "print(original_texts[0])"
      ],
      "execution_count": 76,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model set to evaluation mode.\n",
            "Selected 200 samples for evaluation.\n",
            "Generating predictions...\n",
            "Generated prediction for 10/200 samples.\n",
            "Generated prediction for 20/200 samples.\n",
            "Generated prediction for 30/200 samples.\n",
            "Generated prediction for 40/200 samples.\n",
            "Generated prediction for 50/200 samples.\n",
            "Generated prediction for 60/200 samples.\n",
            "Generated prediction for 70/200 samples.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Unsloth: Input IDs of shape torch.Size([1, 1078]) with length 1078 > the model's max sequence length of 1024.\n",
            "We shall truncate it ourselves. It's imperative if you correct this issue first.\n",
            "Unsloth: Input IDs of shape torch.Size([1, 1045]) with length 1045 > the model's max sequence length of 1024.\n",
            "We shall truncate it ourselves. It's imperative if you correct this issue first.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generated prediction for 80/200 samples.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Unsloth: Input IDs of shape torch.Size([1, 1062]) with length 1062 > the model's max sequence length of 1024.\n",
            "We shall truncate it ourselves. It's imperative if you correct this issue first.\n",
            "Unsloth: Input IDs of shape torch.Size([1, 1315]) with length 1315 > the model's max sequence length of 1024.\n",
            "We shall truncate it ourselves. It's imperative if you correct this issue first.\n",
            "Unsloth: Input IDs of shape torch.Size([1, 1380]) with length 1380 > the model's max sequence length of 1024.\n",
            "We shall truncate it ourselves. It's imperative if you correct this issue first.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generated prediction for 90/200 samples.\n",
            "Generated prediction for 100/200 samples.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Unsloth: Input IDs of shape torch.Size([1, 1172]) with length 1172 > the model's max sequence length of 1024.\n",
            "We shall truncate it ourselves. It's imperative if you correct this issue first.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generated prediction for 110/200 samples.\n",
            "Generated prediction for 120/200 samples.\n",
            "Generated prediction for 130/200 samples.\n",
            "Generated prediction for 140/200 samples.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Unsloth: Input IDs of shape torch.Size([1, 1505]) with length 1505 > the model's max sequence length of 1024.\n",
            "We shall truncate it ourselves. It's imperative if you correct this issue first.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generated prediction for 150/200 samples.\n",
            "Generated prediction for 160/200 samples.\n",
            "Generated prediction for 170/200 samples.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Unsloth: Input IDs of shape torch.Size([1, 1059]) with length 1059 > the model's max sequence length of 1024.\n",
            "We shall truncate it ourselves. It's imperative if you correct this issue first.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generated prediction for 180/200 samples.\n",
            "Generated prediction for 190/200 samples.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Unsloth: Input IDs of shape torch.Size([1, 1339]) with length 1339 > the model's max sequence length of 1024.\n",
            "We shall truncate it ourselves. It's imperative if you correct this issue first.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generated prediction for 200/200 samples.\n",
            "Prediction generation complete.\n",
            "Generated 200 predictions.\n",
            "\n",
            "--- First Generated Prediction ---\n",
            "<doc-sep> @article{2016arXiv160705690B, author = {Bojanowski, Piotr and Grave, Edouard and Joulin, Armand and Mikolov, Tomas}, title = {{Enriching Word Vectors With Subword Information}}, year = 2016} @inproceedings{DBLP:journals/corr/ChenWY16a, author={Zichao Yang and Di Wang and Edward Y. Chang}, howpublished=\"{{\\textcopyright}} Z.Yang, D.Wang, E.Chang, 2015.\", journal=\"{\\textbackslash }jmlr\", month=jun #, note=\"Submitted on 15 Jun\n",
            "\n",
            "--- First Original Related Work ---\n",
            "Firstly, previous datasets in this area are not yet released or in their infancy for verification of their applicability as abuse ground truth gold standard. The authors of @cite_14 claim to outperform deep learning techniques to detect hate speech, derogatory language and profanity. They compare their results with a previous dataset from @cite_12 and assess the accuracy of detecting abusive language with distributional semantic features to find out that it does largely depends upon the evolution of the content that abusers post in the platform or else having to retrain the model.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 6. Calculate Comprehensive Evaluation Metrics\n",
        "\n",
        "This section defines a function to compute ROUGE scores (for lexical similarity), BERTScore (for semantic similarity), and analyze the length of generated texts compared to the references. These metrics provide a holistic view of the model's generation quality."
      ],
      "metadata": {
        "id": "yVXMtZ54Vdyh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def calculate_comprehensive_metrics(predictions, references):\n",
        "    \"\"\"\n",
        "    Menghitung ROUGE, BERTScore, dan Rasio Panjang.\n",
        "\n",
        "    Args:\n",
        "        predictions (list): List string hasil output model.\n",
        "        references (list): List string kunci jawaban asli (ground truth).\n",
        "\n",
        "    Returns:\n",
        "        dict: Dictionary berisi semua skor evaluasi.\n",
        "    \"\"\"\n",
        "\n",
        "    print(f\"üìä Memulai Evaluasi untuk {len(predictions)} sampel data...\")\n",
        "    results = {}\n",
        "\n",
        "    # --- 1. ROUGE SCORE (Lexical / Kata) ---\n",
        "    print(\"‚è≥ Menghitung ROUGE...\")\n",
        "    rouge_metric = evaluate.load(\"rouge\")\n",
        "    rouge_scores = rouge_metric.compute(\n",
        "        predictions=predictions,\n",
        "        references=references,\n",
        "        use_stemmer=True # Penting untuk bahasa Inggris\n",
        "    )\n",
        "    # Konversi ke Persen (0-100)\n",
        "    results['ROUGE-1'] = round(rouge_scores['rouge1'] * 100, 2)\n",
        "    results['ROUGE-2'] = round(rouge_scores['rouge2'] * 100, 2)\n",
        "    results['ROUGE-L'] = round(rouge_scores['rougeL'] * 100, 2)\n",
        "\n",
        "    # --- 2. BERTSCORE (Semantic / Makna) ---\n",
        "    print(\"‚è≥ Menghitung BERTScore (Mungkin butuh waktu & download model)...\")\n",
        "    bertscore_metric = evaluate.load(\"bertscore\")\n",
        "    # Gunakan batch_size agar tidak OOM\n",
        "    bert_scores = bertscore_metric.compute(\n",
        "        predictions=predictions,\n",
        "        references=references,\n",
        "        lang=\"en\",\n",
        "        batch_size=16\n",
        "    )\n",
        "    # Kita ambil rata-rata F1 Score dari semua data\n",
        "    results['BERTScore-F1'] = round(np.mean(bert_scores['f1']) * 100, 2)\n",
        "    results['BERTScore-Precision'] = round(np.mean(bert_scores['precision']) * 100, 2)\n",
        "    results['BERTScore-Recall'] = round(np.mean(bert_scores['recall']) * 100, 2)\n",
        "\n",
        "    # --- 3. LENGTH ANALYSIS (Analisis Panjang) ---\n",
        "    print(\"‚è≥ Menghitung Statistik Panjang Teks...\")\n",
        "    pred_lens = [len(p.split()) for p in predictions]\n",
        "    ref_lens = [len(r.split()) for r in references]\n",
        "\n",
        "    avg_pred_len = np.mean(pred_lens)\n",
        "    avg_ref_len = np.mean(ref_lens)\n",
        "    length_ratio = (avg_pred_len / avg_ref_len) * 100\n",
        "\n",
        "    results['Avg Gen Length'] = round(avg_pred_len, 1)\n",
        "    results['Avg Ref Length'] = round(avg_ref_len, 1)\n",
        "    results['Length Ratio (%)'] = round(length_ratio, 2)\n",
        "\n",
        "    return results\n",
        "\n",
        "# Jalankan Fungsi\n",
        "final_metrics = calculate_comprehensive_metrics(predictions, original_texts)\n",
        "\n",
        "# Tampilkan Hasil Rapih\n",
        "print(\"\\n\" + \"=\"*40)\n",
        "print(\"      LAPORAN HASIL EVALUASI AKHIR      \")\n",
        "print(\"=\"*40)\n",
        "for metric, score in final_metrics.items():\n",
        "    print(f\"{metric:<20} : {score}\")\n",
        "print(\"=\"*40)\n",
        "\n",
        "# Interpretasi Singkat\n",
        "print(\"\\n--- Interpretasi Cepat ---\")\n",
        "if final_metrics['BERTScore-F1'] > 85:\n",
        "    print(\"Kualitas Makna SANGAT BAIK (Mirip manusia).\")\n",
        "elif final_metrics['BERTScore-F1'] > 80:\n",
        "    print(\"Kualitas Makna CUKUP BAIK.\")\n",
        "else:\n",
        "    print(\"Kualitas Makna KURANG (Model mungkin halusinasi/tidak nyambung).\")\n",
        "\n",
        "if final_metrics['Length Ratio (%)'] < 80:\n",
        "    print(\"WARNING: Output model terlalu pendek dibanding referensi asli.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X6HcXxVmEP65",
        "outputId": "bfe17f9c-fea0-401c-dec7-c92826959644"
      },
      "execution_count": 77,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üìä Memulai Evaluasi untuk 200 sampel data...\n",
            "‚è≥ Menghitung ROUGE...\n",
            "‚è≥ Menghitung BERTScore (Mungkin butuh waktu & download model)...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚è≥ Menghitung Statistik Panjang Teks...\n",
            "\n",
            "========================================\n",
            "      LAPORAN HASIL EVALUASI AKHIR      \n",
            "========================================\n",
            "ROUGE-1              : 16.93\n",
            "ROUGE-2              : 1.31\n",
            "ROUGE-L              : 9.1\n",
            "BERTScore-F1         : 80.38\n",
            "BERTScore-Precision  : 79.94\n",
            "BERTScore-Recall     : 80.87\n",
            "Avg Gen Length       : 93.2\n",
            "Avg Ref Length       : 106.2\n",
            "Length Ratio (%)     : 87.8\n",
            "========================================\n",
            "\n",
            "--- Interpretasi Cepat ---\n",
            "Kualitas Makna CUKUP BAIK.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 7. Perform Quality Check with Sample Outputs\n",
        "\n",
        "To get a qualitative understanding of the model's performance, we randomly select a few samples from the validation set and display their generated 'Related Work' alongside the original reference text. This helps in visually inspecting the coherence and relevance of the generated content."
      ],
      "metadata": {
        "id": "QUyACmZMVjdr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Ambil 3 sampel acak\n",
        "import random\n",
        "indices = random.sample(range(len(predictions)), 3)\n",
        "\n",
        "print(\"=== QUALITY CHECK ===\")\n",
        "for i in indices:\n",
        "    print(f\"\\n[SAMPEL {i}]\")\n",
        "    print(\"GENERATED:\", predictions[i])\n",
        "    print(\"-\" * 20)\n",
        "    print(\"REFERENCE:\", original_texts[i])\n",
        "    print(\"=\" * 40)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xclerqq4F0cx",
        "outputId": "170c43cf-7bc7-4877-bcab-cdcc7c7b6a34"
      },
      "execution_count": 78,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=== QUALITY CHECK ===\n",
            "\n",
            "[SAMPEL 14]\n",
            "GENERATED: <div> </div><div> Abstract‚ÄîIn recent years, there has been increasing interest in generating stylized illustrations using deep generative networks (DGNs). While DGN-based approaches have shown promising performance in synthesizing realistic-looking illustrations, they typically require massive amounts of training samples to achieve satisfactory quality. Moreover, most previous works focus only on single-image generation tasks without considering the spatial relationships among multiple objects or regions.</div></div>, <div><h3>References</h3><ul><li>A. Dosovitskiy, G. Larsson, E. Sohn, H. Wang, J.Y. Zhu, C.L. Zitnick, M. Fathy, B. Chandrasekhar,\n",
            "--------------------\n",
            "REFERENCE: Due to the popularity of comics, many related research topics, such as @cite_31 , @cite_25 and @cite_13 have drawn considerable research attention in computer graphics community. Particularly, several techniques have been studied to facilitate layout generation. For example, Arai @cite_35 and Pang @cite_26 studied how to automatically extract each panel from e-comics; and display e-comics on different devices. In order to convert conversational videos to comics, Jing @cite_27 made use of a rule based optimization scheme for layout generation. Cao @cite_17 presented a generative probabilistic framework to arrange input artworks into a manga page, and then used optimization techniques to refine it. Furthermore, Cao @cite_33 took text balloons and picture subjects into consideration for manga layout generation and guided the reader's attention. However, in our poster generation, one has to consider both texts and graphical elements composition within each panel, which has not been discussed previously.\n",
            "========================================\n",
            "\n",
            "[SAMPEL 107]\n",
            "GENERATED: sequences with variable length. However, despite its success at learning complex patterns over time, LSTM does not explicitly model event timing or duration. To address this issue, we present an extension of LSTM that incorporates explicit representations of elapsed time into each unit's computation. Our approach is based on two key ideas: first, we add a new gate to LSTM units that controls how much attention they pay to recent inputs; second, we modify the update rule so that the value stored in each unit depends on the difference between current and previous values rather than their sum. This allows us to represent durations directly within the network without having to rely solely upon gradients backpropagated through time. Experimental evaluation shows that our modified version of LSTM significantly improves performance compared to standard implementations\n",
            "--------------------\n",
            "REFERENCE: Highway LSTMs @cite_14 and depth-gated LSTMs @cite_4 are similar to our proposed models in that they use cell states from the previous layer, and they are successfully applied to the field of automatic speech recognition and language modeling. However in contrast to CAS-LSTM, where the additional forget gate aggregates the previous layer states, and thus contexts from the left and below participate in computation equitably, in Highway LSTMs and depth-gated LSTMs the previous layer states are considered only through peephole connections @cite_39 . The comparison of our models and this architecture is presented in .\n",
            "========================================\n",
            "\n",
            "[SAMPEL 33]\n",
            "GENERATED: models do not accurately reflect the behavior of humans or vehicles. In particular, they fail to reproduce the fact that people tend to move between well-defined areas (communities) with strong ties among them, whereas their movements within these areas can vary significantly over time. This paper proposes an extension of the Random Waypoint Mobility Model called Community-based Movement Model (CBMM). CBMM allows modeling of complex patterns of human and vehicle movements based on two parameters: the size of the area covered during each trip and the probability of moving from one place to another. It also incorporates a mechanism for generating random trips inside a given region. Simulation results indicate that CBMM provides more accurate representations than other existing models when compared against real data sets. Furthermore, CBMM has\n",
            "--------------------\n",
            "REFERENCE: As a final note, in @cite_13 , the authors assume the attraction of a community (i.e., a geographical area) to a mobile node is derived from the number of friends of this node currently residing in the community. In our paper we assume that the nodes make movement decisions independently of the others (nonetheless, node sharing the same community will exhibit mobility correlation, capturing the social feature indirectly). Mobility models with inter-node dependency require a solid understanding of the social network structure, which is an important area under development. We plan to work further in this direction in the future.\n",
            "========================================\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 8. Save the Finetuned Model\n",
        "\n",
        "Finally, we merge the LoRA adapters with the base model and save the complete finetuned model and its tokenizer to a specified path. This allows us to reuse the model for inference without needing the LoRA configuration again."
      ],
      "metadata": {
        "id": "II5Q7ZfAVobt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "presaved_model = model.merge_and_unload()\n",
        "\n",
        "save_path = \"/content/drive/MyDrive/related_works_generation_model_v2\"\n",
        "\n",
        "# Save full merged model\n",
        "presaved_model.save_pretrained(save_path)\n",
        "tokenizer.save_pretrained(save_path)\n",
        "\n",
        "print(f\"Model saved to {save_path}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 332
        },
        "id": "oaEYo8OpG9-P",
        "outputId": "a485909e-685b-4da8-b6f9-10fdf901fce9"
      },
      "execution_count": 79,
      "outputs": [
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "'LlamaForCausalLM' object has no attribute 'merge_and_unload'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-2271049058.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mpresaved_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmerge_and_unload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0msave_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"/content/drive/MyDrive/related_works_generation_model_v2\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# Save full merged model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   1962\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmodules\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1963\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mmodules\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1964\u001b[0;31m         raise AttributeError(\n\u001b[0m\u001b[1;32m   1965\u001b[0m             \u001b[0;34mf\"'{type(self).__name__}' object has no attribute '{name}'\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1966\u001b[0m         )\n",
            "\u001b[0;31mAttributeError\u001b[0m: 'LlamaForCausalLM' object has no attribute 'merge_and_unload'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "a3a4SlZQYXHF"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}