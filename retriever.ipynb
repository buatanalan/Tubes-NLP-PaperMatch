{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "614971f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Iskandar\\Documents\\GitHub\\Tubes-NLP-PaperMatch\\hf_env\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Struktur Dataset:\n",
      "Train: Dataset({\n",
      "    features: ['paper_id', 'version', 'yymm', 'created', 'title', 'secondary_subfield', 'abstract', 'primary_subfield', 'fulltext', 'num_latex', 'text_len', 'clean_abstract', 'final_text', 'label'],\n",
      "    num_rows: 86941\n",
      "})\n",
      "Test : Dataset({\n",
      "    features: ['paper_id', 'version', 'yymm', 'created', 'title', 'secondary_subfield', 'abstract', 'primary_subfield', 'fulltext', 'num_latex', 'text_len', 'clean_abstract', 'final_text', 'label'],\n",
      "    num_rows: 21736\n",
      "})\n",
      "\n",
      "5 Data Pertama (Train)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>paper_id</th>\n",
       "      <th>version</th>\n",
       "      <th>yymm</th>\n",
       "      <th>created</th>\n",
       "      <th>title</th>\n",
       "      <th>secondary_subfield</th>\n",
       "      <th>abstract</th>\n",
       "      <th>primary_subfield</th>\n",
       "      <th>fulltext</th>\n",
       "      <th>num_latex</th>\n",
       "      <th>text_len</th>\n",
       "      <th>clean_abstract</th>\n",
       "      <th>final_text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1209.2298</td>\n",
       "      <td>1</td>\n",
       "      <td>1209</td>\n",
       "      <td>2012-09-11 12:18:56</td>\n",
       "      <td>The Future Has Thicker Tails than the Past: Mo...</td>\n",
       "      <td>[q-fin.RM, stat.AP]</td>\n",
       "      <td>Ex ante forecast outcomes should be interprete...</td>\n",
       "      <td>q-fin.RM</td>\n",
       "      <td>The Future Has Thicker Tails than the Past: \\n...</td>\n",
       "      <td>0</td>\n",
       "      <td>1243</td>\n",
       "      <td>ante forecast outcome interpreted counterfactu...</td>\n",
       "      <td>ante forecast outcome interpreted counterfactu...</td>\n",
       "      <td>17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1607.06587</td>\n",
       "      <td>1</td>\n",
       "      <td>1607</td>\n",
       "      <td>2016-07-22 08:00:55</td>\n",
       "      <td>3D Character Customization for Non-Professiona...</td>\n",
       "      <td>[cs.HC]</td>\n",
       "      <td>In gaming, customizing individual characters, ...</td>\n",
       "      <td>cs.HC</td>\n",
       "      <td>3D Character Customization for Non-Professiona...</td>\n",
       "      <td>0</td>\n",
       "      <td>1050</td>\n",
       "      <td>gaming customizing individual character create...</td>\n",
       "      <td>gaming customizing individual character create...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1211.7294</td>\n",
       "      <td>1</td>\n",
       "      <td>1211</td>\n",
       "      <td>2012-11-30 15:53:49</td>\n",
       "      <td>Copper passivation procedure for water-filled ...</td>\n",
       "      <td>[physics.ins-det]</td>\n",
       "      <td>In the framework of the European research proj...</td>\n",
       "      <td>physics.ins-det</td>\n",
       "      <td>Copper passivation procedure for water-filled ...</td>\n",
       "      <td>0</td>\n",
       "      <td>1746</td>\n",
       "      <td>framework european research project meteomet l...</td>\n",
       "      <td>framework european research project meteomet l...</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1802.05865</td>\n",
       "      <td>1</td>\n",
       "      <td>1802</td>\n",
       "      <td>2018-02-16 08:38:06</td>\n",
       "      <td>Statistical Analysis of Metrics for Software Q...</td>\n",
       "      <td>[cs.SE]</td>\n",
       "      <td>Software product quality can be defined as the...</td>\n",
       "      <td>cs.SE</td>\n",
       "      <td>STATISTICAL ANALYSIS OF METRICS FOR \\n\\nSOFTWA...</td>\n",
       "      <td>0</td>\n",
       "      <td>997</td>\n",
       "      <td>software product quality defined feature chara...</td>\n",
       "      <td>software product quality defined feature chara...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1709.05483</td>\n",
       "      <td>2</td>\n",
       "      <td>1709</td>\n",
       "      <td>2017-10-19 13:40:13</td>\n",
       "      <td>sPIN: High-performance streaming Processing in...</td>\n",
       "      <td>[cs.DC]</td>\n",
       "      <td>Optimizing communication performance is impera...</td>\n",
       "      <td>cs.DC</td>\n",
       "      <td>sPIN: High-performance streaming Processing in...</td>\n",
       "      <td>0</td>\n",
       "      <td>1145</td>\n",
       "      <td>optimizing communication imperative large scal...</td>\n",
       "      <td>optimizing communication imperative large scal...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     paper_id version  yymm             created  \\\n",
       "0   1209.2298       1  1209 2012-09-11 12:18:56   \n",
       "1  1607.06587       1  1607 2016-07-22 08:00:55   \n",
       "2   1211.7294       1  1211 2012-11-30 15:53:49   \n",
       "3  1802.05865       1  1802 2018-02-16 08:38:06   \n",
       "4  1709.05483       2  1709 2017-10-19 13:40:13   \n",
       "\n",
       "                                               title   secondary_subfield  \\\n",
       "0  The Future Has Thicker Tails than the Past: Mo...  [q-fin.RM, stat.AP]   \n",
       "1  3D Character Customization for Non-Professiona...              [cs.HC]   \n",
       "2  Copper passivation procedure for water-filled ...    [physics.ins-det]   \n",
       "3  Statistical Analysis of Metrics for Software Q...              [cs.SE]   \n",
       "4  sPIN: High-performance streaming Processing in...              [cs.DC]   \n",
       "\n",
       "                                            abstract primary_subfield  \\\n",
       "0  Ex ante forecast outcomes should be interprete...         q-fin.RM   \n",
       "1  In gaming, customizing individual characters, ...            cs.HC   \n",
       "2  In the framework of the European research proj...  physics.ins-det   \n",
       "3  Software product quality can be defined as the...            cs.SE   \n",
       "4  Optimizing communication performance is impera...            cs.DC   \n",
       "\n",
       "                                            fulltext  num_latex  text_len  \\\n",
       "0  The Future Has Thicker Tails than the Past: \\n...          0      1243   \n",
       "1  3D Character Customization for Non-Professiona...          0      1050   \n",
       "2  Copper passivation procedure for water-filled ...          0      1746   \n",
       "3  STATISTICAL ANALYSIS OF METRICS FOR \\n\\nSOFTWA...          0       997   \n",
       "4  sPIN: High-performance streaming Processing in...          0      1145   \n",
       "\n",
       "                                      clean_abstract  \\\n",
       "0  ante forecast outcome interpreted counterfactu...   \n",
       "1  gaming customizing individual character create...   \n",
       "2  framework european research project meteomet l...   \n",
       "3  software product quality defined feature chara...   \n",
       "4  optimizing communication imperative large scal...   \n",
       "\n",
       "                                          final_text  label  \n",
       "0  ante forecast outcome interpreted counterfactu...     17  \n",
       "1  gaming customizing individual character create...      2  \n",
       "2  framework european research project meteomet l...     15  \n",
       "3  software product quality defined feature chara...      2  \n",
       "4  optimizing communication imperative large scal...      2  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Contoh Raw Data ke-0\n",
      "{'paper_id': '1209.2298', 'version': '1', 'yymm': '1209', 'created': datetime.datetime(2012, 9, 11, 12, 18, 56), 'title': 'The Future Has Thicker Tails than the Past: Model Error As Branching Counterfactuals', 'secondary_subfield': ['q-fin.RM', 'stat.AP'], 'abstract': 'Ex ante forecast outcomes should be interpreted as counterfactuals (potential histories), with errors as the spread between outcomes. Reapplying measurements of uncertainty about the estimation errors of the estimation errors of an estimation leads to branching counterfactuals. Such recursions of epistemic uncertainty have markedly different distributial properties from conventional sampling error. Nested counterfactuals of error rates invariably lead to fat tails, regardless of the probability distribution used, and to powerlaws under some conditions. A mere .01% branching error rate about the STD (itself an error rate), and .01% branching error rate about that error rate, etc. (recursing all the way) results in explosive (and infinite) higher moments than 1. Missing any degree of regress leads to the underestimation of small probabilities and concave payoffs (a standard example of which is Fukushima). The paper states the conditions under which higher order rates of uncertainty (expressed in spreads of counterfactuals) alters the shapes the of final distribution and shows which a priori beliefs about conterfactuals are needed to accept the reliability of conventional probabilistic methods (thin tails or mildly fat tails).', 'primary_subfield': 'q-fin.RM', 'fulltext': 'The Future Has Thicker Tails than the Past: \\nModel Error As Branching Counterfactuals  \\nNassim N Taleb\\nNYU-Poly Institute\\nSeptember 2012, 2nd version\\nINITIALLY WRITTEN  IN HONOR OF BENOIT MANDELBROT’S FOR HIS SCIENTIFIC MEMORIAL\\n Yale University, APRIL 29, 2011\\n\\nAbstract\\nEx  ante  forecast  outcomes  should  be  interpreted  as  counterfactuals  (potential  histories),  with\\nerrors  as  the  spread  between  outcomes.  Reapplying  measurements  of  uncertainty  about  the\\nestimation  errors  of  the  estimation  errors  of  an  estimation  leads  to  branching  counterfactuals.\\nSuch  recursions  of  epistemic  uncertainty  have  markedly  different  distributial  properties  from\\nconventional  sampling  error.  Nested  counterfactuals  of  error  rates  invariably  lead  to  fat  tails,\\nregardless  of  the  probability  distribution  used,  and  to  powerlaws  under  some  conditions.  A\\nmere  .01%  branching  error  rate  about  the  STD  (itself  an  error  rate),  and  .01%  branching  error\\nrate  about  that  error  rate,  etc.  (recursing  all  the  way)  results  in  explosive  (and  infinite)  higher\\nmoments  than  1.  Missing  any  degree  of  regress  leads  to  the  underestimation  of  small\\nprobabilities  and  concave  payoffs  (a  standard  example  of  which  is  Fukushima).    The  paper\\nstates  the  conditions  under  which  higher  order  rates  of  uncertainty  (expressed  in  spreads  of\\ncounterfactuals)  alters  the  shapes  the  of  final  distribution  and  shows  which  a  priori  beliefs\\nabout  conterfactuals  are  needed  to  accept  the  reliability  of  conventional  probabilistic  methods\\n(thin tails or mildly fat tails). \\n\\nKEYWORDS:  Fukushima,  Counterfactual  histories,  Risk  management,  Epistemology  of  probability,  Model  errors,  Fragility  and\\nAntifragility, Fourth Quadrant\\n\\nIntroduction\\nIntuition.  An  event  has  never  shown  in  past  samples;  we  are  told  that  it  was  “estimated”  as  having  zero  probability.  But  an  estima-\\ntion has to have an error rate; only measures deemed a priori or fallen from the sky and dictated by some infaillible deity can escape\\nsuch  error.  Since  probabilities  cannot  be  negative,  the  estimation  error  will  necessarily  put  a  lower  bound  on  it  and  make  the\\nprobability > 0. \\nThis,  in  a  nutschell,  is  how  we  should  treat  the  convexity  bias  stemming  from  uncertainty  about  small  probabilities.  Using  the  same\\nreasoning,  we  need  to  increase  the  raw  “estimation”  of  small  probabilities.  There  can  be  uncertainty  about  the  relationship  between\\npast  samples  and  future  ones,  or,  more  philosophically,  from  the  problem  of  induction.    Doubting  the  reliability  of  the  methods  used\\nto  produce  these  probabilities,    or  doubting  beliefs  about  the  future  resembling  the  past  will  lend  us  to  exercise  standard  skepticism\\nand  consider  a  spate  of  different  alternative  future  outcomes.    The  small  probability  event  will  necessarily  have,  in  expectation,  i.e.,\\non  average  across  all  potential  future  histories,  a  higher  than  what  was  measured.  The  increase  in  the  probability  will  be  commensu-\\nrate  with  the  error  rate  in  the  estimation.  It,  simply,  results  from  the  convexity  bias  that  makes  small  probabilities  rise  when  we  are\\nuncertain about them. Accordingly, the future needs to be dealt with as having thicker tails than what was measured in the past.\\nIncoherence  in  Probabilistic  Measurements.  Just  as  \"estimating\"  an  event  to  be  of  measure  0  is  incoherent,  it  is  equally  inconsis-\\ntent  to  estimate  anything  without  introducing  an  estimation  error  in  the  analysis  and  adding  a  convexity  bias  (positive  or  negative).\\nBut  this  incoherence  (or  confusion  between  estimated  and  a  priori)  pervades  the  economics  literature  whenever  probabilistic  and\\nstatistical  methods  are  used.    For  instance,  the  highest  use  of  probabiltiy  in  modern  financial  economics  resulting  from  the  seminal\\nMarkowitz  (1952)  has  his  derivations  starting  with  assuming  E  and  V  (expectation  and  variance).  At  the  end  of  the  paper  he  states\\nthat these parameters need to be estimated. Injecting an estimation error in the analysis entirely cancel the  derivations of the paper as\\nthey are based on immutable certainties.\\nRegressing Counterfactuals\\nWe  can  go  beyond  probabilities  and,  in  place  of  probabilities,  perturbate  parameters  of  probability  distributions  used  in  practice  and\\nperturbate  the  rates  of  perturbation.  So  this  paper  introduces  two  notions:  treating  errors  as  branching  counterfactual  histories  and\\nregressing  (i.e.,  compounding)  the  error  rates.  An  error  rate  about  a  forecast  can  be  estimated  (or,  of  course,  \"guessed\").  The\\nestimation (or \"guess\"), in turn, will have an error rate. The estimation of such error rate will have an error rate. (The forecast can be\\nan economic variable, the future rainfall in Brazil, or the damage from  a nuclear accident).  \\n\\n\\x0c2  Nassim N. Taleb\\n\\nWe  can  go  beyond  probabilities  and,  in  place  of  probabilities,  perturbate  parameters  of  probability  distributions  used  in  practice  and\\nperturbate  the  rates  of  perturbation.  So  this  paper  introduces  two  notions:  treating  errors  as  branching  counterfactual  histories  and\\nregressing  (i.e.,  compounding)  the  error  rates.  An  error  rate  about  a  forecast  can  be  estimated  (or,  of  course,  \"guessed\").  The\\nestimation (or \"guess\"), in turn, will have an error rate. The estimation of such error rate will have an error rate. (The forecast can be\\nan economic variable, the future rainfall in Brazil, or the damage from  a nuclear accident).  \\nWhat is called a regress argument by philosophers can be used to put some scrutiny on  quantitative methods or risk and probability.\\nThe  mere  existence  of  such  regress  argument  will  lead  to  series  of  branching  counterfactuals  three  different  regimes,  two  of  which\\nlead  to  the  necessity  to  raise  the  values  of  small  probabilities,  and  one  of  them  to  the  necessity  to  use  power  law  distributions.  This\\nstudy of the structures of the error rates refines the analysis of the  Fourth Quadrant (Taleb, 2008) setting the limit of the possibility\\nof the use of probabilistic methods (and their reliability in the decision-making), based on errors in the tails of the distribution.\\nSo the boundary between the regimes is what this paper is about -what assumptions one needs to have set beforehand to avoid radical\\nskepticism and which specific a priori undefeasable beliefs are necessary to hold for that. In other words someone using probabilistic\\nestimates  should  tell  us  beforehand  which  immutable  certainties  are  built  into  his  representation,  and  what  should  be  subjected  to\\nerror  -and  regress  --otherwise  they  risk  falling  into  a  certain  form  of  incoherence:  if  a  parameter  is  estimated,  second  order  effects\\nneed to be considered.\\nThis paper can also help setting a wedge between forecasting and statistical estimation.\\n\\nThe Regress Argument (Error about Error)\\nThe main problem behind The Black Swan  is the limited understanding of model (or representation) error, and, for those who get it, a\\nlack  of  understanding  of  second  order  errors  (about  the  methods  used  to  compute  the  errors)  and  by  a  regress  argument,  an  inability\\nto continuously reapplying the thinking all the way to its limit (particularly when they provide no reason to stop).  Again, I have no\\nproblem  with  stopping  the  recursion,  provided  it  is  accepted  as  a  declared  a  priori  that  escapes  quantitative  and  statistical  methods.\\nAlso,  few  get  the  point  that  the  skepticism  in  The  Back  Swan  it  does  not  invalidate  all  measurements  of  probability;  its  value  lies  in\\nshowing a map of domains that are vulnerable to such opacity,  defining these domains based on their propensity to fat-tailedness (of\\nend  outcomes),  sensitivity  to  convexity  effects,  and  building  robustness  (i.e.,  mitigation  of  tail  effects)  by  appropriate  decision-\\nmaking rules. \\nPhilosophers  and  Regress  Arguments:  I  was  having  a  conversation  with  the  philosopher  Paul  Boghossian  about  errors  in  the\\nassumptions  of  a  model  (or  its  structure)  not  being  customarily  included  back  into  the  model  itself  when  I  realized  that  only  a\\nphilosopher  can  understand  a  problem  to  which  the  entire  quantitative  field  seems  blind.  For  instance,  probability  professionals  do\\nnot  include  in  the  probabilistic  measurement  itself  an  error  rate  about,  say,  the  estimation  of  a  parameter  provided  by  an  expert,  or\\nother uncertainties attending the computations.  This would only be acceptable if they consciously accepted such limit.\\nUnlike philosophers, quantitative risk professionals (“quants”) don\\'t seem to remotely get regress arguments; questioning all the way\\n(without making a stopping assumption) is foreign to them (what I’ve called scientific autism, the kind of scientific autism that got us\\ninto  so  many  mistakes  in  finance  and  risk  management  situations  such  as  the  problem  with  the  Fukushima  reactor).  Just  reapplying\\nlayers  of  uncertainties  may  show  convexity  biases,  and,  fortunately,  it  does  not  necessarily  kill  probability  theory;  it  just  disciplines\\nthe use of some distributions, at the expense of others --distributions in the  L\\uf04c2  norm (i.e., square integrable) are no longer valid, for\\nepistemic reasons.  This is does not mean we cannot have paremetric distributions; it just means that when there is no structure to the\\nerror rates we need to stay in the power law domains, even if the data does not give us reasons for that.\\nThe  bad  news  is  that  this  recursion  of  the  error  rate  invalidates  all  common  measures  of  small  probabilitites  --and  has  the  effect  of\\nraising them.\\nIndeed, the conversation with the philosopher was quite a relief as I had a hard time discussing with quants and risk persons the point\\nthat without understanding errors, a measure is nothing and one should take the point to its logical consequence that any measure of\\nerror needs to have its own error taken into account. \\n\\nThe  epistemic  and  counterfactual  aspect  of  standard  deviations:  One  sad  conversation  took  place  a  decade  ago  with  another\\nacademic,  a  professor  of  risk  management  who  writes  papers  on  Value  at  Risk  (and  still  does):  he  could  not  get  that  the  standard\\ndeviation  of  a  distribution  for  future  outcomes  (and  not  the  sampling  of  some  properties  of  existing  population),  the  measure  of\\ndispersion,  needs  to  be  interpreted  as  the  measure  of  uncertainty,  distance  between  counterfactuals,  hence  epistemic,  and  that  it,  in\\nturn,  should  necessarily  have  uncertainties  (errors)  attached  to  it  (unless  he  accepted  infallibility  of  belief  in  such  measure).  One\\nneeds to look at the standard deviation -or other measures of dispersion -as a degree of ignorance about the future realizations of the\\nprocess. The higher the uncertainty, the higher the measure of dispersion (variance, mean deviation, etc.) \\nSuch  uncertainty,  by  Jensen’s  inequality,  creates  non-negligible  convexity  biases.  So  far  this  is  well  known  in  places  in  which\\nsubordinated  processes  have  been  used  --for  instance  stochastic  variance  models    --but  I  have  not  seen  the  layering  of  uncertainties\\ntaken into account.\\nBetting  on  Rare  Events:  Finally,  this  note  explains  my  main  points  about  betting  on  rare  events.  Do  I  believe  that  the  probability  of\\nthe event is higher? No. I believe that any additional layer of uncertainty raises the payoff from such probability (because of convex-\\nity effects); in other words, a mistake in one direction is less harmful than the benefits in the other --and this paper has the derivations\\nto show it.\\n\\nNote: Counterfactuals, Estimation of the Future v/s Sampling Problem\\nNote  that  it  is  hard  to  escape  higher  order  uncertainties,  even  outside  of  the  use  of  counterfactual:  even  when  sampling  from  a\\nconventional population, an error rate can come from the production of information (such as: is the information about the sample size\\ncorrect? is the information correct and reliable?), etc. These higher order errors exist and could be severe in the event of convexity to\\nparameters, but they are qualitatively different with forecasts concerning events that have not taken place yet. \\n\\n\\x0cNassim N. Taleb    3\\n\\nNote  that  it  is  hard  to  escape  higher  order  uncertainties,  even  outside  of  the  use  of  counterfactual:  even  when  sampling  from  a\\nconventional population, an error rate can come from the production of information (such as: is the information about the sample size\\ncorrect? is the information correct and reliable?), etc. These higher order errors exist and could be severe in the event of convexity to\\nparameters, but they are qualitatively different with forecasts concerning events that have not taken place yet. \\nThis  discussion  is  about  an  epistemic  situation  that  is  markedly  different  from  a  sampling  problem  as  treated  conventionally  by  the\\nstatistical  community,  particularly  the  Bayesian  one.  In  the  classical  case  of  sampling  by  Gosset  (“Student”,  1908)  from  a  normal\\ndistribution  with  an  unknown  variance  (Fisher,  1925),  the  Student  T  Distribution  (itself  a  power  law)  arises  for  the  estimated  mean\\nsince  the  square  of  the  variations  (deemed  Gaussian)  will  be  Chi-square  distributed.  The  initial  situation  is  one  of  completely\\nunknown  variance,  but  that  is  progressively  discovered  through  sampling;  and  the  degrees  of  freedom  (from  an  increase  in  sample\\nsize) rapidly shrink the tails involved in the underlying distribution. \\nThe  case  here  is  the  exact  opposite,  as  we  have  an  a  priori  approach  with  no  data:  we  start  with  a  known  priorly  estimated  or\\n\"guessed\" standard deviation, but with an unknown error on it expressed as a spread of branching outcomes, and, given the a priori\\naspect of the exercise, we have no sample increase helping us to add to the information and shrink the tails. We just deal with nested\\ncounterfactuals.\\nNote that given that, unlike the Gosset’s situation, we have a finite mean (since we don’t hold it to be stochastic and know it a priori)\\nhence  we  necessarily  end  in  a  situation  of  finite  first  moment  (hence  escape  the  Cauchy  distribution),  but,  as  we  will  see,  a  more\\ncomplicated second moment. \\nSee  the  discussion  of  the  Gosset  and  Fisher  approach  in  Chapter  1  of  Mosteller  and  Tukey  (1977).  [I  thank  Andrew  Gelman  and\\nAaron Brown for the discussion].\\n\\nMain Results\\nNote that unless one stops the branching at an early stage, all the results raise small probabilities (in relation to their remoteness; the\\nmore remote the event, the worse the relative effect). \\n1. Under the regime of proportional constant (or increasing) recursive layers of uncertainty about rates of uncertainty, the \\ndistribution has infinite variance, even when one starts with a standard Gaussian.\\n2. Under the other regime, where the errors are decreasing (proportionally) for higher order errors, the ending distribution becomes \\nfat-tailed but in a benign way as it retains its finite variance attribute (as well as all higher moments), allowing convergence to \\nGaussian under Central Limit.\\n3. We manage to set a boundary between these two regimes.\\n4. In both regimes the use of a thin-tailed distribution is not warranted unless higher order errors can be completely eliminated a \\npriori.\\n\\nEpistemic not statistical re-derivation of power laws: Note that previous derivations of power laws have been statistical (cumulative\\nadvantage,  preferential  attachment,  winner-take-all  effects,  criticality),  and  the  properties  derived  by  Yule,  Mandelbrot,  Zipf,  Simon,\\nBak, and others result from structural conditions or breaking the independence assumptions in the sums of random variables allowing\\nfor  the  application  of  the  central  limit  theorem.  This  work  is  entirely  epistemic,  based  on  standard  philosophical  doubts  and  regress\\narguments.\\n\\nMethods and Derivations\\n\\nLayering Uncertainties\\nThe idea is to hunt for convexity effects from the layering of higher order uncertainties (Taleb, 1997).\\nTake  a  standard  probability  distribution,  say  the  Gaussian.    The  measure  of  dispersion,  here  s,  is  estimated,  and  we  need  to  attach\\nsome measure of dispersion around it. The uncertainty about the rate of uncertainty, so to speak, or higher order parameter, similar to\\nwhat  called  the  \"volatility  of  volatility\"  in  the  lingo  of  option  operators  (see  Taleb,  1997,  Derman,  1994,    Dupire,  1994,  Hull  and\\nWhite,  1997)  --here  it  would  be  \"uncertainty  rate  about  the  uncertainty  rate\".  And  there  is  no  reason  to  stop  there:    we  can  keep\\nnesting  these  uncertainties  into  higher  orders,  with  the  uncertainty  rate  of  the  uncertainty  rate  of  the  uncertainty  rate,  and  so  forth.\\nThere is no reason to have certainty anywhere in the process.\\nNow,  for  that  very  reason,  this  paper  shows  that,  in  the  absence  of  knowledge  about  the  structure  of  higher  orders  of  deviations,  we\\nare  forced  to  use  a  power-law  tails.  Most  derivations  of  power  law  tails  have  focused  on  processes  (Zipf-Simon  preferential  attach-\\nment,  cumulative  advantage,  entropy  maximization  under  constraints,  etc.)  Here  we  just  derive  them  using  lack  of  knowledge  about\\nthe rates of knowledge.\\nA 2nd order stochastic standard deviation is the integral of f across values of s oe ]0,¶[, under the measure  f Hs, s1 , sL,  with s1  its\\nHigher order integrals in the Standard Gaussian Case\\nWe  start  with  the  case  of  a  Gaussian  and  focus  the  uncertainty  on  the  assumed  standard  deviation.  Define  f(m,s,x)  as  the  Gaussian\\ndensity function for value x with mean m and standard deviation s.\\nscale parameter (our approach to trach the error of the error), not necessarily its standard deviation; the expected value of  s1  is  s1 . \\n\\n\\x0c4  Nassim N. Taleb\\n\\n(1)\\n\\n(2)\\n\\n¶\\n\\nfHm, s, xL f Hs, s1 , sL „ s\\nf HxL1 = ‡\\n¶\\n0\\nfHm, s, xL f Hs, s1 , sL f Hs1 , s2 , s1 L ... f HsN-1 , sN , sN-1 L „ s „ s1 „ s2 ... „ sN\\nf HxLN = ‡\\n¶ ... ‡\\nGeneralizing to the Nth  order, the density function f(x) becomes\\n0\\n0\\nThe problem is that this approach is parameter-heavy and requires the specifications of the subordinated distributions (in finance, the\\n2\\nlognormal has been traditionally used for s2  (or Gaussian for the ratio Log[ st\\ns2 ] since the direct use of a Gaussian allows for negative\\nvalues).  We  would  need  to  specify  a  measure  f  for  each  layer  of  error  rate.  Instead  this  can  be  approximated  by  using  the  mean\\ndeviation for s, as we will see next.\\nNote  that  branching  variance  does  not  always  result  in  higher  Kurtosis  (4th  moment)  compared  to  the  Gaussian;  in  the  case  of  N-2,\\n¶fHm, s, xL f Hs, s1 , sL „ s  (the  first  order  standard  deviation)  would  be  to  use  a  weighted  average  of  values  of  s,  say,  for    a\\nusing the Gaussian and stochasticzing both m and s will lead to bimodality the lowering of the 4th moment.\\nŸ0\\nDiscretization using nested series of two-states for s- a simple multiplicative process\\ns H1 ± a H1LL, 0 § a H1L < 1\\nA  quite  effective  simplification  to  capture  the  convexity,  the  ratio  of  (or  difference  between)  f(m,s,x)    and\\nsimple case of one-order stochastic volatility:  \\nwhere a(1) is the proportional mean absolute deviation for s, in other word the measure of the absolute error rate for s. We use  1\\n2 as\\nthe probability of each state.\\n8f Hm, s H1 + aH1LL, xL + f Hm, sH1 - aH1LL, xL<\\nf HxL1 =\\nThus the distribution using the first order stochastic standard deviation can be expressed as:\\n1\\n2\\nIllustration of the Convexity Effect: Figure 1 shows the convexity effect of a(1) for a probability of exceeding the deviation of x=6.\\n5 , we can see the effect of multiplying the probability by 7.\\nWith a[1]= 1\\nP>x\\n\\n(3)\\n\\n0.0004\\n\\n0.0003\\n\\n0.0002\\n\\n0.0001\\n\\n1.8 STD\\n1.7\\n1.6\\n1.5\\n1.4\\n1.3\\nFigure 1 Illustration of he convexity bias for a Gaussian raising small probabilities: The plot shows the STD effect on P>x, and compares P>6 \\nwith a STD of 1.5 compared to P> 6 assuming a linear combination of 1.2 and 1.8 (here a(1)=1/5). \\n\\nNow assume uncertainty about the error rate a(1), expressed by a(2), in the same manner as before. Thus in place of a(1) we have  1\\n2\\na(1)( 1± a(2)).\\n\\n\\x0cNassim N. Taleb    5\\n\\nHaH1L + 1L s\\n\\nH1 - aH1LL s\\n\\nHaH1L + 1L HaH2L + 1L HaH3L + 1L s\\nHaH1L + 1L HaH2L + 1L s\\nHaH1L + 1L HaH2L + 1L H1 - aH3LL s\\nHaH1L + 1L H1 - aH2LL HaH3L + 1L s\\nHaH1L + 1L H1 - aH2LL s\\nHaH1L + 1L H1 - aH2LL H1 - aH3LL s\\nH1 - aH1LL HaH2L + 1L HaH3L + 1L s\\nH1 - aH1LL HaH2L + 1L s\\nH1 - aH1LL HaH2L + 1L H1 - aH3LL s\\nH1 - aH1LL H1 - aH2LL HaH3L + 1L s\\nH1 - aH1LL H1 - aH2LL s\\nH1 - aH1LL H1 - aH2LL H1 - aH3LL s\\n\\ns\\n\\nFigure 2- Three levels of error rates for s following a multiplicative process\\n8fHm, sH1 + aH1L H1 + aH2LLL, xL +\\nf HxL2 =\\nThe second order stochastic standard deviation:\\nfHm, sH1 - aH1L H1 + aH2LLL, xL + fHm, sH1 + aH1L H1 - aH2LL, xL + f Hm, sH1 - aH1L H1 - aH2LLL, xL<\\n1\\n4\\nfIm, s Mi\\nf HxL N =\\nN , xM\\n2N ‚\\nand the Nth  order:\\nN   is the  ith  scalar (line)  of the matrix MN I2N µ 1M\\n1\\n2N\\nMN = :‰\\nN Ha HjL TPi, jT + 1L>\\ni=1\\nwhere  Mi\\nj=1\\nand  T[[i,j]]    the  element    of  ith line  and  jthcolumn  of  the  matrix  of  the  exhaustive  combination  of  N-Tuples  of  (-1,1),that  is  the  N-\\ndimentionalvector {1,1,1,...} representing all combinations of 1 and -1.\\nfor N=3\\n\\n2N\\ni=1\\n\\n(6)\\n\\n(4)\\n\\n(5)\\n\\n\\x0c6  Nassim N. Taleb\\n\\n1\\n1\\n1\\n1 -1\\n1\\n1 -1\\n1\\n1 -1 -1\\nT =\\n-1\\n1\\n1\\n1 -1\\n-1\\n3 = 8H1 - aH1LL H1 - aH2LL H1 - aH3LL<, etc.\\n-1 -1\\n1\\n-1 -1 -1\\nso M1\\n\\nH1 - aH1LL H1 - aH2LL H1 - aH3LL\\nH1 - aH1LL H1 - aH2LL HaH3L + 1L\\nH1 - aH1LL HaH2L + 1L H1 - aH3LL\\nH1 - aH1LL HaH2L + 1L HaH3L + 1L\\nHaH1L + 1L H1 - aH2LL H1 - aH3LL\\nHaH1L + 1L H1 - aH2LL HaH3L + 1L\\nHaH1L + 1L HaH2L + 1L H1 - aH3LL\\nHaH1L + 1L HaH2L + 1L HaH3L + 1L\\n\\nand M 3 =\\n\\nFigure 3, Thicker tails (higher peaks) for higher values of N; here N=0,5,10,25,50,  all values of a= 1\\n10\\n\"low\"  variance    Hs H1 - vLL2  and  a  \"high\"  one   s -v2 + 2 v + 1\\nA  remark  seems  necessary  at  this  point:  the  various  error  rates  a(i)  are  not  similar  to  sampling  errors,  but  rather  projection  of  error\\nrates into the future.\\nNote:  we  are  assuming  here,  that  s  is  stochastic  with  steps  (1±  a(n)),  not  s2 .  An  alternative  method  would  be  the  mixture  with  a\\n2\\n selecting  a  single  v  so  that  s2  remains  the  same  in  expectation.\\nWith 1 >v¥0, the total standard deviation.\\nThe Final Mixture Distribution\\nf Hx m, s, M , N L = 2-N ‚\\nfIm, s Mi\\nN , x M\\nThe mixture weighted average distribution (recall that f is the ordinary Gaussian with mean m, std s and the random variable x). \\n2N\\ni=1\\nNote: It could be approximated by a lognormal distribution for s and the corresponding V as its own variance. But it is precisely \\nthe V that interest us, and V depends on how higher order errors behave. \\nNext let us consider the different regimes for higher order errors.\\nRegime 1 (Explosive): Case of a Constant parameter a\\nj O fIm, s Ha + 1L j H1 - aLN- j , xM\\nf Hx m, s, M , N L = 2-N ‚\\nN K N\\nSpecial  case  of  constant  a:  Assume  that  a(1)=a(2)=...a(N)=a,  i.e.  the  case  of  flat  proportional  error  rate  a.  The  Matrix  M  collapses\\ninto a conventional binomial tree for the dispersion at the level N.\\nj=0\\nBecause of the linearity of the sums, when a is constant, we can use the binomial distribution as weights for the moments (note again\\nthe artificial effect of constraining the first moment m in the analysis to a set, certain, and known a priori).\\n\\n(7)\\n\\n(8)\\n\\n\\x0cNassim N. Taleb    7\\n\\ns2 Ha2 + 1LN + m2\\n3 m s2 Ha2 + 1LN + m3\\nMoment\\n6 m2 s2 Ha2 + 1LN + m4 + 3 Ha4 + 6 a2 + 1LN s4\\nm\\n10 m3 s2 Ha2 + 1LN + m5 + 15 Ha4 + 6 a2 + 1LN m s4\\n15 m4 s2 Ha2 + 1LN + m6 + 15 HHa2 + 1L Ha4 + 14 a2 + 1LLN s6 + 45 Ha4 + 6 a2 + 1LN m2 s4\\n21 m5 s2 Ha2 + 1LN + m7 + 105 HHa2 + 1L Ha4 + 14 a2 + 1LLN m s6 + 105 Ha4 + 6 a2 + 1LN m3 s4\\nm6 s2 Ha2 + 1LN + m8 + 105 Ha8 + 28 a6 + 70 a4 + 28 a2 + 1LN s8 + 420 HHa2 + 1L Ha4 + 14 a2 + 1LLN m2 s6 + 210 Ha4 + 6 a2 + N\\nFor clarity, we simplify the table of moments, with m=0\\nHa2 + 1LN s2\\nOrder\\nMoment\\n3 Ha4 + 6 a2 + 1LN s4\\n1\\n0\\n2\\n15 Ha6 + 15 a4 + 15 a2 + 1LN s6\\n3\\n0\\n4\\n105 Ha8 + 28 a6 + 70 a4 + 28 a2 + 1LN s8\\n0\\n5\\n6\\n7\\n0\\nf HxL „ x= \\nindependent of a and N, since the perturbations of s do not affect the first absolute moment Ÿ x\\n8\\nNote  again  the  oddity  that  in  spite  of  the  explosive  nature  of  higher  moments,  the  expectation  of  the  absolute  value  of  x  is  both\\ns  (that is, the\\n2\\np\\ninitial assumed s). The situation would be different under addition of x.\\nEvery  recursion  multiplies  the  variance  of  the  process  by  (1+a2 ).    The  process  is  similar  to  a  stochastic  volatility  model,  with  the\\nstandard  deviation  (not  the  variance)  following  a  lognormal  distribution,  the  volatility  of  which  grows  with  M,  hence  will  reach\\ninfinite variance at the limit.\\nA- Even the smallest value of a >0, since I1 + a2MN  is unbounded,  leads to the second moment going to infinity (though not \\nConsequences\\nFor a constant a > 0, and in the more general case with variable a where a(n) ¥ a(n-1),  the moments explode.\\nthe first) when NØ¶. So something as small as a .001% error rate will still lead to explosion of moments and invalidation of the \\nuse of the class of  L\\uf04c2  distributions.\\nB- In these conditions, we need to use  power laws for epistemic reasons, or, at least, distributions outside the L\\uf04c2  norm, \\nregardless of observations of past data.\\nNote  that  we  need  an  a  priori  reason  (in  the  philosophical  sense)  to  cutoff  the  N  somewhere,  hence  bound  the  expansion  of  the\\nsecond moment.\\n\\n\\x0c8  Nassim N. Taleb\\n\\nConvergence to Properties Similar to Power Laws\\nWe  can  see  on  the  example  next  Log-Log  plot  (Figure  1)  how,  at  higher  orders  of  stochastic  volatility,  with  equally  proportional\\nstochastic  coefficient,  (where  a(1)=a(2)=...=a(N)=   1\\n10 )    how  the  density  approaches  that  of  a  power  law  (just  like  the  Lognormal\\ndistribution  at  higher  variance),  as  shown  in  flatter  density  on  the  LogLog  plot.    The  probabilities  keep  rising  in  the  tails    as  we  add\\nlayers of uncertainty until they seem to reach the boundary of the power law, while ironically the first moment remains invariant.\\nLog PrHxL\\n1\\n10\\n0.1\\n\\n, N=0,5,10,25,50\\n\\na=\\n\\n10-4\\n\\n10-7\\n\\n10-10\\n\\n10-13\\n\\n7.0\\n15.0\\n1.5\\n30.0\\n3.0\\n20.0\\n2.0\\n5.0\\n10.0\\nFigure x - LogLog Plot of the probability of exceeding x showing power law-style flattening as N rises. Here all values of a= 1/10\\nThe same effect takes place as a increases towards 1, as at the limit the tail exponent P>x approaches 1 but remains >1.\\n\\nLog x\\n\\nEffect on Small Probabilities\\nNext we measure the effect on the thickness of the tails. The obvious effect is the rise of small probabilities.\\n2-N-1 K N\\nj O erfc\\nP > K N = ‚\\n2 s Ha + 1L j H1 - aLN- j\\nTake the exceedant probability, that is, the probability of exceeding K , given N , for parameter a constant :\\nwhere erfc(.) is the complementary of the error function, 1-erf(.), erf HzL =\\nŸ0\\nK\\nN\\nj=0\\n2\\nze-t2 dt\\np\\nConvexity  effect:The  next  Table  shows  the  ratio  of  exceedant  probability  under  different  values  of  N  divided  by  the  probability  in\\nthe case of a standard Gaussian.\\n\\n(9)\\n\\na =\\n\\n1\\n100\\nP>5,N\\nP>5,N=0\\n1.155\\n1.326\\n1.514\\n1.720\\n1.943\\n1\\na =\\n10\\nP>5,N\\nP>5,N=0\\n146\\n805\\n\\nP>3,N\\nP>3,N=0\\n1.01724\\n1.0345\\n1.05178\\n1.06908\\n1.0864\\n\\nP>3,N\\nP>3,N=0\\n2.74\\n4.43\\n\\nN\\n5\\n10\\n15\\n20\\n25\\n\\nN\\n5\\n10\\n\\nP>10,N\\nP>10,N=0\\n7\\n45\\n221\\n922\\n3347\\n\\nP>10,N\\nP>10,N=0\\n1.09 µ 1012\\n8.99 µ 1015\\n\\n\\x0cNassim N. Taleb    9\\n\\n15\\n20\\n25\\n\\n5.98\\n7.38\\n8.64\\n\\n1980\\n3529\\n5321\\n\\n2.21 µ 1017\\n1.20 µ 1018\\n3.62 µ 1018\\n\\nRegime 2: Cases of decaying parameters a(n)\\n\\nAs  we  said,  we  may  have  (actually  we  need  to  have)  a  priori  reasons  to  decrease  the  parameter  a  or  stop  N  somewhere.  When  the\\nhigher order of a(i) decline, then the moments tend to be capped (the inherited tails will come from the lognormality of s).\\n\\n(10)\\n\\n(11)\\n\\n(12)\\n\\nWith N=3,\\n\\nRegime 2-a; First Method: “bleed” of higher order error\\nM2H2L = IaH1L2 + 1M s2 IaH1L2 l2 + 1M\\nTake a \"bleed\" of higher order errors at the rate l,  0§ l < 1 , such as a(N) = l  a(N-1), hence a(N) = lN  a(1), with a(1) the conven-\\ntional intensity of stochastic standard deviation. Assume m=0.\\nWith N=2 , the second moment becomes:\\nM2H3L = s2 I1 + aH1L2 M I1 + l2 aH1L2 M I1 + l4 aH1L2 M\\nM3HN L = IaH1L2 + 1M s2 ‰\\nN-1 IaH1L2 l2 i + 1M\\nfinally, for the general N:\\nWe can reexpress H12L using the Q - Pochhammer symbol Ha; qLN = ‰\\nN-1 I1 - aqi M\\ni=1\\nM2HN L = s2 I-aH1L2 ; l2 MN\\ni=1\\nLimit M2 HN LNØ¶ = s2 Hl2 ; l2 L2 HaH1L2 ; l2 L¶\\nHl2 - 1L2 Hl2 + 1L\\nWhich allows us to get to the limit\\nM4HN L = 3 s4 ‰\\nN-1 I6 aH1L2 l2 i + aH1L4 l4 i + 1M\\nM4HN L = 3 s4 KK2\\n2 - 3O aH1L2 ; l2 O\\nK-K3 + 2\\n2 O aH1L2 ; l2 O\\ni=0\\n2 O aH1L2 ; l2 O\\nK-K3 + 2\\n2 - 3O aH1L2 ; l2 O\\nLimit M4H N LNØ¶ = 3 s4 KK2\\nN\\nN\\n¶\\n¶\\nSo  the  limiting  second  moment  for  l=.9  and  a(1)=.2  is  just  1.28  s2 ,  a  significant  but  relatively  benign  convexity  bias.  The  limiting\\nfourth  moment  is  just  9.88 s4 ,    more  than  3  times  the  Gaussian’s  (3  s4 ),  but  still  finite  fourth  moment.  For  small  values  of  a  and\\nvalues of l close to 1, the fourth moment collapses to that of a Gaussian.\\ns H1 ± HaH1L H1 ± H a H2L H1 ± aH3L H ... LLL\\nRegime 2-b; Second Method, a Non Multiplicative Error Rate\\nFor N recursions\\n\\nAs to the fourth moment:\\nBy recursion:\\n\\n(13)\\n\\n(14)\\n\\n(15)\\n\\n(16)\\n\\n(17)\\n\\n\\x0cSo for instance, for N=3, T= {1, a, a2 , a3}\\n\\nf Hx, m, s H1 + H T N . AN Li L\\n⁄i=1L\\n10  Nassim N. Taleb\\nPHx, m, s, N L =\\nIM N .T + 1Mi is the ith component of the HN ä 1L dot product of T N the matrix of Tuples in H6L ,\\nL\\nAN = 9a j = j=1,... N\\nL the length of the matrix, and A is the vector of parameters\\na + a2 + a3\\na + a2 - a3\\na - a2 + a3\\na - a2 - a3\\nT 3. A3 =\\n- a + a2 + a3\\n- a + a2 - a3\\n- a - a2 + a3\\nM1HN L = m\\n- a - a2 - a3\\nM2HN L = m2 + 2 s\\nM4HN L = m4 + 12 m2 s + 12 s2 ‚\\nN\\na2 i\\nLim N Ø¶ M4HN L = m4 + 12 m2 s + 12 s2\\ni=0\\nConclusions and Open Questions\\nSo  far  we  examined  two  regimes,  one  in  which  the  higher  order  errors  are  proportionally  constant,  the  other  one  in  which  we  can\\nallow them to decline. The difference between the two is easy to spot: the first category corresponds to naturally thin-tailed  domains\\n(higher  errors  decline  rapidly),  something  very  rare  on  mother  earth.  Outside  of  these  very  special  situations  (say  in  some  strict\\napplications  or  clear  cut  sampling  problems  from  a  homogeneous  population,  or  similar  matters  stripped  of  higher  order  uncertain-\\nties),  the  Gaussian  and  its  siblings  (along  with  the  measures  such  as  STD,  correlation,  etc.)  should  be  completely  abandoned,  along\\nwith any attempt to measure small probabilities. So the power law distributions are to be used more prevalently than initially thought.\\n\\nThe moments are as follows:\\n\\nat the limit of N Ø ¶\\n\\nwhich is very mild.\\n\\n(18)\\n\\n(19)\\n(20)\\n\\n(21)\\n\\n(22)\\n\\n1\\n1 - a2\\n\\nË Can we separate the two domains along the rules of tangibility/subjectivity of the probabilistic measurement? Daniel Kahneman \\nhad a saying about measuring future states: how can one “measure” something that does not exist?  So we could use:\\n\\nË Regime 1: elements entailing forecasting and “measuring” future risks.  So should we use time as a dividing criterion: \\nAnything that has time in it (meaning involves a forecast of future states) needs to fall into the first regime of non-declining \\nproportional uncertainty parameters a(i).\\n\\nË Regime 2: conventional statistical measurements of matters patently thin-tailed, say as in conventional sampling theory, with \\na strong a priori acceptance of the methods without any form of skepticism.\\n\\nË We can even work backwards, using the behavior of the estimation errors a(n) < a(1) or a(n) ¥ a(1) as a way to separate \\nuncertainties.\\n\\nNote 1\\nInfinite  variance  is  not  a  problem  at  all  --  yet  economists  have  been  historically  scared  of  it.    All  we  have  to  do  is  avoid  using\\nvariance  and  measures  in  the  L\\uf04c2  norm.    For  instance  we  can  do  much  of  what  we  currently  do  (even  price  financial  derivatives)  by\\nusing  mean  absolute  deviation  of  the  random  variable,  E[x]  in  place  of  s,  so  long  as  the  tail  exponent  of  the  power  law  exceeds  1\\n(Taleb, 2008). \\n\\n\\x0cNassim N. Taleb    11\\n\\nInfinite  variance  is  not  a  problem  at  all  --  yet  economists  have  been  historically  scared  of  it.    All  we  have  to  do  is  avoid  using\\nvariance  and  measures  in  the  L\\uf04c2  norm.    For  instance  we  can  do  much  of  what  we  currently  do  (even  price  financial  derivatives)  by\\nusing  mean  absolute  deviation  of  the  random  variable,  E[x]  in  place  of  s,  so  long  as  the  tail  exponent  of  the  power  law  exceeds  1\\n(Taleb, 2008). \\n\\nNote 2\\nThere  is  most  certainly  a  cognitive  dimension,  rarely  (or,  I  believe,  never)  addressed  or  investigated,  in  the  following  mental\\nshortcomings that, from the research, appears to be common among probability modelers:\\n\\nË Inability (or, perhaps, as the cognitive science literature seems to now hold, lack of motivation) to perform higher order \\nrecursions among a certain class of people (I know that he knows that I know that he knows...). See the second edition of The \\nBlack Swan, Taleb (2010).\\n\\nË Inability (or lack of motivation) to transfer from one situation to another (similar to the problem of weakness of central \\ncoherence). For instance, a researcher can accept power laws in one domain yet not recognize them in another, not integrating \\nthe ideas (lack of central coherence). I have observed this total lack of central coherence with someone who can do stochastic \\nvolatility models but is unable to understand them outside the exact same conditions when doing other papers.\\n\\nAcknowledgments\\nJean-Philippe Bouchaud, Raphael Douady, Charles Tapiero, Aaron Brown, Dana Meyer, Andrew Gelman, Felix Salmon.\\n\\nReferences\\nAbramovich and Stegun (1972) Handbook of Mathematical Functions,  Dover Publications\\nDavid Lewis (1973) Counterfactuals, Harvard U. Press\\nDerman, E., Kani, I. (1994). Riding on a smile. Risk 7, 32–39.\\nDupire, Bruno (1994) Pricing with a smile, Risk, 7, 18–20. \\nFisher, R.A. (1925), Applications of “Student’s” distribution, Metron 5 90-104\\nHull, J., White, A. (1997) The pricing of options on assets with stochastic volatilities, Journal of Finance , 42\\nMandelbot, B. (1997) Fractals and Scaling in Finance, Springer.\\nMosteller, Frederick & John W Tukey (1977). Data Analysis and Regression : a Second Course in Statistics. Addison-Wesley.\\n“Student” (1908) The probable error of a mean, Biometrica VI, 1-25\\nTaleb, N.N.  (1997)  Dynamic Hedging: Managing Vanilla and Exotic Options, Wiley\\nTaleb, N.N. (2008) Finite variance is not necessary for the practice of quantitative finance. Complexity 14(2)\\nTaleb, N.N. (2009) Errors, robustness and the fourth quadrant, International Journal of Forecasting, 25-4, 744--759\\n\\n\\x0c', 'num_latex': 0, 'text_len': 1243, 'clean_abstract': 'ante forecast outcome interpreted counterfactuals potential history error spread outcome reapplying measurement uncertainty estimation error estimation error estimation lead branching counterfactuals recursion epistemic uncertainty markedly different distributial property conventional sampling error nested counterfactuals error rate invariably lead fat tail regardless probability distribution used powerlaws condition mere branching error rate std error rate branching error rate error rate etc recursing way explosive infinite higher moment missing degree regress lead underestimation small probability concave payoff standard example fukushima state condition higher order rate uncertainty expressed spread counterfactuals alters shape final distribution show priori belief conterfactuals needed accept reliability conventional probabilistic method thin tail mildly fat tail', 'final_text': 'ante forecast outcome interpreted counterfactuals potential history error spread outcome reapplying measurement uncertainty estimation error estimation error estimation lead branching counterfactuals recursion epistemic uncertainty markedly different distributial property conventional sampling error nested counterfactuals error rate invariably lead fat tail regardless probability distribution used powerlaws condition mere branching error rate std error rate branching error rate error rate etc recursing way explosive infinite higher moment missing degree regress lead underestimation small probability concave payoff standard example fukushima state condition higher order rate uncertainty expressed spread counterfactuals alters shape final distribution show priori belief conterfactuals needed accept reliability conventional probabilistic method thin tail mildly fat tail', 'label': 17}\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_from_disk\n",
    "import pandas as pd\n",
    "\n",
    "ds_train_loaded = load_from_disk(\"./train_raw_dataset\")\n",
    "ds_test_loaded = load_from_disk(\"./test_raw_dataset\")\n",
    "\n",
    "print(\"Struktur Dataset:\")\n",
    "print(f\"Train: {ds_train_loaded}\")\n",
    "print(f\"Test : {ds_test_loaded}\")\n",
    "\n",
    "print(\"\\n5 Data Pertama (Train)\")\n",
    "df_sample = pd.DataFrame(ds_train_loaded[:5])\n",
    "display(df_sample)\n",
    "\n",
    "print(\"\\nContoh Raw Data ke-0\")\n",
    "print(ds_train_loaded[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "891ec550",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Jumlah dokumen: 86941\n",
      "Contoh 1 dokumen:\n",
      "Metadata: {'paper_id': '1209.2298', 'yymm': '1209', 'title': 'The Future Has Thicker Tails than the Past: Model Error As Branching Counterfactuals', 'primary_subfield': 'q-fin.RM', 'source': 'Arxiv'}\n",
      "--------------------\n",
      "ante forecast outcome interpreted counterfactuals potential history error spread outcome reapplying measurement uncertainty estimation error estimation error estimation lead branching counterfactuals recursion epistemic uncertainty markedly different distributial property conventional sampling...\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.documents import Document\n",
    "import textwrap\n",
    "\n",
    "def convert_hf_to_documents(dataset):\n",
    "    \"\"\"\n",
    "    Mengubah Hugging Face Dataset menjadi list Document LangChain.\n",
    "    \"\"\"\n",
    "    docs = []\n",
    "    \n",
    "    for row in dataset:\n",
    "        text = row.get(\"clean_abstract\", \"\")\n",
    "        \n",
    "        paper_id = row.get(\"paper_id\", None)\n",
    "        yymm = row.get(\"yymm\", None)\n",
    "        title = row.get(\"title\", \"\")\n",
    "        primary_subfield = row.get(\"primary_subfield\", None)\n",
    "        \n",
    "        metadata = {\n",
    "            \"paper_id\": paper_id,\n",
    "            \"yymm\": yymm,\n",
    "            \"title\": title,\n",
    "            \"primary_subfield\": primary_subfield,\n",
    "            \"source\": \"Arxiv\"\n",
    "        }\n",
    "        \n",
    "        docs.append(Document(page_content=text, metadata=metadata))\n",
    "        \n",
    "    return docs\n",
    "\n",
    "train_documents = convert_hf_to_documents(ds_train_loaded)\n",
    "\n",
    "print(f\"Jumlah dokumen: {len(train_documents)}\")\n",
    "\n",
    "if len(train_documents) > 0:\n",
    "    print(\"Contoh 1 dokumen:\")\n",
    "    print(\"Metadata:\", train_documents[0].metadata)\n",
    "    print(\"-\" * 20)\n",
    "    print(textwrap.shorten(train_documents[0].page_content, width=300, placeholder=\"...\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1c160c03",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "page_content='ante forecast outcome interpreted counterfactuals potential history error spread outcome reapplying measurement uncertainty estimation error estimation error estimation lead branching counterfactuals recursion epistemic uncertainty markedly different distributial property conventional sampling error nested counterfactuals error rate invariably lead fat tail regardless probability distribution used powerlaws condition mere branching error rate std error rate branching error rate error rate etc recursing way explosive infinite higher moment missing degree regress lead underestimation small probability concave payoff standard example fukushima state condition higher order rate uncertainty expressed spread counterfactuals alters shape final distribution show priori belief conterfactuals needed accept reliability conventional probabilistic method thin tail mildly fat tail' metadata={'paper_id': '1209.2298', 'yymm': '1209', 'title': 'The Future Has Thicker Tails than the Past: Model Error As Branching Counterfactuals', 'primary_subfield': 'q-fin.RM', 'source': 'Arxiv'}\n"
     ]
    }
   ],
   "source": [
    "print(train_documents[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f76ae7b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "# text_splitter = RecursiveCharacterTextSplitter(\n",
    "#     chunk_size=2000,\n",
    "#     chunk_overlap=200,\n",
    "#     length_function=len,\n",
    "#     separators=[\"\\n\\n\", \"\\n\", \" \", \"\"]\n",
    "# )\n",
    "\n",
    "# final_docs = text_splitter.split_documents(train_documents)\n",
    "\n",
    "# print(f\"Jumlah dokumen awal: {len(train_documents)}\")\n",
    "# print(f\"Jumlah dokumen akhir: {len(final_docs)}\")\n",
    "\n",
    "# if len(train_documents) == len(final_docs):\n",
    "#     print(\"Sempurna! Setiap abstract tetap menjadi 1 chunk utuh.\")\n",
    "# else:\n",
    "#     print(\"Beberapa abstract yang sangat panjang telah dipecah.\")\n",
    "\n",
    "# avg_len = sum(len(d.page_content) for d in final_docs) / len(final_docs)\n",
    "# print(f\"Rata-rata panjang karakter per chunk: {avg_len:.0f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7fded639",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Menyiapkan Embedding Model...\n",
      "Mulai proses embedding untuk 86941 dokumen...\n",
      "Processing... 100.00% (86941/86941)\n",
      "Selesai! Semua data berhasil masuk Vector DB.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import gc\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from langchain_chroma import Chroma\n",
    "\n",
    "CHROMA_PATH = \"./chroma_db\"\n",
    "BATCH_SIZE = 100\n",
    "MODEL_NAME = \"sentence-transformers/all-MiniLM-L6-v2\"\n",
    "\n",
    "print(\"Menyiapkan Embedding Model...\")\n",
    "embedding_function = HuggingFaceEmbeddings(model_name=MODEL_NAME, encode_kwargs={\"normalize_embeddings\": True})\n",
    "\n",
    "vector_db = Chroma(\n",
    "    persist_directory=CHROMA_PATH,\n",
    "    embedding_function=embedding_function,\n",
    "    collection_name=\"paper_abstracts\"\n",
    ")\n",
    "\n",
    "def process_batches(documents, batch_size):\n",
    "    total_docs = len(documents)\n",
    "    print(f\"Mulai proses embedding untuk {total_docs} dokumen...\")\n",
    "    \n",
    "    for i in range(0, total_docs, batch_size):\n",
    "        batch = documents[i : i + batch_size]\n",
    "        \n",
    "        vector_db.add_documents(batch)\n",
    "        \n",
    "        percent = ((i + len(batch)) / total_docs) * 100\n",
    "        print(f\"Processing... {percent:.2f}% ({i + len(batch)}/{total_docs})\", end=\"\\r\")\n",
    "        \n",
    "        del batch\n",
    "        gc.collect()\n",
    "\n",
    "    print(\"\\nSelesai! Semua data berhasil masuk Vector DB.\")\n",
    "\n",
    "process_batches(train_documents, BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "152cb6d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query: 'tiongkok debt trap for developing countries'\n",
      "\n",
      "SCORE      | JUDUL PAPER\n",
      "--------------------------------------------------------------------------------\n",
      "0.9045     | Sustainable Credit And Interest Rates | q-fin.GN\n",
      "Abstract   : negative growth real production many country debt level become increasing burden developed society call change economic policy even monetary system become louder increasingly impatient research conseq...\n",
      "--------------------------------------------------------------------------------\n",
      "1.0531     | Scale invariant properties of public debt growth | q-fin.GN\n",
      "Abstract   : public debt one important economic variable quantitatively describes nation economy bankruptcy risk faced even institution large government iceland national debt strictly controlled respect national w...\n",
      "--------------------------------------------------------------------------------\n",
      "1.0847     | Some Possible Solution of Problem of Sovereign Debts: a short plan | q-fin.GN\n",
      "Abstract   : possible solution problem sovereign debt suggested current moment solution still provided method world monetary policy...\n",
      "--------------------------------------------------------------------------------\n",
      "1.1264     | EMU and ECB Conflicts | econ.EM\n",
      "Abstract   : dynamical framework conflict government central bank according exchange rate payment fixed rate fixed rate fixed income emu convergence criterion public debt gdp ratio consists calculating private pub...\n",
      "--------------------------------------------------------------------------------\n",
      "1.1692     | Global economic dynamics of the forthcoming years. A forecast | q-fin.ST\n",
      "Abstract   : analyzes current state world economy offer short term forecast development log periodic oscillation djia dynamic suggests second half united state developed country could experience recession due thir...\n",
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from langchain_chroma import Chroma\n",
    "\n",
    "CHROMA_PATH = \"./chroma_db\"\n",
    "EMBEDDING_MODEL = \"sentence-transformers/all-MiniLM-L6-v2\"\n",
    "\n",
    "embedding_function = HuggingFaceEmbeddings(\n",
    "    model_name=EMBEDDING_MODEL,\n",
    "    encode_kwargs={\"normalize_embeddings\": True} \n",
    ")\n",
    "\n",
    "vector_db = Chroma(\n",
    "    persist_directory=CHROMA_PATH,\n",
    "    embedding_function=embedding_function,\n",
    "    collection_name=\"paper_abstracts\"\n",
    ")\n",
    "\n",
    "my_query = \"tiongkok debt trap for developing countries\"\n",
    "top_k = 5\n",
    "\n",
    "results = vector_db.similarity_search_with_score(my_query, k=top_k)\n",
    "\n",
    "print(f\"Query: '{my_query}'\\n\")\n",
    "print(f\"{'SCORE':<10} | {'JUDUL PAPER'}\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "for doc, score in results:\n",
    "    title = doc.metadata.get(\"title\", \"No Title\")\n",
    "    primary_subfield = doc.metadata.get(\"primary_subfield\", \"No Title\")\n",
    "    \n",
    "    print(f\"{score:.4f}     | {title} | {primary_subfield}\")\n",
    "    \n",
    "    print(f\"Abstract   : {doc.page_content[:200]}...\") \n",
    "    print(\"-\" * 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5899388c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hf_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
