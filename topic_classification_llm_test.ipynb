{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e8c5112e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Iskandar\\Documents\\GitHub\\Tubes-NLP-PaperMatch\\hf_env\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "# Login using e.g. `huggingface-cli login` to access this dataset\n",
    "ds = load_dataset(\"kilian-group/arxiv-classifier\", \"default\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f111a470",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Convert to pandas\n",
    "train_df = ds[\"train\"].to_pandas()\n",
    "test_df  = ds[\"test\"].to_pandas()\n",
    "\n",
    "# Stratify by column \"field\"\n",
    "train_small_df, _ = train_test_split(\n",
    "    train_df,\n",
    "    train_size=5000,\n",
    "    stratify=train_df[\"field\"],\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "test_small_df, _ = train_test_split(\n",
    "    test_df,\n",
    "    train_size=1000,\n",
    "    stratify=test_df[\"field\"],\n",
    "    random_state=42\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5892dece",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_small_df = train_small_df[[\"field\", \"fulltext\"]]\n",
    "test_small_df  = test_small_df[[\"field\", \"fulltext\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8cc2f72f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>field</th>\n",
       "      <th>fulltext</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>35286</th>\n",
       "      <td>math</td>\n",
       "      <td>\\nA new fusion procedure for the Brauer algebr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>88609</th>\n",
       "      <td>cond-mat</td>\n",
       "      <td>Neural-network quantum state tomography for ma...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42384</th>\n",
       "      <td>q-bio</td>\n",
       "      <td>A little walk from physical to biological comp...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23420</th>\n",
       "      <td>cs</td>\n",
       "      <td>\\nFrom individual to population:\\nChallenges i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100500</th>\n",
       "      <td>physics</td>\n",
       "      <td>Acoustic flat lensing using an indefinite medi...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           field                                           fulltext\n",
       "35286       math  \\nA new fusion procedure for the Brauer algebr...\n",
       "88609   cond-mat  Neural-network quantum state tomography for ma...\n",
       "42384      q-bio  A little walk from physical to biological comp...\n",
       "23420         cs  \\nFrom individual to population:\\nChallenges i...\n",
       "100500   physics  Acoustic flat lensing using an indefinite medi..."
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_small_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4e266174",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = sorted(train_small_df['field'].unique())\n",
    "label2id = {lab: i for i, lab in enumerate(labels)}\n",
    "id2label = {i: lab for lab, i in label2id.items()}\n",
    "train_small_df['label_id'] = train_small_df['field'].map(label2id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c4249cc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "from nltk.stem import WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ef213b6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatizer = WordNetLemmatizer()\n",
    "stop_words = set(stopwords.words(\"english\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "67d19811",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(text):\n",
    "  text = str(text).lower()\n",
    "\n",
    "  tokens = word_tokenize(text)\n",
    "\n",
    "  tokens = [lemmatizer.lemmatize(word) for word in tokens if word not in stop_words]\n",
    "\n",
    "  return \" \".join(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7b17dde9",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_small_df[\"new_full_text\"] = train_small_df[\"fulltext\"].apply(preprocess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e9a28487",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.57.1\n",
      "2.5.1+cu121\n"
     ]
    }
   ],
   "source": [
    "import transformers, torch\n",
    "print(transformers.__version__)\n",
    "print(torch.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3a234509",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\Iskandar\\Documents\\GitHub\\Tubes-NLP-PaperMatch\\hf_env\\Lib\\site-packages\\tf_keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import transformers\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, Trainer, TrainingArguments\n",
    "from peft import LoraConfig, get_peft_model, TaskType\n",
    "import torch, os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5f751eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Iskandar\\Documents\\GitHub\\Tubes-NLP-PaperMatch\\hf_env\\Lib\\site-packages\\huggingface_hub\\file_download.py:143: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\Iskandar\\.cache\\huggingface\\hub\\models--roberta-base. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n",
      "c:\\Users\\Iskandar\\Documents\\GitHub\\Tubes-NLP-PaperMatch\\hf_env\\Lib\\site-packages\\huggingface_hub\\file_download.py:143: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\Iskandar\\.cache\\huggingface\\hub\\models--microsoft--deberta-base. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    }
   ],
   "source": [
    "tokenizer_map = {\n",
    "    \"bert\": \"bert-base-uncased\",\n",
    "    \"roberta\": \"roberta-base\",\n",
    "    \"distilbert\": \"distilbert-base-uncased\"\n",
    "}\n",
    "\n",
    "tokenizers = {k: AutoTokenizer.from_pretrained(v) for k, v in tokenizer_map.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "dce4dae5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train_texts, val_texts, train_labels, val_labels = train_test_split(\n",
    "    train_small_df[\"new_full_text\"].astype(str).values,\n",
    "    train_small_df[\"label_id\"].values,\n",
    "    test_size=0.2,\n",
    "    random_state=42,\n",
    "    stratify=train_small_df[\"label_id\"].values\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "85553271",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (15472 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (15523 > 512). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           avg_tokens  median_tokens  max_tokens  min_tokens  percent_truncated_at_{}\n",
      "bert     12098.378667         9421.5    308821.0         2.0                 0.066667\n",
      "roberta  12753.700667         9856.0    321580.0       436.0                 0.100000\n",
      "deberta  12753.700667         9856.0    321580.0       436.0                 0.100000\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def tokenizer_stats(sample_texts, tokenizers, max_length=40, n_samples=1000):\n",
    "    sample = list(sample_texts)[:n_samples]\n",
    "    stats = {}\n",
    "    for name, tok in tokenizers.items():\n",
    "        counts = []\n",
    "        n_trunc = 0\n",
    "        for t in sample:\n",
    "            enc = tok(t, add_special_tokens=True)\n",
    "            length = len(enc[\"input_ids\"])\n",
    "            counts.append(length)\n",
    "            if length > max_length:\n",
    "                n_trunc += 1\n",
    "        stats[name] = {\n",
    "            \"avg_tokens\": float(np.mean(counts)),\n",
    "            \"median_tokens\": float(np.median(counts)),\n",
    "            \"max_tokens\": int(np.max(counts)),\n",
    "            \"min_tokens\": int(np.min(counts)),\n",
    "            \"percent_truncated_at_{}\": 100.0 * n_trunc / len(sample)\n",
    "        }\n",
    "    return stats\n",
    "\n",
    "tok_stats = tokenizer_stats(train_texts, tokenizers, max_length=200000, n_samples=min(3000, len(train_texts)))\n",
    "pd.set_option('display.width', 120)\n",
    "print(pd.DataFrame(tok_stats).T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "40504f71",
   "metadata": {},
   "outputs": [],
   "source": [
    "import datasets\n",
    "from datasets import Dataset\n",
    "\n",
    "def prepare_hf_dataset(texts, labels, tokenizer, max_length=40):\n",
    "    ds = Dataset.from_dict({\"text\": list(texts), \"label\": list(labels)})\n",
    "    def tokenize_fn(batch):\n",
    "        return tokenizer(batch[\"text\"], truncation=True, padding=\"max_length\", max_length=max_length)\n",
    "    ds = ds.map(tokenize_fn, batched=True)\n",
    "    ds = ds.remove_columns([c for c in ds.column_names if c not in (\"input_ids\", \"attention_mask\", \"label\")])\n",
    "    ds.set_format(type=\"torch\")\n",
    "    return ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "615fcb7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "models_to_train = {\n",
    "    \"bert\": \"bert-base-uncased\",\n",
    "    \"roberta\": \"roberta-base\",\n",
    "}\n",
    "\n",
    "MAX_LEN = 200\n",
    "BATCH_SIZE = 16\n",
    "TRAIN_EPOCHS = 1\n",
    "OUTPUT_BASE = \"./hf_finetune_results\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "7d939432",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import evaluate\n",
    "\n",
    "os.makedirs(OUTPUT_BASE, exist_ok=True)\n",
    "\n",
    "metric_accuracy = evaluate.load(\"accuracy\")\n",
    "metric_f1 = evaluate.load(\"f1\")\n",
    "metric_precision = evaluate.load(\"precision\")\n",
    "metric_recall = evaluate.load(\"recall\")\n",
    "\n",
    "def compute_metrics(pred):\n",
    "    labels = pred.label_ids\n",
    "    preds = np.argmax(pred.predictions, axis=1)\n",
    "\n",
    "    acc = metric_accuracy.compute(predictions=preds, references=labels)[\"accuracy\"]\n",
    "    f1w = metric_f1.compute(predictions=preds, references=labels, average=\"weighted\")[\"f1\"]\n",
    "    precw = metric_precision.compute(predictions=preds, references=labels, average=\"weighted\")[\"precision\"]\n",
    "    recw = metric_recall.compute(predictions=preds, references=labels, average=\"weighted\")[\"recall\"]\n",
    "\n",
    "    return {\"accuracy\": acc, \"precision\": precw, \"recall\": recw, \"f1\": f1w}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "b03bb16e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "1\n",
      "NVIDIA GeForce RTX 3060 Laptop GPU\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(torch.cuda.is_available())\n",
    "print(torch.cuda.device_count())\n",
    "print(torch.cuda.get_device_name(0) if torch.cuda.is_available() else \"No GPU\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "e8bb4a2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "Fine-tuning bert (bert-base-uncased)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Map: 100%|██████████| 4000/4000 [01:33<00:00, 42.93 examples/s]\n",
      "Map: 100%|██████████| 1000/1000 [00:25<00:00, 39.00 examples/s]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='250' max='250' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [250/250 00:47, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1.265100</td>\n",
       "      <td>1.184704</td>\n",
       "      <td>0.721000</td>\n",
       "      <td>0.622345</td>\n",
       "      <td>0.721000</td>\n",
       "      <td>0.665635</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Iskandar\\Documents\\GitHub\\Tubes-NLP-PaperMatch\\hf_env\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1731: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='63' max='63' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [63/63 00:02]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Iskandar\\Documents\\GitHub\\Tubes-NLP-PaperMatch\\hf_env\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1731: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval results for bert: {'eval_loss': 1.1847039461135864, 'eval_accuracy': 0.721, 'eval_precision': 0.62234500975747, 'eval_recall': 0.721, 'eval_f1': 0.6656353230281403, 'eval_runtime': 2.7989, 'eval_samples_per_second': 357.277, 'eval_steps_per_second': 22.508, 'epoch': 1.0}\n",
      "Model saved to: ./hf_finetune_results\\bert\\saved_model\n",
      "\n",
      "============================================================\n",
      "Fine-tuning roberta (roberta-base)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Map: 100%|██████████| 4000/4000 [00:39<00:00, 101.96 examples/s]\n",
      "Map: 100%|██████████| 1000/1000 [00:12<00:00, 81.36 examples/s]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='250' max='250' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [250/250 00:54, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1.400300</td>\n",
       "      <td>1.346017</td>\n",
       "      <td>0.627000</td>\n",
       "      <td>0.554996</td>\n",
       "      <td>0.627000</td>\n",
       "      <td>0.565865</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Iskandar\\Documents\\GitHub\\Tubes-NLP-PaperMatch\\hf_env\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1731: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='63' max='63' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [63/63 00:03]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Iskandar\\Documents\\GitHub\\Tubes-NLP-PaperMatch\\hf_env\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1731: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval results for roberta: {'eval_loss': 1.3460173606872559, 'eval_accuracy': 0.627, 'eval_precision': 0.554995664531772, 'eval_recall': 0.627, 'eval_f1': 0.5658652632064733, 'eval_runtime': 3.2183, 'eval_samples_per_second': 310.724, 'eval_steps_per_second': 19.576, 'epoch': 1.0}\n",
      "Model saved to: ./hf_finetune_results\\roberta\\saved_model\n",
      "\n",
      "============================================================\n",
      "Fine-tuning distilbert (distilbert-base-uncased)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Iskandar\\Documents\\GitHub\\Tubes-NLP-PaperMatch\\hf_env\\Lib\\site-packages\\huggingface_hub\\file_download.py:143: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\Iskandar\\.cache\\huggingface\\hub\\models--distilbert-base-uncased. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n",
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n",
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Map: 100%|██████████| 4000/4000 [01:25<00:00, 46.84 examples/s]\n",
      "Map: 100%|██████████| 1000/1000 [00:27<00:00, 36.99 examples/s]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='250' max='250' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [250/250 00:31, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1.388800</td>\n",
       "      <td>1.322352</td>\n",
       "      <td>0.674000</td>\n",
       "      <td>0.581264</td>\n",
       "      <td>0.674000</td>\n",
       "      <td>0.608250</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Iskandar\\Documents\\GitHub\\Tubes-NLP-PaperMatch\\hf_env\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1731: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='63' max='63' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [63/63 00:01]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Iskandar\\Documents\\GitHub\\Tubes-NLP-PaperMatch\\hf_env\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1731: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval results for distilbert: {'eval_loss': 1.3223522901535034, 'eval_accuracy': 0.674, 'eval_precision': 0.5812637288651479, 'eval_recall': 0.674, 'eval_f1': 0.6082499699791217, 'eval_runtime': 1.9651, 'eval_samples_per_second': 508.885, 'eval_steps_per_second': 32.06, 'epoch': 1.0}\n",
      "Model saved to: ./hf_finetune_results\\distilbert\\saved_model\n"
     ]
    }
   ],
   "source": [
    "results = {}\n",
    "\n",
    "for shortname, model_name in models_to_train.items():\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(f\"Fine-tuning {shortname} ({model_name})\")\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=len(labels))\n",
    "    ds_train = prepare_hf_dataset(train_texts, train_labels, tokenizer, max_length=MAX_LEN)\n",
    "    ds_val = prepare_hf_dataset(val_texts, val_labels, tokenizer, max_length=MAX_LEN)\n",
    "\n",
    "    training_args = TrainingArguments(\n",
    "        output_dir=os.path.join(OUTPUT_BASE, shortname),\n",
    "        num_train_epochs=TRAIN_EPOCHS,\n",
    "        per_device_train_batch_size=BATCH_SIZE,\n",
    "        per_device_eval_batch_size=BATCH_SIZE,\n",
    "        eval_strategy=\"epoch\",\n",
    "        logging_strategy=\"steps\",\n",
    "        logging_steps=50,\n",
    "        save_strategy=\"no\",\n",
    "        learning_rate=2e-5,\n",
    "        seed=42,\n",
    "        load_best_model_at_end=False,\n",
    "        report_to=[],\n",
    "        fp16=torch.cuda.is_available()\n",
    "    )\n",
    "\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=ds_train,\n",
    "        eval_dataset=ds_val,\n",
    "        compute_metrics=compute_metrics\n",
    "    )\n",
    "\n",
    "    trainer.train()\n",
    "    eval_res = trainer.evaluate()\n",
    "    print(f\"Eval results for {shortname}:\", eval_res)\n",
    "\n",
    "    save_dir = os.path.join(OUTPUT_BASE, shortname, \"saved_model\")\n",
    "    model.save_pretrained(save_dir)\n",
    "    tokenizer.save_pretrained(save_dir)\n",
    "\n",
    "    print(f\"Model saved to: {save_dir}\")\n",
    "    results[shortname] = {\"model_name\": model_name, \"eval\": eval_res, \"tokenizer\": tokenizer, \"trainer\": trainer}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "9b0d9a9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Model comparison on validation set:\n",
      "        model  accuracy  precision  recall        f1\n",
      "0        bert     0.721   0.622345   0.721  0.665635\n",
      "1  distilbert     0.674   0.581264   0.674  0.608250\n",
      "2     roberta     0.627   0.554996   0.627  0.565865\n"
     ]
    }
   ],
   "source": [
    "rows = []\n",
    "for k, v in results.items():\n",
    "    ev = v[\"eval\"]\n",
    "    rows.append({\n",
    "        \"model\": k,\n",
    "        \"accuracy\": ev.get(\"eval_accuracy\", ev.get(\"accuracy\")),\n",
    "        \"precision\": ev.get(\"eval_precision\", ev.get(\"precision\")),\n",
    "        \"recall\": ev.get(\"eval_recall\", ev.get(\"recall\")),\n",
    "        \"f1\": ev.get(\"eval_f1\", ev.get(\"f1\"))\n",
    "    })\n",
    "df_comp = pd.DataFrame(rows).sort_values(\"f1\", ascending=False).reset_index(drop=True)\n",
    "print(\"\\nModel comparison on validation set:\")\n",
    "print(df_comp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "4da753fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_trained_model(path):\n",
    "    tokenizer = AutoTokenizer.from_pretrained(path)\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(path)\n",
    "    return tokenizer, model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "86479c97",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertForSequenceClassification(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-11): 12 x BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSdpaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): BertPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       "  (classifier): Linear(in_features=768, out_features=20, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "save_dir = os.path.join(OUTPUT_BASE, \"bert\", \"saved_model\")\n",
    "\n",
    "tokenizer, model = load_trained_model(save_dir)\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "0e2ce9b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted label: math\n"
     ]
    }
   ],
   "source": [
    "def predict_text(text):\n",
    "    inputs = tokenizer(\n",
    "        text,\n",
    "        return_tensors=\"pt\",\n",
    "        truncation=True,\n",
    "        padding=\"max_length\",\n",
    "        max_length=MAX_LEN\n",
    "    )\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "    pred_class = torch.argmax(outputs.logits, dim=1).item()\n",
    "    return id2label[pred_class]\n",
    "\n",
    "example_text = \"AUTOMORPHISMS AND IDEALS OF NONCOMMUTATIVE DEF...\"\n",
    "print(\"Predicted label:\", predict_text(example_text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "544ab7a7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>field</th>\n",
       "      <th>fulltext</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3401</th>\n",
       "      <td>math</td>\n",
       "      <td>AUTOMORPHISMS AND IDEALS OF NONCOMMUTATIVE DEF...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4490</th>\n",
       "      <td>math</td>\n",
       "      <td>\\nSOME WEAK VERSIONS OF THE M1-SPACES\\n\\nFUCAI...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23226</th>\n",
       "      <td>math</td>\n",
       "      <td>\\nNoname manuscript No.\\n(will be inserted by ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16858</th>\n",
       "      <td>quant-ph</td>\n",
       "      <td>\\nA complicated Duffing oscillator in the surf...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18797</th>\n",
       "      <td>cs</td>\n",
       "      <td>1\\nInternational Journal of Scient ific &amp; E...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          field                                           fulltext\n",
       "3401       math  AUTOMORPHISMS AND IDEALS OF NONCOMMUTATIVE DEF...\n",
       "4490       math  \\nSOME WEAK VERSIONS OF THE M1-SPACES\\n\\nFUCAI...\n",
       "23226      math  \\nNoname manuscript No.\\n(will be inserted by ...\n",
       "16858  quant-ph  \\nA complicated Duffing oscillator in the surf...\n",
       "18797        cs     1\\nInternational Journal of Scient ific & E..."
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_small_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "689af8e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "Fine-tuning bert with LoRA (bert-base-uncased)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 605,204 || all params: 110,102,824 || trainable%: 0.5497\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 4000/4000 [01:32<00:00, 43.39 examples/s]\n",
      "Map: 100%|██████████| 1000/1000 [00:24<00:00, 41.24 examples/s]\n",
      "C:\\Users\\Iskandar\\AppData\\Local\\Temp\\ipykernel_7240\\3319961513.py:70: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='750' max='750' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [750/750 01:50, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1.368900</td>\n",
       "      <td>1.243051</td>\n",
       "      <td>0.640000</td>\n",
       "      <td>0.520870</td>\n",
       "      <td>0.640000</td>\n",
       "      <td>0.568359</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>1.135100</td>\n",
       "      <td>1.004200</td>\n",
       "      <td>0.703000</td>\n",
       "      <td>0.604814</td>\n",
       "      <td>0.703000</td>\n",
       "      <td>0.648634</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>1.015400</td>\n",
       "      <td>0.957339</td>\n",
       "      <td>0.710000</td>\n",
       "      <td>0.618754</td>\n",
       "      <td>0.710000</td>\n",
       "      <td>0.658708</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Iskandar\\Documents\\GitHub\\Tubes-NLP-PaperMatch\\hf_env\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1731: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\n",
      "c:\\Users\\Iskandar\\Documents\\GitHub\\Tubes-NLP-PaperMatch\\hf_env\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1731: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\n",
      "c:\\Users\\Iskandar\\Documents\\GitHub\\Tubes-NLP-PaperMatch\\hf_env\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1731: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='63' max='63' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [63/63 00:03]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Iskandar\\Documents\\GitHub\\Tubes-NLP-PaperMatch\\hf_env\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1731: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval results for bert: {'eval_loss': 0.9573387503623962, 'eval_accuracy': 0.71, 'eval_precision': 0.6187541557291949, 'eval_recall': 0.71, 'eval_f1': 0.6587084656251998, 'eval_runtime': 3.2426, 'eval_samples_per_second': 308.395, 'eval_steps_per_second': 19.429, 'epoch': 3.0}\n",
      "LoRA model saved to: ./hf_finetune_results_lora\\bert\\lora_model\n",
      "\n",
      "============================================================\n",
      "Fine-tuning roberta with LoRA (roberta-base)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "c:\\Users\\Iskandar\\Documents\\GitHub\\Tubes-NLP-PaperMatch\\hf_env\\Lib\\site-packages\\peft\\mapping_func.py:78: UserWarning: The PEFT config's `base_model_name_or_path` was renamed from 'bert-base-uncased' to 'roberta-base'. Please ensure that the correct base model is loaded when loading this checkpoint.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 1,195,796 || all params: 125,856,808 || trainable%: 0.9501\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 4000/4000 [00:39<00:00, 101.30 examples/s]\n",
      "Map: 100%|██████████| 1000/1000 [00:11<00:00, 88.42 examples/s]\n",
      "C:\\Users\\Iskandar\\AppData\\Local\\Temp\\ipykernel_7240\\3319961513.py:70: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='750' max='750' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [750/750 01:49, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1.061700</td>\n",
       "      <td>0.919921</td>\n",
       "      <td>0.715000</td>\n",
       "      <td>0.674144</td>\n",
       "      <td>0.715000</td>\n",
       "      <td>0.687791</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.977000</td>\n",
       "      <td>0.836819</td>\n",
       "      <td>0.732000</td>\n",
       "      <td>0.696458</td>\n",
       "      <td>0.732000</td>\n",
       "      <td>0.708573</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.820500</td>\n",
       "      <td>0.795536</td>\n",
       "      <td>0.744000</td>\n",
       "      <td>0.725588</td>\n",
       "      <td>0.744000</td>\n",
       "      <td>0.724323</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Iskandar\\Documents\\GitHub\\Tubes-NLP-PaperMatch\\hf_env\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1731: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\n",
      "c:\\Users\\Iskandar\\Documents\\GitHub\\Tubes-NLP-PaperMatch\\hf_env\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1731: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\n",
      "c:\\Users\\Iskandar\\Documents\\GitHub\\Tubes-NLP-PaperMatch\\hf_env\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1731: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='63' max='63' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [63/63 00:03]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Iskandar\\Documents\\GitHub\\Tubes-NLP-PaperMatch\\hf_env\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1731: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval results for roberta: {'eval_loss': 0.7955358624458313, 'eval_accuracy': 0.744, 'eval_precision': 0.7255876907374668, 'eval_recall': 0.744, 'eval_f1': 0.7243232750636168, 'eval_runtime': 3.2169, 'eval_samples_per_second': 310.855, 'eval_steps_per_second': 19.584, 'epoch': 3.0}\n",
      "LoRA model saved to: ./hf_finetune_results_lora\\roberta\\lora_model\n",
      "\n",
      "============================================================\n",
      "Fine-tuning distilbert with LoRA (distilbert-base-uncased)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "c:\\Users\\Iskandar\\Documents\\GitHub\\Tubes-NLP-PaperMatch\\hf_env\\Lib\\site-packages\\peft\\mapping_func.py:78: UserWarning: The PEFT config's `base_model_name_or_path` was renamed from 'roberta-base' to 'distilbert-base-uncased'. Please ensure that the correct base model is loaded when loading this checkpoint.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "No modules were targeted for adaptation. This might be caused by a combination of mismatched target modules and excluded modules. Please check your `target_modules` and `exclude_modules` configuration. You may also have only targeted modules that are marked to be saved (`modules_to_save`).",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[30]\u001b[39m\u001b[32m, line 43\u001b[39m\n\u001b[32m     37\u001b[39m model = AutoModelForSequenceClassification.from_pretrained(\n\u001b[32m     38\u001b[39m     model_name,\n\u001b[32m     39\u001b[39m     num_labels=\u001b[38;5;28mlen\u001b[39m(labels)\n\u001b[32m     40\u001b[39m )\n\u001b[32m     42\u001b[39m \u001b[38;5;66;03m# Wrap model dengan LoRA\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m43\u001b[39m model = \u001b[43mget_peft_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlora_config\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     44\u001b[39m model.print_trainable_parameters()\n\u001b[32m     46\u001b[39m \u001b[38;5;66;03m# Prepare dataset\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Iskandar\\Documents\\GitHub\\Tubes-NLP-PaperMatch\\hf_env\\Lib\\site-packages\\peft\\mapping_func.py:122\u001b[39m, in \u001b[36mget_peft_model\u001b[39m\u001b[34m(model, peft_config, adapter_name, mixed, autocast_adapter_dtype, revision, low_cpu_mem_usage)\u001b[39m\n\u001b[32m    113\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m peft_config.task_type \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m MODEL_TYPE_TO_PEFT_MODEL_MAPPING.keys() \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m peft_config.is_prompt_learning:\n\u001b[32m    114\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m PeftModel(\n\u001b[32m    115\u001b[39m         model,\n\u001b[32m    116\u001b[39m         peft_config,\n\u001b[32m   (...)\u001b[39m\u001b[32m    119\u001b[39m         low_cpu_mem_usage=low_cpu_mem_usage,\n\u001b[32m    120\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m122\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mMODEL_TYPE_TO_PEFT_MODEL_MAPPING\u001b[49m\u001b[43m[\u001b[49m\u001b[43mpeft_config\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtask_type\u001b[49m\u001b[43m]\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    123\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    124\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpeft_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    125\u001b[39m \u001b[43m    \u001b[49m\u001b[43madapter_name\u001b[49m\u001b[43m=\u001b[49m\u001b[43madapter_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    126\u001b[39m \u001b[43m    \u001b[49m\u001b[43mautocast_adapter_dtype\u001b[49m\u001b[43m=\u001b[49m\u001b[43mautocast_adapter_dtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    127\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlow_cpu_mem_usage\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlow_cpu_mem_usage\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    128\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Iskandar\\Documents\\GitHub\\Tubes-NLP-PaperMatch\\hf_env\\Lib\\site-packages\\peft\\peft_model.py:1656\u001b[39m, in \u001b[36mPeftModelForSequenceClassification.__init__\u001b[39m\u001b[34m(self, model, peft_config, adapter_name, **kwargs)\u001b[39m\n\u001b[32m   1651\u001b[39m         peft_config.modules_to_save.extend(classifier_module_names)\n\u001b[32m   1653\u001b[39m \u001b[38;5;66;03m# The modification of peft_config must happen before the init call as the `modules_to_save` information\u001b[39;00m\n\u001b[32m   1654\u001b[39m \u001b[38;5;66;03m# will be used to guard the target layer matching against matching `modules_to_save` layers. Only the\u001b[39;00m\n\u001b[32m   1655\u001b[39m \u001b[38;5;66;03m# config is relevant for this, the `modules_to_save` attribute can follow later.\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1656\u001b[39m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[34;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpeft_config\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43madapter_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1658\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(peft_config, \u001b[33m\"\u001b[39m\u001b[33mmodules_to_save\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m   1659\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m name, _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.base_model.named_children():\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Iskandar\\Documents\\GitHub\\Tubes-NLP-PaperMatch\\hf_env\\Lib\\site-packages\\peft\\peft_model.py:129\u001b[39m, in \u001b[36mPeftModel.__init__\u001b[39m\u001b[34m(self, model, peft_config, adapter_name, autocast_adapter_dtype, low_cpu_mem_usage)\u001b[39m\n\u001b[32m    127\u001b[39m     ctx = init_empty_weights \u001b[38;5;28;01mif\u001b[39;00m low_cpu_mem_usage \u001b[38;5;28;01melse\u001b[39;00m nullcontext\n\u001b[32m    128\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m ctx():\n\u001b[32m--> \u001b[39m\u001b[32m129\u001b[39m         \u001b[38;5;28mself\u001b[39m.base_model = \u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m{\u001b[49m\u001b[43madapter_name\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mpeft_config\u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43madapter_name\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    131\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m.base_model, \u001b[33m\"\u001b[39m\u001b[33m_cast_adapter_dtype\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m    132\u001b[39m     \u001b[38;5;28mself\u001b[39m.base_model._cast_adapter_dtype(\n\u001b[32m    133\u001b[39m         adapter_name=adapter_name, autocast_adapter_dtype=autocast_adapter_dtype\n\u001b[32m    134\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Iskandar\\Documents\\GitHub\\Tubes-NLP-PaperMatch\\hf_env\\Lib\\site-packages\\peft\\tuners\\tuners_utils.py:295\u001b[39m, in \u001b[36mBaseTuner.__init__\u001b[39m\u001b[34m(self, model, peft_config, adapter_name, low_cpu_mem_usage, state_dict)\u001b[39m\n\u001b[32m    293\u001b[39m \u001b[38;5;28mself\u001b[39m._pre_injection_hook(\u001b[38;5;28mself\u001b[39m.model, \u001b[38;5;28mself\u001b[39m.peft_config[adapter_name], adapter_name)\n\u001b[32m    294\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m peft_config != PeftType.XLORA \u001b[38;5;129;01mor\u001b[39;00m peft_config[adapter_name] != PeftType.XLORA:\n\u001b[32m--> \u001b[39m\u001b[32m295\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43minject_adapter\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43madapter_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlow_cpu_mem_usage\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlow_cpu_mem_usage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstate_dict\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstate_dict\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    297\u001b[39m \u001b[38;5;66;03m# Copy the peft_config in the injected model.\u001b[39;00m\n\u001b[32m    298\u001b[39m \u001b[38;5;28mself\u001b[39m.model.peft_config = \u001b[38;5;28mself\u001b[39m.peft_config\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Iskandar\\Documents\\GitHub\\Tubes-NLP-PaperMatch\\hf_env\\Lib\\site-packages\\peft\\tuners\\tuners_utils.py:890\u001b[39m, in \u001b[36mBaseTuner.inject_adapter\u001b[39m\u001b[34m(self, model, adapter_name, autocast_adapter_dtype, low_cpu_mem_usage, state_dict)\u001b[39m\n\u001b[32m    888\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(peft_config, \u001b[33m\"\u001b[39m\u001b[33mlayers_pattern\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    889\u001b[39m             error_msg += \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m You also specified \u001b[39m\u001b[33m'\u001b[39m\u001b[33mlayers_pattern\u001b[39m\u001b[33m'\u001b[39m\u001b[33m: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpeft_config.layers_pattern\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m--> \u001b[39m\u001b[32m890\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(error_msg)\n\u001b[32m    892\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(peft_config, \u001b[33m\"\u001b[39m\u001b[33mexclude_modules\u001b[39m\u001b[33m\"\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m peft_config.exclude_modules \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m excluded_modules:\n\u001b[32m    893\u001b[39m     \u001b[38;5;66;03m# exclude_modules was passed but was not used\u001b[39;00m\n\u001b[32m    894\u001b[39m     warnings.warn(\n\u001b[32m    895\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mYou have passed exclude_modules=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpeft_config.exclude_modules\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m but no modules were excluded. \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    896\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mPlease check that exclude_modules was set correctly.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    897\u001b[39m     )\n",
      "\u001b[31mValueError\u001b[39m: No modules were targeted for adaptation. This might be caused by a combination of mismatched target modules and excluded modules. Please check your `target_modules` and `exclude_modules` configuration. You may also have only targeted modules that are marked to be saved (`modules_to_save`)."
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, Trainer, TrainingArguments, EarlyStoppingCallback\n",
    "from peft import LoraConfig, get_peft_model, TaskType\n",
    "import torch, os\n",
    "\n",
    "# ============================================================\n",
    "# Config\n",
    "# ============================================================\n",
    "MAX_LEN = 200\n",
    "BATCH_SIZE = 16\n",
    "TRAIN_EPOCHS = 3\n",
    "OUTPUT_BASE = \"./hf_finetune_results_lora\"\n",
    "os.makedirs(OUTPUT_BASE, exist_ok=True)\n",
    "\n",
    "# ============================================================\n",
    "# LoRA Config\n",
    "# ============================================================\n",
    "lora_config = LoraConfig(\n",
    "    task_type=TaskType.SEQ_CLS,\n",
    "    r=16,\n",
    "    lora_alpha=32,\n",
    "    lora_dropout=0.1,\n",
    "    bias=\"none\",\n",
    "    target_modules=[\"query\", \"value\"]\n",
    ")\n",
    "\n",
    "# ============================================================\n",
    "# Loop training\n",
    "# ============================================================\n",
    "results = {}\n",
    "\n",
    "for shortname, model_name in models_to_train.items():\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Fine-tuning {shortname} with LoRA ({model_name})\")\n",
    "\n",
    "    # Tokenizer & base model\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(\n",
    "        model_name,\n",
    "        num_labels=len(labels)\n",
    "    )\n",
    "\n",
    "    # Wrap model dengan LoRA\n",
    "    model = get_peft_model(model, lora_config)\n",
    "    model.print_trainable_parameters()\n",
    "\n",
    "    # Prepare dataset\n",
    "    ds_train = prepare_hf_dataset(train_texts, train_labels, tokenizer, max_length=MAX_LEN)\n",
    "    ds_val   = prepare_hf_dataset(val_texts, val_labels, tokenizer, max_length=MAX_LEN)\n",
    "\n",
    "    # TrainingArguments\n",
    "    training_args = TrainingArguments(\n",
    "        output_dir=os.path.join(OUTPUT_BASE, shortname),\n",
    "        num_train_epochs=TRAIN_EPOCHS,\n",
    "        per_device_train_batch_size=BATCH_SIZE,\n",
    "        per_device_eval_batch_size=BATCH_SIZE,\n",
    "        eval_strategy=\"epoch\",\n",
    "        logging_strategy=\"steps\",\n",
    "        logging_steps=50,\n",
    "        save_strategy=\"epoch\",\n",
    "        learning_rate=2e-4,\n",
    "        fp16=torch.cuda.is_available(),\n",
    "        save_total_limit=2,\n",
    "        metric_for_best_model=\"accuracy\",\n",
    "        load_best_model_at_end=True,\n",
    "        greater_is_better=True,\n",
    "        report_to=[]\n",
    "    )\n",
    "\n",
    "    # Trainer\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=ds_train,\n",
    "        eval_dataset=ds_val,\n",
    "        tokenizer=tokenizer,  # tambahkan tokenizer\n",
    "        compute_metrics=compute_metrics,\n",
    "        callbacks=[EarlyStoppingCallback(early_stopping_patience=2)]\n",
    "    )\n",
    "\n",
    "    # Train & evaluate\n",
    "    trainer.train()\n",
    "    eval_res = trainer.evaluate()\n",
    "    print(f\"Eval results for {shortname}:\", eval_res)\n",
    "\n",
    "    # Save LoRA model\n",
    "    save_dir = os.path.join(OUTPUT_BASE, shortname, \"lora_model\")\n",
    "    model.save_pretrained(save_dir)\n",
    "    tokenizer.save_pretrained(save_dir)\n",
    "    print(f\"LoRA model saved to: {save_dir}\")\n",
    "\n",
    "    results[shortname] = {\n",
    "        \"model_name\": model_name,\n",
    "        \"eval\": eval_res,\n",
    "        \"save_dir\": save_dir,\n",
    "        \"tokenizer\": tokenizer\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "d5bc5468",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "PeftModelForSequenceClassification(\n",
       "  (base_model): LoraModel(\n",
       "    (model): RobertaForSequenceClassification(\n",
       "      (roberta): RobertaModel(\n",
       "        (embeddings): RobertaEmbeddings(\n",
       "          (word_embeddings): Embedding(50265, 768, padding_idx=1)\n",
       "          (position_embeddings): Embedding(514, 768, padding_idx=1)\n",
       "          (token_type_embeddings): Embedding(1, 768)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (encoder): RobertaEncoder(\n",
       "          (layer): ModuleList(\n",
       "            (0-11): 12 x RobertaLayer(\n",
       "              (attention): RobertaAttention(\n",
       "                (self): RobertaSdpaSelfAttention(\n",
       "                  (query): lora.Linear(\n",
       "                    (base_layer): Linear(in_features=768, out_features=768, bias=True)\n",
       "                    (lora_dropout): ModuleDict(\n",
       "                      (default): Dropout(p=0.1, inplace=False)\n",
       "                    )\n",
       "                    (lora_A): ModuleDict(\n",
       "                      (default): Linear(in_features=768, out_features=16, bias=False)\n",
       "                    )\n",
       "                    (lora_B): ModuleDict(\n",
       "                      (default): Linear(in_features=16, out_features=768, bias=False)\n",
       "                    )\n",
       "                    (lora_embedding_A): ParameterDict()\n",
       "                    (lora_embedding_B): ParameterDict()\n",
       "                    (lora_magnitude_vector): ModuleDict()\n",
       "                  )\n",
       "                  (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "                  (value): lora.Linear(\n",
       "                    (base_layer): Linear(in_features=768, out_features=768, bias=True)\n",
       "                    (lora_dropout): ModuleDict(\n",
       "                      (default): Dropout(p=0.1, inplace=False)\n",
       "                    )\n",
       "                    (lora_A): ModuleDict(\n",
       "                      (default): Linear(in_features=768, out_features=16, bias=False)\n",
       "                    )\n",
       "                    (lora_B): ModuleDict(\n",
       "                      (default): Linear(in_features=16, out_features=768, bias=False)\n",
       "                    )\n",
       "                    (lora_embedding_A): ParameterDict()\n",
       "                    (lora_embedding_B): ParameterDict()\n",
       "                    (lora_magnitude_vector): ModuleDict()\n",
       "                  )\n",
       "                  (dropout): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "                (output): RobertaSelfOutput(\n",
       "                  (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "                  (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "                  (dropout): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "              )\n",
       "              (intermediate): RobertaIntermediate(\n",
       "                (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "                (intermediate_act_fn): GELUActivation()\n",
       "              )\n",
       "              (output): RobertaOutput(\n",
       "                (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "                (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (classifier): ModulesToSaveWrapper(\n",
       "        (original_module): RobertaClassificationHead(\n",
       "          (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (out_proj): Linear(in_features=768, out_features=20, bias=True)\n",
       "        )\n",
       "        (modules_to_save): ModuleDict(\n",
       "          (default): RobertaClassificationHead(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (out_proj): Linear(in_features=768, out_features=20, bias=True)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "from peft import PeftModel\n",
    "\n",
    "save_dir = \"./hf_finetune_results_lora/roberta/lora_model\"\n",
    "base_model_name = \"roberta-base\"\n",
    "\n",
    "# Load base model\n",
    "base_model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    base_model_name,\n",
    "    num_labels=len(labels)\n",
    ")\n",
    "# Wrap dengan LoRA\n",
    "model = PeftModel.from_pretrained(base_model, save_dir)\n",
    "tokenizer = AutoTokenizer.from_pretrained(save_dir)\n",
    "model.eval()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "087c686e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cs\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Prediksi\n",
    "def predict_text(text):\n",
    "    inputs = tokenizer(\n",
    "        text,\n",
    "        return_tensors=\"pt\",\n",
    "        truncation=True,\n",
    "        padding=\"max_length\",\n",
    "        max_length=MAX_LEN\n",
    "    )\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "    pred_class = torch.argmax(outputs.logits, dim=1).item()\n",
    "    return id2label[pred_class]\n",
    "\n",
    "print(predict_text(\"AUTOMORPHISMS AND IDEALS OF NONCOMMUTATIVE DEF...\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "7038ef2a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>field</th>\n",
       "      <th>fulltext</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3401</th>\n",
       "      <td>math</td>\n",
       "      <td>AUTOMORPHISMS AND IDEALS OF NONCOMMUTATIVE DEF...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4490</th>\n",
       "      <td>math</td>\n",
       "      <td>\\nSOME WEAK VERSIONS OF THE M1-SPACES\\n\\nFUCAI...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23226</th>\n",
       "      <td>math</td>\n",
       "      <td>\\nNoname manuscript No.\\n(will be inserted by ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16858</th>\n",
       "      <td>quant-ph</td>\n",
       "      <td>\\nA complicated Duffing oscillator in the surf...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18797</th>\n",
       "      <td>cs</td>\n",
       "      <td>1\\nInternational Journal of Scient ific &amp; E...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          field                                           fulltext\n",
       "3401       math  AUTOMORPHISMS AND IDEALS OF NONCOMMUTATIVE DEF...\n",
       "4490       math  \\nSOME WEAK VERSIONS OF THE M1-SPACES\\n\\nFUCAI...\n",
       "23226      math  \\nNoname manuscript No.\\n(will be inserted by ...\n",
       "16858  quant-ph  \\nA complicated Duffing oscillator in the surf...\n",
       "18797        cs     1\\nInternational Journal of Scient ific & E..."
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_small_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ed98488",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hf_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
