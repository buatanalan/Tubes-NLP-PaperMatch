{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "edd23238",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "ds = load_dataset(\"allenai/multixscience_sparse_mean\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5c593b3c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>aid</th>\n",
       "      <th>mid</th>\n",
       "      <th>abstract</th>\n",
       "      <th>related_work</th>\n",
       "      <th>ref_abstract</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>math9912167</td>\n",
       "      <td>1631980677</td>\n",
       "      <td>Author(s): Kuperberg, Greg; Thurston, Dylan P....</td>\n",
       "      <td>Two other generalizations that can be consider...</td>\n",
       "      <td>{'cite_N': ['@cite_16', '@cite_26'], 'mid': ['...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>cs9910011</td>\n",
       "      <td>2168463568</td>\n",
       "      <td>A statistical model for segmentation and word ...</td>\n",
       "      <td>Model Based Dynamic Programming, hereafter ref...</td>\n",
       "      <td>{'cite_N': ['@cite_0'], 'mid': ['2074546930'],...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>cs9911003</td>\n",
       "      <td>2950670108</td>\n",
       "      <td>We solve the subgraph isomorphism problem in p...</td>\n",
       "      <td>Recently we were able to characterize the grap...</td>\n",
       "      <td>{'cite_N': ['@cite_41'], 'mid': ['2074992286']...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>hep-th9908200</td>\n",
       "      <td>2160091034</td>\n",
       "      <td>Daviau showed the equivalence of matrix Dirac ...</td>\n",
       "      <td>A further genuine and important approach to th...</td>\n",
       "      <td>{'cite_N': ['@cite_6'], 'mid': ['2082565556'],...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>cs9903014</td>\n",
       "      <td>1612660921</td>\n",
       "      <td>We present an open architecture for just-in-ti...</td>\n",
       "      <td>Pioneering research in dynamic runtime optimiz...</td>\n",
       "      <td>{'cite_N': ['@cite_8'], 'mid': ['2101776604'],...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             aid         mid  \\\n",
       "0    math9912167  1631980677   \n",
       "1      cs9910011  2168463568   \n",
       "2      cs9911003  2950670108   \n",
       "3  hep-th9908200  2160091034   \n",
       "4      cs9903014  1612660921   \n",
       "\n",
       "                                            abstract  \\\n",
       "0  Author(s): Kuperberg, Greg; Thurston, Dylan P....   \n",
       "1  A statistical model for segmentation and word ...   \n",
       "2  We solve the subgraph isomorphism problem in p...   \n",
       "3  Daviau showed the equivalence of matrix Dirac ...   \n",
       "4  We present an open architecture for just-in-ti...   \n",
       "\n",
       "                                        related_work  \\\n",
       "0  Two other generalizations that can be consider...   \n",
       "1  Model Based Dynamic Programming, hereafter ref...   \n",
       "2  Recently we were able to characterize the grap...   \n",
       "3  A further genuine and important approach to th...   \n",
       "4  Pioneering research in dynamic runtime optimiz...   \n",
       "\n",
       "                                        ref_abstract  \n",
       "0  {'cite_N': ['@cite_16', '@cite_26'], 'mid': ['...  \n",
       "1  {'cite_N': ['@cite_0'], 'mid': ['2074546930'],...  \n",
       "2  {'cite_N': ['@cite_41'], 'mid': ['2074992286']...  \n",
       "3  {'cite_N': ['@cite_6'], 'mid': ['2082565556'],...  \n",
       "4  {'cite_N': ['@cite_8'], 'mid': ['2101776604'],...  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df = ds[\"train\"].to_pandas()\n",
    "\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2811a310",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> Memproses TRAIN Split (Membuat Pasangan)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 30369/30369 [00:03<00:00, 10027.02it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> Memproses VALIDATION Split...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5066/5066 [00:00<00:00, 10136.34it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> Memproses TEST Split...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5093/5093 [00:00<00:00, 12539.49it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> Menyusun Corpus Unik...\n",
      "\n",
      "--- RINGKASAN DATA ---\n",
      "1. Training Pairs (Finetune) : 97292 pasang\n",
      "2. Validation Queries        : 5066 soal\n",
      "3. Test Queries (Ujian)      : 5093 soal\n",
      "4. Corpus (Isi ChromaDB)     : 74015 dokumen unik\n",
      "Tersimpan: final_dataset_retrieval\\train_pairs.json\n",
      "Tersimpan: final_dataset_retrieval\\val_queries.json\n",
      "Tersimpan: final_dataset_retrieval\\test_queries.json\n",
      "Tersimpan: final_dataset_retrieval\\corpus.json\n",
      "\n",
      "SELESAI! Gunakan file di folder 'final_dataset_skripsi'.\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import os\n",
    "import hashlib\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Setup\n",
    "output_dir = \"final_dataset_retrieval\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "def get_hash(text):\n",
    "    \"\"\"Membuat ID unik berdasarkan konten teks\"\"\"\n",
    "    return hashlib.md5(text.encode('utf-8')).hexdigest()\n",
    "\n",
    "# Variabel penampung\n",
    "seen_train_pairs = set()\n",
    "corpus_set = set() # Untuk menampung semua dokumen unik yang akan masuk Chroma\n",
    "train_data = []\n",
    "val_data = []\n",
    "test_data = []\n",
    "\n",
    "# Proses TRAIN Split (Untuk Fine-Tuning Model)\n",
    "print(\">>> Memproses TRAIN Split (Membuat Pasangan)...\")\n",
    "for row in tqdm(ds['train']):\n",
    "    anchor = row['abstract']\n",
    "    refs = row['ref_abstract']['abstract']\n",
    "    \n",
    "    if not anchor or len(anchor) < 50: continue\n",
    "    if not refs: continue\n",
    "\n",
    "    for ref in refs:\n",
    "        if not ref or len(ref) < 50: continue\n",
    "        if anchor == ref: continue \n",
    "        \n",
    "        # Simpan ke Corpus (Database Chroma nanti)\n",
    "        corpus_set.add(ref)\n",
    "        \n",
    "        # Deduplikasi Pasangan untuk Training\n",
    "        pair_sig = (get_hash(anchor), get_hash(ref))\n",
    "        if pair_sig not in seen_train_pairs:\n",
    "            seen_train_pairs.add(pair_sig)\n",
    "            train_data.append({\n",
    "                \"anchor\": anchor,\n",
    "                \"positive\": ref\n",
    "            })\n",
    "\n",
    "# Proses VALIDATION & TEST Split (Untuk Evaluasi)\n",
    "# Di sini kita TIDAK flatten jadi pasangan, tapi simpan format:\n",
    "# { \"query\": \"...\", \"ground_truths\": [\"...\", \"...\"] }\n",
    "# Agar kita bisa hitung Recall@10 (berapa banyak GT yang ketangkap)\n",
    "\n",
    "def process_eval_split(split_name, dataset_split):\n",
    "    processed = []\n",
    "    print(f\">>> Memproses {split_name} Split...\")\n",
    "    for row in tqdm(dataset_split):\n",
    "        query = row['abstract']\n",
    "        refs = row['ref_abstract']['abstract']\n",
    "        \n",
    "        if not query or len(query) < 50: continue\n",
    "        \n",
    "        valid_refs = []\n",
    "        for ref in refs:\n",
    "            if ref and len(ref) > 50:\n",
    "                valid_refs.append(ref)\n",
    "                corpus_set.add(ref)\n",
    "        \n",
    "        if valid_refs:\n",
    "            processed.append({\n",
    "                \"query\": query,\n",
    "                \"ground_truths\": valid_refs\n",
    "            })\n",
    "    return processed\n",
    "\n",
    "val_data = process_eval_split(\"VALIDATION\", ds['validation'])\n",
    "test_data = process_eval_split(\"TEST\", ds['test'])\n",
    "\n",
    "# Finalisasi Corpus (Isi ChromaDB)\n",
    "print(\">>> Menyusun Corpus Unik...\")\n",
    "# Ubah set menjadi list of dict biar rapi\n",
    "corpus_list = [{\"id\": get_hash(text), \"text\": text} for text in corpus_set]\n",
    "\n",
    "# Simpan ke File\n",
    "print(f\"\\n--- RINGKASAN DATA ---\")\n",
    "print(f\"1. Training Pairs (Finetune) : {len(train_data)} pasang\")\n",
    "print(f\"2. Validation Queries        : {len(val_data)} soal\")\n",
    "print(f\"3. Test Queries (Ujian)      : {len(test_data)} soal\")\n",
    "print(f\"4. Corpus (Isi ChromaDB)     : {len(corpus_list)} dokumen unik\")\n",
    "\n",
    "def save_json(data, filename):\n",
    "    path = os.path.join(output_dir, filename)\n",
    "    with open(path, 'w', encoding='utf-8') as f:\n",
    "        json.dump(data, f, ensure_ascii=False, indent=4)\n",
    "    print(f\"Tersimpan: {path}\")\n",
    "\n",
    "save_json(train_data, \"train_pairs.json\")\n",
    "save_json(val_data, \"val_queries.json\")\n",
    "save_json(test_data, \"test_queries.json\")\n",
    "save_json(corpus_list, \"corpus.json\")\n",
    "\n",
    "print(\"\\nSELESAI! Gunakan file di folder 'final_dataset_retrieval'.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f64fde7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================== TRAIN_PAIRS.JSON ====================\n",
      "Tipe Data  : TRAIN PAIRS\n",
      "Total Data : 97292 item\n",
      "\n",
      "CONTOH DATA PERTAMA:\n",
      "   [Anchor]   : Author(s): Kuperberg, Greg; Thurston, Dylan P. | Abstract: We give a purely topological definition o...\n",
      "   [Positive] : This note is a sequel to our earlier paper of the same title [4] and describes invariants of rationa...\n",
      "\n",
      "==================== VAL_QUERIES.JSON ====================\n",
      "Tipe Data  : QUERIES (Test/Val)\n",
      "Total Data : 5066 item\n",
      "\n",
      "CONTOH DATA PERTAMA:\n",
      "   [Query]         : One of the key concepts in testing is that of adequate test sets. A test selection criterion decides...\n",
      "   [Ground Truths] : Ada 1 jawaban benar.\n",
      "     -> Jwbn 1     : An approach to functional testing is described in which the design of a program is viewed as an inte...\n",
      "\n",
      "==================== TEST_QUERIES.JSON ====================\n",
      "Tipe Data  : QUERIES (Test/Val)\n",
      "Total Data : 5093 item\n",
      "\n",
      "CONTOH DATA PERTAMA:\n",
      "   [Query]         : We present our approach to the problem of how an agent, within an economic Multi-Agent System, can d...\n",
      "   [Ground Truths] : Ada 4 jawaban benar.\n",
      "     -> Jwbn 1     : Cooperative multi-agent systems (MAS) are ones in which several agents attempt, through their intera...\n",
      "\n",
      "==================== CORPUS.JSON ====================\n",
      "Tipe Data  : CORPUS\n",
      "Total Data : 74015 item\n",
      "\n",
      "CONTOH DATA PERTAMA:\n",
      "   [ID Unik] : b7be15378d838f82c027523b0df945ef\n",
      "   [Text]    : It is shown how a system consisting of two neuronlike adaptive elements can solve a difficult learni...\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import os\n",
    "\n",
    "output_dir = \"final_dataset_retrieval\"\n",
    "\n",
    "def load_and_inspect(filename, file_type):\n",
    "    path = os.path.join(output_dir, filename)\n",
    "    \n",
    "    if not os.path.exists(path):\n",
    "        print(f\"File tidak ditemukan: {path}\")\n",
    "        return\n",
    "\n",
    "    print(f\"\\n{'='*20} {filename.upper()} {'='*20}\")\n",
    "    \n",
    "    with open(path, 'r', encoding='utf-8') as f:\n",
    "        data = json.load(f)\n",
    "    \n",
    "    print(f\"Tipe Data  : {file_type}\")\n",
    "    print(f\"Total Data : {len(data)} item\")\n",
    "    \n",
    "    sample = data[0]\n",
    "    \n",
    "    print(\"\\nCONTOH DATA PERTAMA:\")\n",
    "    if file_type == \"TRAIN PAIRS\":\n",
    "        print(f\"   [Anchor]   : {sample['anchor'][:100]}...\")\n",
    "        print(f\"   [Positive] : {sample['positive'][:100]}...\")\n",
    "        \n",
    "    elif file_type == \"QUERIES (Test/Val)\":\n",
    "        print(f\"   [Query]         : {sample['query'][:100]}...\")\n",
    "        print(f\"   [Ground Truths] : Ada {len(sample['ground_truths'])} jawaban benar.\")\n",
    "        print(f\"     -> Jwbn 1     : {sample['ground_truths'][0][:100]}...\")\n",
    "        \n",
    "    elif file_type == \"CORPUS\":\n",
    "        print(f\"   [ID Unik] : {sample['id']}\")\n",
    "        print(f\"   [Text]    : {sample['text'][:100]}...\")\n",
    "\n",
    "load_and_inspect(\"train_pairs.json\", \"TRAIN PAIRS\")\n",
    "load_and_inspect(\"val_queries.json\", \"QUERIES (Test/Val)\")\n",
    "load_and_inspect(\"test_queries.json\", \"QUERIES (Test/Val)\")\n",
    "load_and_inspect(\"corpus.json\", \"CORPUS\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8dec5e96",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. Loading Dataset...\n",
      "Menyiapkan Evaluator Standard\n",
      "   - Siap mengevaluasi 5093 queries terhadap 74015 dokumen.\n",
      "\n",
      "==================================================\n",
      "EXPERIMENT: all-MiniLM-L6-v2\n",
      "==================================================\n",
      "A. Menguji Baseline all-MiniLM-L6-v2...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 160/160 [00:08<00:00, 19.73it/s]\n",
      "Batches: 100%|██████████| 1563/1563 [01:14<00:00, 20.97it/s]\n",
      "Batches: 100%|██████████| 751/751 [00:36<00:00, 20.31it/s]t]\n",
      "Corpus Chunks: 100%|██████████| 2/2 [01:54<00:00, 57.29s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['test_cosine_accuracy@1', 'test_cosine_accuracy@5', 'test_cosine_accuracy@10', 'test_cosine_precision@10', 'test_cosine_recall@10', 'test_cosine_ndcg@10', 'test_cosine_mrr@10', 'test_cosine_map@100'])\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'test_accuracy@10'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyError\u001b[39m                                  Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 155\u001b[39m\n\u001b[32m    153\u001b[39m \u001b[38;5;66;03m# EKSEKUSI\u001b[39;00m\n\u001b[32m    154\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m model_name \u001b[38;5;129;01min\u001b[39;00m MODELS_TO_TEST:\n\u001b[32m--> \u001b[39m\u001b[32m155\u001b[39m     \u001b[43mrun_experiment\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    157\u001b[39m \u001b[38;5;66;03m# TAMPILKAN HASIL AKHIR\u001b[39;00m\n\u001b[32m    158\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m + \u001b[33m\"\u001b[39m\u001b[33m=\u001b[39m\u001b[33m\"\u001b[39m*\u001b[32m20\u001b[39m + \u001b[33m\"\u001b[39m\u001b[33m HASIL AKHIR RETRIEVAL \u001b[39m\u001b[33m\"\u001b[39m + \u001b[33m\"\u001b[39m\u001b[33m=\u001b[39m\u001b[33m\"\u001b[39m*\u001b[32m20\u001b[39m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 101\u001b[39m, in \u001b[36mrun_experiment\u001b[39m\u001b[34m(model_name)\u001b[39m\n\u001b[32m     95\u001b[39m \u001b[38;5;28mprint\u001b[39m(metrics_base.keys())\n\u001b[32m     97\u001b[39m \u001b[38;5;66;03m# Simpan hasil\u001b[39;00m\n\u001b[32m     98\u001b[39m results_table.append({\n\u001b[32m     99\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mModel\u001b[39m\u001b[33m\"\u001b[39m: short_name,\n\u001b[32m    100\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mType\u001b[39m\u001b[33m\"\u001b[39m: \u001b[33m\"\u001b[39m\u001b[33mBaseline (Pre-trained)\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m--> \u001b[39m\u001b[32m101\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mRecall@10\u001b[39m\u001b[33m\"\u001b[39m: \u001b[43mmetrics_base\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mtest_accuracy@10\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m, \u001b[38;5;66;03m# Hit Rate\u001b[39;00m\n\u001b[32m    102\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mMRR@10\u001b[39m\u001b[33m\"\u001b[39m: metrics_base[\u001b[33m'\u001b[39m\u001b[33mtest_mrr@10\u001b[39m\u001b[33m'\u001b[39m]\n\u001b[32m    103\u001b[39m })\n\u001b[32m    104\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m   -> Baseline Recall@10: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmetrics_base[\u001b[33m'\u001b[39m\u001b[33mtest_accuracy@10\u001b[39m\u001b[33m'\u001b[39m]\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m    105\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m   -> Baseline MRR@10   : \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmetrics_base[\u001b[33m'\u001b[39m\u001b[33mtest_mrr@10\u001b[39m\u001b[33m'\u001b[39m]\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mKeyError\u001b[39m: 'test_accuracy@10'"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import os\n",
    "import pandas as pd\n",
    "from sentence_transformers import SentenceTransformer, InputExample, losses, models, evaluation\n",
    "from torch.utils.data import DataLoader\n",
    "import torch\n",
    "\n",
    "# Config\n",
    "DATA_DIR = \"final_dataset_retrieval\"\n",
    "OUTPUT_DIR = \"models-retrieval\"\n",
    "BATCH_SIZE = 16\n",
    "EPOCHS = 3\n",
    "\n",
    "# Daftar Model yang akan diuji\n",
    "MODELS_TO_TEST = [\n",
    "    \"sentence-transformers/all-MiniLM-L6-v2\",\n",
    "    \"sentence-transformers/all-mpnet-base-v2\"\n",
    "]\n",
    "\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "# Load Data Function\n",
    "def load_json(filename):\n",
    "    with open(os.path.join(DATA_DIR, filename), 'r', encoding='utf-8') as f:\n",
    "        return json.load(f)\n",
    "\n",
    "print(\"1. Loading Dataset...\")\n",
    "train_raw = load_json(\"train_pairs.json\")\n",
    "test_queries_raw = load_json(\"test_queries.json\")\n",
    "corpus_raw = load_json(\"corpus.json\")\n",
    "\n",
    "# Evaluator Setup\n",
    "# Evaluator butuh format khusus: Corpus Dict, Queries Dict, dan Relevance Dict\n",
    "print(\"Menyiapkan Evaluator Standard\")\n",
    "\n",
    "# Buat Dictionary Corpus {id: text} & Reverse Map {text: id} untuk lookup\n",
    "corpus_dict = {}\n",
    "text_to_id_map = {}\n",
    "\n",
    "for item in corpus_raw:\n",
    "    doc_id = item['id']\n",
    "    text = item['text']\n",
    "    corpus_dict[doc_id] = text\n",
    "    text_to_id_map[text] = doc_id\n",
    "\n",
    "# Buat Dictionary Queries & Relevance\n",
    "queries_dict = {}\n",
    "relevant_docs = {}\n",
    "\n",
    "for i, item in enumerate(test_queries_raw):\n",
    "    qid = f\"q_{i}\"\n",
    "    queries_dict[qid] = item['query']\n",
    "    \n",
    "    # Cari ID dokumen untuk setiap ground truth text\n",
    "    ground_truth_ids = set()\n",
    "    for gt_text in item['ground_truths']:\n",
    "        found_id = text_to_id_map.get(gt_text)\n",
    "        if found_id:\n",
    "            ground_truth_ids.add(found_id)\n",
    "            \n",
    "    if ground_truth_ids:\n",
    "        relevant_docs[qid] = ground_truth_ids\n",
    "\n",
    "# Setup Object Evaluator\n",
    "ir_evaluator = evaluation.InformationRetrievalEvaluator(\n",
    "    queries_dict,\n",
    "    corpus_dict,\n",
    "    relevant_docs,\n",
    "    show_progress_bar=True,\n",
    "    name=\"test\",\n",
    "    mrr_at_k=[10],\n",
    "    ndcg_at_k=[10],\n",
    "    accuracy_at_k=[1, 5, 10], # Ini sama dengan Recall@K / Hit Rate\n",
    "    precision_recall_at_k=[10]\n",
    ")\n",
    "\n",
    "print(f\"   - Siap mengevaluasi {len(queries_dict)} queries terhadap {len(corpus_dict)} dokumen.\")\n",
    "\n",
    "# FUNGSI TRAINING & EVALUASI \n",
    "results_table = []\n",
    "\n",
    "def run_experiment(model_name):\n",
    "    short_name = model_name.split(\"/\")[-1]\n",
    "    print(f\"\\n{'='*50}\")\n",
    "    print(f\"EXPERIMENT: {short_name}\")\n",
    "    print(f\"{'='*50}\")\n",
    "\n",
    "    # --- PHASE A: BASELINE EVALUATION (Sebelum Fine-tune) ---\n",
    "    print(f\"A. Menguji Baseline {short_name}...\")\n",
    "    model_baseline = SentenceTransformer(model_name)\n",
    "    \n",
    "    # Run Evaluator\n",
    "    metrics_base = ir_evaluator(model_baseline)\n",
    "\n",
    "    print(metrics_base.keys())\n",
    "    \n",
    "    # Simpan hasil\n",
    "    results_table.append({\n",
    "        \"Model\": short_name,\n",
    "        \"Type\": \"Baseline (Pre-trained)\",\n",
    "        \"Recall@10\": metrics_base['test_accuracy@10'], # Hit Rate\n",
    "        \"MRR@10\": metrics_base['test_mrr@10']\n",
    "    })\n",
    "    print(f\"   -> Baseline Recall@10: {metrics_base['test_accuracy@10']:.4f}\")\n",
    "    print(f\"   -> Baseline MRR@10   : {metrics_base['test_mrr@10']:.4f}\")\n",
    "    \n",
    "    del model_baseline # Hapus dari RAM\n",
    "    torch.cuda.empty_cache() # Bersihkan GPU\n",
    "\n",
    "    # --- PHASE B: FINE-TUNING ---\n",
    "    print(f\"B. Melakukan Fine-Tuning {short_name}...\")\n",
    "    \n",
    "    # Setup Model (Max Length 300 agar aman memori)\n",
    "    word_emb = models.Transformer(model_name, max_seq_length=300)\n",
    "    pooling = models.Pooling(word_emb.get_word_embedding_dimension())\n",
    "    model_finetune = SentenceTransformer(modules=[word_emb, pooling])\n",
    "    \n",
    "    # Setup Data Loader\n",
    "    train_examples = [InputExample(texts=[d['anchor'], d['positive']]) for d in train_raw]\n",
    "    train_dataloader = DataLoader(train_examples, shuffle=True, batch_size=BATCH_SIZE)\n",
    "    \n",
    "    # Loss\n",
    "    train_loss = losses.MultipleNegativesRankingLoss(model_finetune)\n",
    "    \n",
    "    # Train\n",
    "    save_path = os.path.join(OUTPUT_DIR, f\"{short_name}-finetuned\")\n",
    "    model_finetune.fit(\n",
    "        train_objectives=[(train_dataloader, train_loss)],\n",
    "        epochs=EPOCHS,\n",
    "        warmup_steps=int(len(train_dataloader) * 0.1),\n",
    "        show_progress_bar=True\n",
    "    )\n",
    "    \n",
    "    # PHASE C: EVALUASI FINETUNED MODEL\n",
    "    print(f\"Menguji Model Finetuned...\")\n",
    "    metrics_ft = ir_evaluator(model_finetune)\n",
    "    \n",
    "    results_table.append({\n",
    "        \"Model\": short_name,\n",
    "        \"Type\": \"Fine-Tuned\",\n",
    "        \"Recall@10\": metrics_ft['test_accuracy@10'],\n",
    "        \"MRR@10\": metrics_ft['test_mrr@10']\n",
    "    })\n",
    "    print(f\"   -> Finetuned Recall@10: {metrics_ft['test_accuracy@10']:.4f}\")\n",
    "    print(f\"   -> Finetuned MRR@10   : {metrics_ft['test_mrr@10']:.4f}\")\n",
    "    \n",
    "    # Save model lokal jika mau dipakai nanti\n",
    "    model_finetune.save(save_path)\n",
    "    \n",
    "    del model_finetune\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "# EKSEKUSI\n",
    "for model_name in MODELS_TO_TEST:\n",
    "    run_experiment(model_name)\n",
    "\n",
    "# TAMPILKAN HASIL AKHIR\n",
    "print(\"\\n\\n\" + \"=\"*20 + \" HASIL AKHIR RETRIEVAL \" + \"=\"*20)\n",
    "df_results = pd.DataFrame(results_table)\n",
    "# Format tampilan agar cantik\n",
    "pd.options.display.float_format = '{:,.4f}'.format\n",
    "print(df_results)\n",
    "\n",
    "# Opsional: Simpan ke CSV untuk laporan\n",
    "df_results.to_csv(\"hasil_benchmark_retrieval.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5a48ce88",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading train_pairs.json...\n",
      "Loading test_queries.json...\n",
      "Loading corpus.json...\n",
      "\n",
      "Menyiapkan Evaluator...\n",
      "   -> Evaluator siap: 5093 queries vs 74015 corpus.\n",
      "\n",
      "============================================================\n",
      "EXPERIMENT: all-MiniLM-L6-v2\n",
      "============================================================\n",
      "Evaluasi Baseline (Pre-trained)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 160/160 [00:07<00:00, 20.06it/s]\n",
      "Batches: 100%|██████████| 1563/1563 [01:12<00:00, 21.44it/s]\n",
      "Batches: 100%|██████████| 751/751 [00:35<00:00, 21.28it/s]t]\n",
      "Corpus Chunks: 100%|██████████| 2/2 [01:51<00:00, 55.54s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   [Base] Hit@10: 0.4129 | MRR@10: 0.2572 | NDCG@10: 0.1663 | Recall@10: 0.1797 | Precision@10: 0.0719\n",
      "Training (Fine-tuning)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                     \r"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='4563' max='4563' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [4563/4563 24:19, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>1.073900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.993700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>0.944000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>0.845300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2500</td>\n",
       "      <td>0.835000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>0.833300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3500</td>\n",
       "      <td>0.778600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4000</td>\n",
       "      <td>0.770800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4500</td>\n",
       "      <td>0.762900</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C. Evaluasi Model Finetuned...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 160/160 [00:03<00:00, 52.06it/s]\n",
      "Batches: 100%|██████████| 1563/1563 [00:29<00:00, 53.34it/s]\n",
      "Batches: 100%|██████████| 751/751 [00:14<00:00, 52.88it/s]t]\n",
      "Corpus Chunks: 100%|██████████| 2/2 [00:47<00:00, 23.69s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   [Fine] Hit@10: 0.4127 | MRR@10: 0.2557 | NDCG@10: 0.1653 | Recall@10: 0.1802 | Precision@10: 0.0721\n",
      "\n",
      "\n",
      "============================== HASIL AKHIR LENGKAP ==============================\n",
      "              Model        Type  Hit@1  Hit@5  Hit@10  MRR@10  NDCG@10  \\\n",
      "0  all-MiniLM-L6-v2    Baseline 0.1932 0.3442  0.4129  0.2572   0.1663   \n",
      "1  all-MiniLM-L6-v2  Fine-Tuned 0.1907 0.3405  0.4127  0.2557   0.1653   \n",
      "\n",
      "   Precision@10  Recall@10  \n",
      "0        0.0719     0.1797  \n",
      "1        0.0721     0.1802  \n",
      "\n",
      "Hasil lengkap tersimpan di: hasil_retrieval.csv\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import os\n",
    "import pandas as pd\n",
    "from sentence_transformers import SentenceTransformer, InputExample, losses, models, evaluation\n",
    "from torch.utils.data import DataLoader\n",
    "import torch\n",
    "import gc\n",
    "\n",
    "# --- CONFIG ---\n",
    "DATA_DIR = \"final_dataset_retrieval\"\n",
    "OUTPUT_DIR = \"models-retrieval\"\n",
    "BATCH_SIZE = 64\n",
    "EPOCHS = 3\n",
    "\n",
    "# Daftar Model\n",
    "MODELS_TO_TEST = [\n",
    "    \"sentence-transformers/all-MiniLM-L6-v2\",\n",
    "    # \"sentence-transformers/all-mpnet-base-v2\"\n",
    "]\n",
    "\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "# LOAD DATA\n",
    "def load_json(filename):\n",
    "    print(f\"Loading {filename}...\")\n",
    "    with open(os.path.join(DATA_DIR, filename), 'r', encoding='utf-8') as f:\n",
    "        return json.load(f)\n",
    "\n",
    "train_raw = load_json(\"train_pairs.json\")\n",
    "test_queries_raw = load_json(\"test_queries.json\")\n",
    "corpus_raw = load_json(\"corpus.json\")\n",
    "\n",
    "# SETUP EVALUATOR\n",
    "print(\"\\nMenyiapkan Evaluator...\")\n",
    "\n",
    "# Corpus Dictionary\n",
    "corpus_dict = {}\n",
    "text_to_id_map = {}\n",
    "for item in corpus_raw:\n",
    "    corpus_dict[item['id']] = item['text']\n",
    "    text_to_id_map[item['text']] = item['id']\n",
    "\n",
    "# Queries & Relevance Dictionary\n",
    "queries_dict = {}\n",
    "relevant_docs = {}\n",
    "\n",
    "for i, item in enumerate(test_queries_raw):\n",
    "    qid = f\"q_{i}\"\n",
    "    queries_dict[qid] = item['query']\n",
    "    \n",
    "    ground_truth_ids = set()\n",
    "    for gt_text in item['ground_truths']:\n",
    "        found_id = text_to_id_map.get(gt_text)\n",
    "        if found_id:\n",
    "            ground_truth_ids.add(found_id)\n",
    "            \n",
    "    if ground_truth_ids:\n",
    "        relevant_docs[qid] = ground_truth_ids\n",
    "\n",
    "# Init Evaluator\n",
    "ir_evaluator = evaluation.InformationRetrievalEvaluator(\n",
    "    queries_dict,\n",
    "    corpus_dict,\n",
    "    relevant_docs,\n",
    "    show_progress_bar=True,\n",
    "    name=\"test\",  # Prefix key nanti jadi \"test_cosine_...\"\n",
    "    mrr_at_k=[10],\n",
    "    ndcg_at_k=[10],\n",
    "    accuracy_at_k=[1, 5, 10],      # Hit Rate @ K\n",
    "    precision_recall_at_k=[10]     # Precision & Recall @ K\n",
    ")\n",
    "print(f\"   -> Evaluator siap: {len(queries_dict)} queries vs {len(corpus_dict)} corpus.\")\n",
    "\n",
    "# FUNGSI EKSPERIMEN\n",
    "results_table = []\n",
    "\n",
    "def extract_metrics(metrics_dict, model_name, run_type):\n",
    "    \"\"\"Helper untuk mengambil semua metrik yang diminta user\"\"\"\n",
    "    return {\n",
    "        \"Model\": model_name,\n",
    "        \"Type\": run_type,\n",
    "        \"Hit@1\": metrics_dict.get('test_cosine_accuracy@1', 0),\n",
    "        \"Hit@5\": metrics_dict.get('test_cosine_accuracy@5', 0),\n",
    "        \"Hit@10\": metrics_dict.get('test_cosine_accuracy@10', 0),\n",
    "        \"MRR@10\": metrics_dict.get('test_cosine_mrr@10', 0),\n",
    "        \"NDCG@10\": metrics_dict.get('test_cosine_ndcg@10', 0),\n",
    "        \"Precision@10\": metrics_dict.get('test_cosine_precision@10', 0),\n",
    "        \"Recall@10\": metrics_dict.get('test_cosine_recall@10', 0)\n",
    "    }\n",
    "\n",
    "def run_experiment(model_name):\n",
    "    short_name = model_name.split(\"/\")[-1]\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"EXPERIMENT: {short_name}\")\n",
    "    print(f\"{'='*60}\")\n",
    "\n",
    "    # BASELINE\n",
    "    print(f\"Evaluasi Baseline (Pre-trained)...\")\n",
    "    model_base = SentenceTransformer(model_name)\n",
    "    metrics_base = ir_evaluator(model_base)\n",
    "    \n",
    "    # Ambil & Simpan Metrik\n",
    "    res_base = extract_metrics(metrics_base, short_name, \"Baseline\")\n",
    "    results_table.append(res_base)\n",
    "    \n",
    "    print(f\"   [Base] Hit@10: {res_base['Hit@10']:.4f} | MRR@10: {res_base['MRR@10']:.4f} | NDCG@10: {res_base['NDCG@10']:.4f} | Recall@10: {res_base['Recall@10']:.4f} | Precision@10: {res_base['Precision@10']:.4f}\")\n",
    "    \n",
    "    del model_base\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "    # FINE-TUNING\n",
    "    print(f\"Training (Fine-tuning)...\")\n",
    "    # Init Model\n",
    "    word_emb = models.Transformer(model_name, max_seq_length=256)\n",
    "    pooling = models.Pooling(word_emb.get_word_embedding_dimension())\n",
    "    model_ft = SentenceTransformer(modules=[word_emb, pooling])\n",
    "    \n",
    "    # DataLoader\n",
    "    train_ex = [InputExample(texts=[d['anchor'], d['positive']]) for d in train_raw]\n",
    "    train_dl = DataLoader(train_ex, shuffle=True, batch_size=BATCH_SIZE)\n",
    "    train_loss = losses.MultipleNegativesRankingLoss(model_ft)\n",
    "    \n",
    "    # Fit\n",
    "    save_path = os.path.join(OUTPUT_DIR, f\"{short_name}-finetuned\")\n",
    "    model_ft.fit(\n",
    "        train_objectives=[(train_dl, train_loss)],\n",
    "        epochs=EPOCHS,\n",
    "        warmup_steps=int(len(train_dl) * 0.1),\n",
    "        optimizer_params={'lr': 2e-5},\n",
    "        use_amp=True,\n",
    "        show_progress_bar=True\n",
    "    )\n",
    "    \n",
    "    # EVALUASI FINETUNED\n",
    "    print(f\"C. Evaluasi Model Finetuned...\")\n",
    "    metrics_ft = ir_evaluator(model_ft)\n",
    "    \n",
    "    # Ambil & Simpan Metrik\n",
    "    res_ft = extract_metrics(metrics_ft, short_name, \"Fine-Tuned\")\n",
    "    results_table.append(res_ft)\n",
    "    \n",
    "    print(f\"   [Fine] Hit@10: {res_ft['Hit@10']:.4f} | MRR@10: {res_ft['MRR@10']:.4f} | NDCG@10: {res_ft['NDCG@10']:.4f} | Recall@10: {res_ft['Recall@10']:.4f} | Precision@10: {res_ft['Precision@10']:.4f}\")\n",
    "\n",
    "    # Save\n",
    "    model_ft.save(save_path)\n",
    "    del model_ft\n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "\n",
    "# EKSEKUSI\n",
    "for model in MODELS_TO_TEST:\n",
    "    run_experiment(model)\n",
    "\n",
    "# HASIL AKHIR\n",
    "print(\"\\n\\n\" + \"=\"*30 + \" HASIL AKHIR LENGKAP \" + \"=\"*30)\n",
    "df = pd.DataFrame(results_table)\n",
    "\n",
    "# Urutkan kolom agar rapi\n",
    "cols = [\"Model\", \"Type\", \"Hit@1\", \"Hit@5\", \"Hit@10\", \"MRR@10\", \"NDCG@10\", \"Precision@10\", \"Recall@10\"]\n",
    "df = df[cols]\n",
    "\n",
    "# Print Tabel\n",
    "pd.options.display.float_format = '{:,.4f}'.format\n",
    "print(df)\n",
    "\n",
    "# Simpan ke CSV\n",
    "csv_path = \"hasil_retrieval.csv\"\n",
    "df.to_csv(csv_path, index=False)\n",
    "print(f\"\\nHasil lengkap tersimpan di: {csv_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cf697571",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Iskandar\\Documents\\GitHub\\Tubes-NLP-PaperMatch\\ret_env\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "ds = load_dataset(\"allenai/multixscience_sparse_mean\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0eae4a26",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Dataset...\n",
      ">>> Memproses train Split...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 30369/30369 [00:03<00:00, 8311.42it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> Memproses validation Split...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5066/5066 [00:00<00:00, 13677.96it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> Memproses test Split...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5093/5093 [00:00<00:00, 13528.06it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- PENYIMPANAN DATA ---\n",
      "Tersimpan: final_dataset_retrieval_split\\train_pairs.json (97292 items)\n",
      "Tersimpan: final_dataset_retrieval_split\\corpus_train.json (62989 items)\n",
      "Tersimpan: final_dataset_retrieval_split\\val_queries.json (5066 items)\n",
      "Tersimpan: final_dataset_retrieval_split\\corpus_val.json (13612 items)\n",
      "Tersimpan: final_dataset_retrieval_split\\test_queries.json (5093 items)\n",
      "Tersimpan: final_dataset_retrieval_split\\corpus_test.json (11568 items)\n",
      "\n",
      "SELESAI! Folder 'final_dataset_retrieval_split' berisi corpus yang terpisah.\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import os\n",
    "import hashlib\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Setup\n",
    "output_dir = \"final_dataset_retrieval_split\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "def get_hash(text):\n",
    "    \"\"\"Membuat ID unik berdasarkan konten teks\"\"\"\n",
    "    return hashlib.md5(text.encode('utf-8')).hexdigest()\n",
    "\n",
    "# Load Dataset\n",
    "print(\"Loading Dataset...\")\n",
    "\n",
    "# --- FUNGSI HELPER UNTUK MEMPROSES TIAP SPLIT ---\n",
    "def process_split_complete(dataset_split, split_name):\n",
    "    pairs = []          # Untuk Training (Anchor, Positive)\n",
    "    queries = []        # Untuk Evaluasi (Query, Ground Truth IDs)\n",
    "    split_corpus = set() # Corpus KHUSUS untuk split ini\n",
    "    \n",
    "    seen_pairs = set()\n",
    "    \n",
    "    print(f\">>> Memproses {split_name} Split...\")\n",
    "    \n",
    "    for row in tqdm(dataset_split):\n",
    "        anchor = row['abstract']\n",
    "        refs = row['ref_abstract']['abstract']\n",
    "        \n",
    "        if not anchor or len(anchor) < 50: continue\n",
    "        if not refs: continue\n",
    "        \n",
    "        # Simpan Query (untuk Val/Test)\n",
    "        valid_refs = []\n",
    "        for ref in refs:\n",
    "            if ref and len(ref) > 50 and ref != anchor:\n",
    "                valid_refs.append(ref)\n",
    "                split_corpus.add(ref) # Masukkan ke corpus spesifik split ini\n",
    "        \n",
    "        if valid_refs:\n",
    "            # 1. Simpan format Query (untuk Evaluasi)\n",
    "            queries.append({\n",
    "                \"query\": anchor,\n",
    "                \"ground_truths\": valid_refs\n",
    "            })\n",
    "            \n",
    "            # 2. Simpan format Pair (hanya untuk TRAIN)\n",
    "            if split_name == 'train':\n",
    "                for ref in valid_refs:\n",
    "                    pair_sig = (get_hash(anchor), get_hash(ref))\n",
    "                    if pair_sig not in seen_pairs:\n",
    "                        seen_pairs.add(pair_sig)\n",
    "                        pairs.append({\n",
    "                            \"anchor\": anchor,\n",
    "                            \"positive\": ref\n",
    "                        })\n",
    "\n",
    "    # Convert corpus set to list of dicts\n",
    "    corpus_list = [{\"id\": get_hash(text), \"text\": text} for text in split_corpus]\n",
    "    \n",
    "    return pairs, queries, corpus_list\n",
    "\n",
    "# --- EKSEKUSI PROSES ---\n",
    "\n",
    "# 1. TRAIN\n",
    "train_pairs, _, corpus_train = process_split_complete(ds['train'], 'train')\n",
    "\n",
    "# 2. VALIDATION\n",
    "# (Pairs kosong karena val tidak butuh training pairs, cuma butuh queries & corpus)\n",
    "_, val_queries, corpus_val = process_split_complete(ds['validation'], 'validation')\n",
    "\n",
    "# 3. TEST\n",
    "_, test_queries, corpus_test = process_split_complete(ds['test'], 'test')\n",
    "\n",
    "# --- SIMPAN DATA ---\n",
    "def save_json(data, filename):\n",
    "    path = os.path.join(output_dir, filename)\n",
    "    with open(path, 'w', encoding='utf-8') as f:\n",
    "        json.dump(data, f, ensure_ascii=False, indent=4)\n",
    "    print(f\"Tersimpan: {path} ({len(data)} items)\")\n",
    "\n",
    "print(f\"\\n--- PENYIMPANAN DATA ---\")\n",
    "\n",
    "# Train Data\n",
    "save_json(train_pairs, \"train_pairs.json\")\n",
    "save_json(corpus_train, \"corpus_train.json\")\n",
    "\n",
    "# Validation Data\n",
    "save_json(val_queries, \"val_queries.json\")\n",
    "save_json(corpus_val, \"corpus_val.json\")\n",
    "\n",
    "# Test Data\n",
    "save_json(test_queries, \"test_queries.json\")\n",
    "save_json(corpus_test, \"corpus_test.json\")\n",
    "\n",
    "print(\"\\nSELESAI! Folder 'final_dataset_retrieval_split' berisi corpus yang terpisah.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c39f29ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading train_pairs.json...\n",
      "Loading test_queries.json...\n",
      "Loading corpus_test.json...\n",
      "Loading val_queries.json...\n",
      "Loading corpus_val.json...\n",
      "\n",
      "--- BUILDING EVALUATORS ---\n",
      "Menyiapkan Evaluator: val...\n",
      "Menyiapkan Evaluator: test...\n",
      "\n",
      "============================================================\n",
      "EXPERIMENT: all-MiniLM-L6-v2\n",
      "============================================================\n",
      "Evaluasi Baseline...\n",
      " [Base] Hit@10: 0.0000 | MRR@10: 0.0000\n",
      "Training (Fine-tuning)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                     \r"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='501' max='1521' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 501/1521 02:39 < 05:26, 3.12 it/s, Epoch 0.33/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 164\u001b[39m\n\u001b[32m    162\u001b[39m \u001b[38;5;66;03m# EKSEKUSI\u001b[39;00m\n\u001b[32m    163\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m model \u001b[38;5;129;01min\u001b[39;00m MODELS_TO_TEST:\n\u001b[32m--> \u001b[39m\u001b[32m164\u001b[39m     \u001b[43mrun_experiment\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    166\u001b[39m \u001b[38;5;66;03m# HASIL AKHIR\u001b[39;00m\n\u001b[32m    167\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m + \u001b[33m\"\u001b[39m\u001b[33m=\u001b[39m\u001b[33m\"\u001b[39m*\u001b[32m30\u001b[39m + \u001b[33m\"\u001b[39m\u001b[33m HASIL AKHIR \u001b[39m\u001b[33m\"\u001b[39m + \u001b[33m\"\u001b[39m\u001b[33m=\u001b[39m\u001b[33m\"\u001b[39m*\u001b[32m30\u001b[39m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 129\u001b[39m, in \u001b[36mrun_experiment\u001b[39m\u001b[34m(model_name)\u001b[39m\n\u001b[32m    126\u001b[39m save_path = os.path.join(OUTPUT_DIR, \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mshort_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m-finetuned\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    128\u001b[39m \u001b[38;5;66;03m# --- PERUBAHAN PENTING DI SINI ---\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m129\u001b[39m \u001b[43mmodel_ft\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    130\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtrain_objectives\u001b[49m\u001b[43m=\u001b[49m\u001b[43m[\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_dl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loss\u001b[49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    131\u001b[39m \u001b[43m    \u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mEPOCHS\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    132\u001b[39m \u001b[43m    \u001b[49m\u001b[43mwarmup_steps\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mint\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtrain_dl\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m0.1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    133\u001b[39m \u001b[43m    \u001b[49m\u001b[43moptimizer_params\u001b[49m\u001b[43m=\u001b[49m\u001b[43m{\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mlr\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m2e-5\u001b[39;49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    134\u001b[39m \u001b[43m    \u001b[49m\u001b[43muse_amp\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    135\u001b[39m \u001b[43m    \u001b[49m\u001b[43mshow_progress_bar\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    136\u001b[39m \n\u001b[32m    137\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# Masukkan Evaluator Validasi\u001b[39;49;00m\n\u001b[32m    138\u001b[39m \u001b[43m    \u001b[49m\u001b[43mevaluator\u001b[49m\u001b[43m=\u001b[49m\u001b[43mval_evaluator\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    139\u001b[39m \u001b[43m    \u001b[49m\u001b[43mevaluation_steps\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m500\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m     \u001b[49m\u001b[38;5;66;43;03m# Cek nilai validasi setiap 500 step\u001b[39;49;00m\n\u001b[32m    140\u001b[39m \u001b[43m    \u001b[49m\u001b[43msave_best_model\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m     \u001b[49m\u001b[38;5;66;43;03m# Simpan model HANYA jika skor validasi naik\u001b[39;49;00m\n\u001b[32m    141\u001b[39m \u001b[43m    \u001b[49m\u001b[43moutput_path\u001b[49m\u001b[43m=\u001b[49m\u001b[43msave_path\u001b[49m\u001b[43m     \u001b[49m\u001b[38;5;66;43;03m# Folder tempat nyimpan model terbaik\u001b[39;49;00m\n\u001b[32m    142\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    144\u001b[39m \u001b[38;5;66;03m# C. EVALUASI MODEL TERBAIK (TEST SET)\u001b[39;00m\n\u001b[32m    145\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mEvaluasi Model Finetuned (Best Checkpoint)...\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Iskandar\\Documents\\GitHub\\Tubes-NLP-PaperMatch\\ret_env\\Lib\\site-packages\\sentence_transformers\\fit_mixin.py:408\u001b[39m, in \u001b[36mFitMixin.fit\u001b[39m\u001b[34m(self, train_objectives, evaluator, epochs, steps_per_epoch, scheduler, warmup_steps, optimizer_class, optimizer_params, weight_decay, evaluation_steps, output_path, save_best_model, max_grad_norm, use_amp, callback, show_progress_bar, checkpoint_path, checkpoint_save_steps, checkpoint_save_total_limit, resume_from_checkpoint)\u001b[39m\n\u001b[32m    405\u001b[39m         logger.warning(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mCheckpoint directory does not exist or is not a directory: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcheckpoint_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m    406\u001b[39m         resume_from_checkpoint = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m408\u001b[39m \u001b[43mtrainer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m=\u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Iskandar\\Documents\\GitHub\\Tubes-NLP-PaperMatch\\ret_env\\Lib\\site-packages\\transformers\\trainer.py:2325\u001b[39m, in \u001b[36mTrainer.train\u001b[39m\u001b[34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[39m\n\u001b[32m   2323\u001b[39m         hf_hub_utils.enable_progress_bars()\n\u001b[32m   2324\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m2325\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2326\u001b[39m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[43m=\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2327\u001b[39m \u001b[43m        \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m=\u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2328\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2329\u001b[39m \u001b[43m        \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m=\u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2330\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Iskandar\\Documents\\GitHub\\Tubes-NLP-PaperMatch\\ret_env\\Lib\\site-packages\\transformers\\trainer.py:2756\u001b[39m, in \u001b[36mTrainer._inner_training_loop\u001b[39m\u001b[34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[39m\n\u001b[32m   2754\u001b[39m     \u001b[38;5;28mself\u001b[39m.state.epoch = epoch + (step + \u001b[32m1\u001b[39m) / steps_in_epoch\n\u001b[32m   2755\u001b[39m     \u001b[38;5;28mself\u001b[39m.control = \u001b[38;5;28mself\u001b[39m.callback_handler.on_step_end(args, \u001b[38;5;28mself\u001b[39m.state, \u001b[38;5;28mself\u001b[39m.control)\n\u001b[32m-> \u001b[39m\u001b[32m2756\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_maybe_log_save_evaluate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2757\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtr_loss\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2758\u001b[39m \u001b[43m        \u001b[49m\u001b[43mgrad_norm\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2759\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2760\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2761\u001b[39m \u001b[43m        \u001b[49m\u001b[43mepoch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2762\u001b[39m \u001b[43m        \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2763\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstart_time\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2764\u001b[39m \u001b[43m        \u001b[49m\u001b[43mlearning_rate\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlearning_rate\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2765\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2766\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   2767\u001b[39m     \u001b[38;5;28mself\u001b[39m.control = \u001b[38;5;28mself\u001b[39m.callback_handler.on_substep_end(args, \u001b[38;5;28mself\u001b[39m.state, \u001b[38;5;28mself\u001b[39m.control)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Iskandar\\Documents\\GitHub\\Tubes-NLP-PaperMatch\\ret_env\\Lib\\site-packages\\transformers\\trainer.py:3221\u001b[39m, in \u001b[36mTrainer._maybe_log_save_evaluate\u001b[39m\u001b[34m(self, tr_loss, grad_norm, model, trial, epoch, ignore_keys_for_eval, start_time, learning_rate)\u001b[39m\n\u001b[32m   3219\u001b[39m metrics = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   3220\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.control.should_evaluate:\n\u001b[32m-> \u001b[39m\u001b[32m3221\u001b[39m     metrics = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_evaluate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   3222\u001b[39m     is_new_best_metric = \u001b[38;5;28mself\u001b[39m._determine_best_metric(metrics=metrics, trial=trial)\n\u001b[32m   3224\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.args.save_strategy == SaveStrategy.BEST:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Iskandar\\Documents\\GitHub\\Tubes-NLP-PaperMatch\\ret_env\\Lib\\site-packages\\transformers\\trainer.py:3170\u001b[39m, in \u001b[36mTrainer._evaluate\u001b[39m\u001b[34m(self, trial, ignore_keys_for_eval, skip_scheduler)\u001b[39m\n\u001b[32m   3169\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_evaluate\u001b[39m(\u001b[38;5;28mself\u001b[39m, trial, ignore_keys_for_eval, skip_scheduler=\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[32m-> \u001b[39m\u001b[32m3170\u001b[39m     metrics = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mevaluate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mignore_keys\u001b[49m\u001b[43m=\u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   3171\u001b[39m     \u001b[38;5;28mself\u001b[39m._report_to_hp_search(trial, \u001b[38;5;28mself\u001b[39m.state.global_step, metrics)\n\u001b[32m   3173\u001b[39m     \u001b[38;5;66;03m# Run delayed LR scheduler now that metrics are populated\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Iskandar\\Documents\\GitHub\\Tubes-NLP-PaperMatch\\ret_env\\Lib\\site-packages\\sentence_transformers\\trainer.py:545\u001b[39m, in \u001b[36mSentenceTransformerTrainer.evaluate\u001b[39m\u001b[34m(self, eval_dataset, ignore_keys, metric_key_prefix)\u001b[39m\n\u001b[32m    543\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    544\u001b[39m     eval_dataset = \u001b[38;5;28mself\u001b[39m.eval_dataset\n\u001b[32m--> \u001b[39m\u001b[32m545\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mevaluate\u001b[49m\u001b[43m(\u001b[49m\u001b[43meval_dataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mignore_keys\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmetric_key_prefix\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Iskandar\\Documents\\GitHub\\Tubes-NLP-PaperMatch\\ret_env\\Lib\\site-packages\\transformers\\trainer.py:4489\u001b[39m, in \u001b[36mTrainer.evaluate\u001b[39m\u001b[34m(self, eval_dataset, ignore_keys, metric_key_prefix)\u001b[39m\n\u001b[32m   4486\u001b[39m start_time = time.time()\n\u001b[32m   4488\u001b[39m eval_loop = \u001b[38;5;28mself\u001b[39m.prediction_loop \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.args.use_legacy_prediction_loop \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m.evaluation_loop\n\u001b[32m-> \u001b[39m\u001b[32m4489\u001b[39m output = \u001b[43meval_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   4490\u001b[39m \u001b[43m    \u001b[49m\u001b[43meval_dataloader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4491\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdescription\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mEvaluation\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m   4492\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# No point gathering the predictions if there are no metrics, otherwise we defer to\u001b[39;49;00m\n\u001b[32m   4493\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# self.args.prediction_loss_only\u001b[39;49;00m\n\u001b[32m   4494\u001b[39m \u001b[43m    \u001b[49m\u001b[43mprediction_loss_only\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcompute_metrics\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m   4495\u001b[39m \u001b[43m    \u001b[49m\u001b[43mignore_keys\u001b[49m\u001b[43m=\u001b[49m\u001b[43mignore_keys\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4496\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmetric_key_prefix\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmetric_key_prefix\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4497\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   4499\u001b[39m total_batch_size = \u001b[38;5;28mself\u001b[39m.args.eval_batch_size * \u001b[38;5;28mself\u001b[39m.args.world_size\n\u001b[32m   4500\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmetric_key_prefix\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m_jit_compilation_time\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m output.metrics:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Iskandar\\Documents\\GitHub\\Tubes-NLP-PaperMatch\\ret_env\\Lib\\site-packages\\sentence_transformers\\trainer.py:581\u001b[39m, in \u001b[36mSentenceTransformerTrainer.evaluation_loop\u001b[39m\u001b[34m(self, dataloader, description, prediction_loss_only, ignore_keys, metric_key_prefix)\u001b[39m\n\u001b[32m    579\u001b[39m         output_path = os.path.join(output_path, \u001b[33m\"\u001b[39m\u001b[33meval\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    580\u001b[39m         os.makedirs(output_path, exist_ok=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m--> \u001b[39m\u001b[32m581\u001b[39m     evaluator_metrics = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mevaluator\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    582\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput_path\u001b[49m\u001b[43m=\u001b[49m\u001b[43moutput_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepoch\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mstate\u001b[49m\u001b[43m.\u001b[49m\u001b[43mepoch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msteps\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mstate\u001b[49m\u001b[43m.\u001b[49m\u001b[43mglobal_step\u001b[49m\n\u001b[32m    583\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    584\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(evaluator_metrics, \u001b[38;5;28mdict\u001b[39m):\n\u001b[32m    585\u001b[39m     evaluator_metrics = {\u001b[33m\"\u001b[39m\u001b[33mevaluator\u001b[39m\u001b[33m\"\u001b[39m: evaluator_metrics}\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Iskandar\\Documents\\GitHub\\Tubes-NLP-PaperMatch\\ret_env\\Lib\\site-packages\\sentence_transformers\\evaluation\\InformationRetrievalEvaluator.py:237\u001b[39m, in \u001b[36mInformationRetrievalEvaluator.__call__\u001b[39m\u001b[34m(self, model, output_path, epoch, steps, *args, **kwargs)\u001b[39m\n\u001b[32m    234\u001b[39m     \u001b[38;5;28mself\u001b[39m.score_function_names = [model.similarity_fn_name]\n\u001b[32m    235\u001b[39m     \u001b[38;5;28mself\u001b[39m._append_csv_headers(\u001b[38;5;28mself\u001b[39m.score_function_names)\n\u001b[32m--> \u001b[39m\u001b[32m237\u001b[39m scores = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcompute_metrices\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput_path\u001b[49m\u001b[43m=\u001b[49m\u001b[43moutput_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    239\u001b[39m \u001b[38;5;66;03m# Write results to disk\u001b[39;00m\n\u001b[32m    240\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m output_path \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m.write_csv:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Iskandar\\Documents\\GitHub\\Tubes-NLP-PaperMatch\\ret_env\\Lib\\site-packages\\sentence_transformers\\evaluation\\InformationRetrievalEvaluator.py:332\u001b[39m, in \u001b[36mInformationRetrievalEvaluator.compute_metrices\u001b[39m\u001b[34m(self, model, corpus_model, corpus_embeddings, output_path)\u001b[39m\n\u001b[32m    330\u001b[39m \u001b[38;5;66;03m# Encode chunk of corpus\u001b[39;00m\n\u001b[32m    331\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m corpus_embeddings \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m332\u001b[39m     sub_corpus_embeddings = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43membed_inputs\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    333\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcorpus_model\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    334\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcorpus\u001b[49m\u001b[43m[\u001b[49m\u001b[43mcorpus_start_idx\u001b[49m\u001b[43m:\u001b[49m\u001b[43mcorpus_end_idx\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    335\u001b[39m \u001b[43m        \u001b[49m\u001b[43mencode_fn_name\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mdocument\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    336\u001b[39m \u001b[43m        \u001b[49m\u001b[43mprompt_name\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcorpus_prompt_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    337\u001b[39m \u001b[43m        \u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcorpus_prompt\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    338\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    339\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    340\u001b[39m     sub_corpus_embeddings = corpus_embeddings[corpus_start_idx:corpus_end_idx]\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Iskandar\\Documents\\GitHub\\Tubes-NLP-PaperMatch\\ret_env\\Lib\\site-packages\\sentence_transformers\\evaluation\\InformationRetrievalEvaluator.py:426\u001b[39m, in \u001b[36mInformationRetrievalEvaluator.embed_inputs\u001b[39m\u001b[34m(self, model, sentences, encode_fn_name, prompt_name, prompt, **kwargs)\u001b[39m\n\u001b[32m    424\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m encode_fn_name == \u001b[33m\"\u001b[39m\u001b[33mdocument\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m    425\u001b[39m     encode_fn = model.encode_document\n\u001b[32m--> \u001b[39m\u001b[32m426\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mencode_fn\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    427\u001b[39m \u001b[43m    \u001b[49m\u001b[43msentences\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    428\u001b[39m \u001b[43m    \u001b[49m\u001b[43mprompt_name\u001b[49m\u001b[43m=\u001b[49m\u001b[43mprompt_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    429\u001b[39m \u001b[43m    \u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m=\u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    430\u001b[39m \u001b[43m    \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    431\u001b[39m \u001b[43m    \u001b[49m\u001b[43mshow_progress_bar\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mshow_progress_bar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    432\u001b[39m \u001b[43m    \u001b[49m\u001b[43mconvert_to_tensor\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    433\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtruncate_dim\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtruncate_dim\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    434\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    435\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Iskandar\\Documents\\GitHub\\Tubes-NLP-PaperMatch\\ret_env\\Lib\\site-packages\\sentence_transformers\\SentenceTransformer.py:688\u001b[39m, in \u001b[36mSentenceTransformer.encode_document\u001b[39m\u001b[34m(self, sentences, prompt_name, prompt, batch_size, show_progress_bar, output_value, precision, convert_to_numpy, convert_to_tensor, device, normalize_embeddings, truncate_dim, pool, chunk_size, **kwargs)\u001b[39m\n\u001b[32m    685\u001b[39m             prompt_name = candidate_prompt_name\n\u001b[32m    686\u001b[39m             \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m688\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mencode\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    689\u001b[39m \u001b[43m    \u001b[49m\u001b[43msentences\u001b[49m\u001b[43m=\u001b[49m\u001b[43msentences\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    690\u001b[39m \u001b[43m    \u001b[49m\u001b[43mprompt_name\u001b[49m\u001b[43m=\u001b[49m\u001b[43mprompt_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    691\u001b[39m \u001b[43m    \u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m=\u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    692\u001b[39m \u001b[43m    \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    693\u001b[39m \u001b[43m    \u001b[49m\u001b[43mshow_progress_bar\u001b[49m\u001b[43m=\u001b[49m\u001b[43mshow_progress_bar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    694\u001b[39m \u001b[43m    \u001b[49m\u001b[43moutput_value\u001b[49m\u001b[43m=\u001b[49m\u001b[43moutput_value\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    695\u001b[39m \u001b[43m    \u001b[49m\u001b[43mprecision\u001b[49m\u001b[43m=\u001b[49m\u001b[43mprecision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    696\u001b[39m \u001b[43m    \u001b[49m\u001b[43mconvert_to_numpy\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconvert_to_numpy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    697\u001b[39m \u001b[43m    \u001b[49m\u001b[43mconvert_to_tensor\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconvert_to_tensor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    698\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    699\u001b[39m \u001b[43m    \u001b[49m\u001b[43mnormalize_embeddings\u001b[49m\u001b[43m=\u001b[49m\u001b[43mnormalize_embeddings\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    700\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtruncate_dim\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtruncate_dim\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    701\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpool\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpool\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    702\u001b[39m \u001b[43m    \u001b[49m\u001b[43mchunk_size\u001b[49m\u001b[43m=\u001b[49m\u001b[43mchunk_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    703\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtask\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mdocument\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    704\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    705\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Iskandar\\Documents\\GitHub\\Tubes-NLP-PaperMatch\\ret_env\\Lib\\site-packages\\torch\\utils\\_contextlib.py:116\u001b[39m, in \u001b[36mcontext_decorator.<locals>.decorate_context\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    113\u001b[39m \u001b[38;5;129m@functools\u001b[39m.wraps(func)\n\u001b[32m    114\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mdecorate_context\u001b[39m(*args, **kwargs):\n\u001b[32m    115\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[32m--> \u001b[39m\u001b[32m116\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Iskandar\\Documents\\GitHub\\Tubes-NLP-PaperMatch\\ret_env\\Lib\\site-packages\\sentence_transformers\\SentenceTransformer.py:1090\u001b[39m, in \u001b[36mSentenceTransformer.encode\u001b[39m\u001b[34m(self, sentences, prompt_name, prompt, batch_size, show_progress_bar, output_value, precision, convert_to_numpy, convert_to_tensor, device, normalize_embeddings, truncate_dim, pool, chunk_size, **kwargs)\u001b[39m\n\u001b[32m   1081\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mtoken_type_ids\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m features:\n\u001b[32m   1082\u001b[39m             features[\u001b[33m\"\u001b[39m\u001b[33mtoken_type_ids\u001b[39m\u001b[33m\"\u001b[39m] = torch.cat(\n\u001b[32m   1083\u001b[39m                 (\n\u001b[32m   1084\u001b[39m                     features[\u001b[33m\"\u001b[39m\u001b[33mtoken_type_ids\u001b[39m\u001b[33m\"\u001b[39m],\n\u001b[32m   (...)\u001b[39m\u001b[32m   1087\u001b[39m                 -\u001b[32m1\u001b[39m,\n\u001b[32m   1088\u001b[39m             )\n\u001b[32m-> \u001b[39m\u001b[32m1090\u001b[39m features = \u001b[43mbatch_to_device\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfeatures\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1091\u001b[39m features.update(extra_features)\n\u001b[32m   1093\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m torch.no_grad():\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Iskandar\\Documents\\GitHub\\Tubes-NLP-PaperMatch\\ret_env\\Lib\\site-packages\\sentence_transformers\\util\\tensor.py:184\u001b[39m, in \u001b[36mbatch_to_device\u001b[39m\u001b[34m(batch, target_device)\u001b[39m\n\u001b[32m    182\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m key \u001b[38;5;129;01min\u001b[39;00m batch:\n\u001b[32m    183\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(batch[key], Tensor):\n\u001b[32m--> \u001b[39m\u001b[32m184\u001b[39m         batch[key] = \u001b[43mbatch\u001b[49m\u001b[43m[\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m]\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtarget_device\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    185\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m batch\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "import json\n",
    "import os\n",
    "import pandas as pd\n",
    "from sentence_transformers import SentenceTransformer, InputExample, losses, models, evaluation\n",
    "from torch.utils.data import DataLoader\n",
    "import torch\n",
    "import gc\n",
    "\n",
    "# --- CONFIG ---\n",
    "DATA_DIR = \"final_dataset_retrieval_split\" # Pastikan folder ini benar\n",
    "OUTPUT_DIR = \"models-retrieval\"\n",
    "BATCH_SIZE = 64\n",
    "EPOCHS = 1 # Kita set agak lama, nanti 'save_best_model' yang akan pilih yang terbaik\n",
    "\n",
    "MODELS_TO_TEST = [\n",
    "    \"sentence-transformers/all-MiniLM-L6-v2\",\n",
    "    # \"sentence-transformers/all-mpnet-base-v2\"\n",
    "]\n",
    "\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "# LOAD DATA HELPER\n",
    "def load_json(filename):\n",
    "    print(f\"Loading {filename}...\")\n",
    "    path = os.path.join(DATA_DIR, filename)\n",
    "    with open(path, 'r', encoding='utf-8') as f:\n",
    "        return json.load(f)\n",
    "\n",
    "# --- 1. LOAD DATA ---\n",
    "train_raw = load_json(\"train_pairs.json\")\n",
    "\n",
    "# Data Test (Ujian Akhir)\n",
    "test_queries_raw = load_json(\"test_queries.json\")\n",
    "corpus_test_raw = load_json(\"corpus_test.json\") \n",
    "\n",
    "# Data Validation (Try Out saat Training) - BARU DITAMBAHKAN\n",
    "val_queries_raw = load_json(\"val_queries.json\")\n",
    "corpus_val_raw = load_json(\"corpus_val.json\") \n",
    "\n",
    "# --- 2. SETUP EVALUATOR ---\n",
    "\n",
    "def create_evaluator(queries_raw, corpus_raw, name_prefix):\n",
    "    \"\"\"Fungsi helper untuk bikin evaluator biar tidak copy-paste kode\"\"\"\n",
    "    print(f\"Menyiapkan Evaluator: {name_prefix}...\")\n",
    "    \n",
    "    corpus_dict = {item['id']: item['text'] for item in corpus_raw}\n",
    "    text_to_id_map = {item['text']: item['id'] for item in corpus_raw}\n",
    "    \n",
    "    queries_dict = {}\n",
    "    relevant_docs = {}\n",
    "\n",
    "    for i, item in enumerate(queries_raw):\n",
    "        qid = f\"{name_prefix}_q_{i}\"\n",
    "        queries_dict[qid] = item['query']\n",
    "        \n",
    "        ground_truth_ids = set()\n",
    "        for gt_text in item['ground_truths']:\n",
    "            found_id = text_to_id_map.get(gt_text)\n",
    "            if found_id:\n",
    "                ground_truth_ids.add(found_id)\n",
    "        \n",
    "        if ground_truth_ids:\n",
    "            relevant_docs[qid] = ground_truth_ids\n",
    "            \n",
    "    return evaluation.InformationRetrievalEvaluator(\n",
    "        queries_dict,\n",
    "        corpus_dict,\n",
    "        relevant_docs,\n",
    "        show_progress_bar=False, # False biar log training tidak berantakan\n",
    "        name=name_prefix,\n",
    "        mrr_at_k=[10],\n",
    "        accuracy_at_k=[10],\n",
    "        precision_recall_at_k=[10]\n",
    "    )\n",
    "\n",
    "# Buat 2 Evaluator Berbeda\n",
    "print(\"\\n--- BUILDING EVALUATORS ---\")\n",
    "# 1. Evaluator VALIDATION (Dipakai saat training berjalan)\n",
    "val_evaluator = create_evaluator(val_queries_raw, corpus_val_raw, \"val\")\n",
    "\n",
    "# 2. Evaluator TEST (Dipakai setelah training selesai)\n",
    "test_evaluator = create_evaluator(test_queries_raw, corpus_test_raw, \"test\")\n",
    "\n",
    "\n",
    "# --- 3. FUNGSI EKSPERIMEN ---\n",
    "results_table = []\n",
    "\n",
    "def extract_metrics(metrics_dict, model_name, run_type):\n",
    "    # Perhatikan prefix key-nya menyesuaikan nama evaluator ('test_' atau 'val_')\n",
    "    prefix = \"test_\" if \"test\" in run_type.lower() else \"val_\"\n",
    "    return {\n",
    "        \"Model\": model_name,\n",
    "        \"Type\": run_type,\n",
    "        \"Hit@10\": metrics_dict.get(f'{prefix}cosine_accuracy@10', 0),\n",
    "        \"MRR@10\": metrics_dict.get(f'{prefix}cosine_mrr@10', 0),\n",
    "        \"Recall@10\": metrics_dict.get(f'{prefix}cosine_recall@10', 0)\n",
    "    }\n",
    "\n",
    "def run_experiment(model_name):\n",
    "    short_name = model_name.split(\"/\")[-1]\n",
    "    print(f\"\\n{'='*60}\\nEXPERIMENT: {short_name}\\n{'='*60}\")\n",
    "\n",
    "    # A. BASELINE (Cek performa awal di Test Set)\n",
    "    print(f\"Evaluasi Baseline...\")\n",
    "    model_base = SentenceTransformer(model_name)\n",
    "    metrics_base = test_evaluator(model_base)\n",
    "    \n",
    "    res_base = extract_metrics(metrics_base, short_name, \"Baseline\")\n",
    "    results_table.append(res_base)\n",
    "    print(f\" [Base] Hit@10: {res_base['Hit@10']:.4f} | MRR@10: {res_base['MRR@10']:.4f}\")\n",
    "    \n",
    "    del model_base\n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "\n",
    "    # B. FINE-TUNING\n",
    "    print(f\"Training (Fine-tuning)...\")\n",
    "    word_emb = models.Transformer(model_name, max_seq_length=256)\n",
    "    pooling = models.Pooling(word_emb.get_word_embedding_dimension())\n",
    "    model_ft = SentenceTransformer(modules=[word_emb, pooling])\n",
    "    \n",
    "    train_ex = [InputExample(texts=[d['anchor'], d['positive']]) for d in train_raw]\n",
    "    train_dl = DataLoader(train_ex, shuffle=True, batch_size=BATCH_SIZE)\n",
    "    train_loss = losses.MultipleNegativesRankingLoss(model_ft)\n",
    "    \n",
    "    save_path = os.path.join(OUTPUT_DIR, f\"{short_name}-finetuned\")\n",
    "    \n",
    "    # --- PERUBAHAN PENTING DI SINI ---\n",
    "    model_ft.fit(\n",
    "        train_objectives=[(train_dl, train_loss)],\n",
    "        epochs=EPOCHS,\n",
    "        warmup_steps=int(len(train_dl) * 0.1),\n",
    "        optimizer_params={'lr': 2e-5},\n",
    "        use_amp=True,\n",
    "        show_progress_bar=True,\n",
    "        \n",
    "        # Masukkan Evaluator Validasi\n",
    "        evaluator=val_evaluator,\n",
    "        evaluation_steps=500,     # Cek nilai validasi setiap 500 step\n",
    "        save_best_model=True,     # Simpan model HANYA jika skor validasi naik\n",
    "        output_path=save_path     # Folder tempat nyimpan model terbaik\n",
    "    )\n",
    "    \n",
    "    # C. EVALUASI MODEL TERBAIK (TEST SET)\n",
    "    print(f\"Evaluasi Model Finetuned (Best Checkpoint)...\")\n",
    "    \n",
    "    # Kita load ulang model dari folder save_path \n",
    "    # untuk memastikan kita pakai model TERBAIK (bukan model epoch terakhir)\n",
    "    best_model = SentenceTransformer(save_path)\n",
    "    \n",
    "    metrics_ft = test_evaluator(best_model)\n",
    "    \n",
    "    res_ft = extract_metrics(metrics_ft, short_name, \"Fine-Tuned\")\n",
    "    results_table.append(res_ft)\n",
    "    print(f\" [Fine] Hit@10: {res_ft['Hit@10']:.4f} | MRR@10: {res_ft['MRR@10']:.4f}\")\n",
    "\n",
    "    del model_ft\n",
    "    del best_model\n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "\n",
    "# EKSEKUSI\n",
    "for model in MODELS_TO_TEST:\n",
    "    run_experiment(model)\n",
    "\n",
    "# HASIL AKHIR\n",
    "print(\"\\n\\n\" + \"=\"*30 + \" HASIL AKHIR \" + \"=\"*30)\n",
    "df = pd.DataFrame(results_table)\n",
    "cols = [\"Model\", \"Type\", \"Hit@10\", \"MRR@10\", \"Recall@10\"]\n",
    "print(df[cols])\n",
    "\n",
    "csv_path = \"hasil_retrieval_final_validated.csv\"\n",
    "df.to_csv(csv_path, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d5228d9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Iskandar\\Documents\\GitHub\\Tubes-NLP-PaperMatch\\ret_env\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading train_pairs.json...\n",
      "Loading val_queries.json...\n",
      "Loading corpus_val.json...\n",
      "Loading test_queries.json...\n",
      "Loading corpus_test.json...\n",
      "Menyiapkan Evaluator: VAL...\n",
      "Menyiapkan Evaluator: TEST...\n",
      "\n",
      "============================================================\n",
      "EXPERIMENT: all-MiniLM-L6-v2\n",
      "============================================================\n",
      "A. Evaluasi Baseline (Pre-trained)...\n",
      "   [Base] Hit@10: 0.5307 | MRR@10: 0.3230 | Recall@10: 0.2715\n",
      "B. Training (Fine-tuning)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                     \r"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='4563' max='4563' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [4563/4563 25:54, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Val Cosine Accuracy@1</th>\n",
       "      <th>Val Cosine Accuracy@5</th>\n",
       "      <th>Val Cosine Accuracy@10</th>\n",
       "      <th>Val Cosine Precision@10</th>\n",
       "      <th>Val Cosine Recall@10</th>\n",
       "      <th>Val Cosine Ndcg@10</th>\n",
       "      <th>Val Cosine Mrr@10</th>\n",
       "      <th>Val Cosine Map@100</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>1.065700</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.224832</td>\n",
       "      <td>0.508291</td>\n",
       "      <td>0.632254</td>\n",
       "      <td>0.119049</td>\n",
       "      <td>0.404492</td>\n",
       "      <td>0.303725</td>\n",
       "      <td>0.343669</td>\n",
       "      <td>0.252740</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.993900</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.228780</td>\n",
       "      <td>0.510067</td>\n",
       "      <td>0.631070</td>\n",
       "      <td>0.120114</td>\n",
       "      <td>0.406760</td>\n",
       "      <td>0.306328</td>\n",
       "      <td>0.346431</td>\n",
       "      <td>0.255234</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>0.940900</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.229570</td>\n",
       "      <td>0.501974</td>\n",
       "      <td>0.635215</td>\n",
       "      <td>0.120114</td>\n",
       "      <td>0.407959</td>\n",
       "      <td>0.306647</td>\n",
       "      <td>0.346601</td>\n",
       "      <td>0.255486</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1521</td>\n",
       "      <td>0.940900</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.226411</td>\n",
       "      <td>0.506317</td>\n",
       "      <td>0.632057</td>\n",
       "      <td>0.120351</td>\n",
       "      <td>0.406992</td>\n",
       "      <td>0.305200</td>\n",
       "      <td>0.344171</td>\n",
       "      <td>0.254223</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>0.845400</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.227201</td>\n",
       "      <td>0.505922</td>\n",
       "      <td>0.637386</td>\n",
       "      <td>0.120786</td>\n",
       "      <td>0.411006</td>\n",
       "      <td>0.306812</td>\n",
       "      <td>0.345004</td>\n",
       "      <td>0.254911</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2500</td>\n",
       "      <td>0.841100</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.229175</td>\n",
       "      <td>0.509672</td>\n",
       "      <td>0.635807</td>\n",
       "      <td>0.120707</td>\n",
       "      <td>0.408708</td>\n",
       "      <td>0.307771</td>\n",
       "      <td>0.347395</td>\n",
       "      <td>0.257104</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>0.826300</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.226806</td>\n",
       "      <td>0.508488</td>\n",
       "      <td>0.634426</td>\n",
       "      <td>0.120845</td>\n",
       "      <td>0.408421</td>\n",
       "      <td>0.306626</td>\n",
       "      <td>0.345290</td>\n",
       "      <td>0.256028</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3042</td>\n",
       "      <td>0.826300</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.227201</td>\n",
       "      <td>0.511054</td>\n",
       "      <td>0.634426</td>\n",
       "      <td>0.120944</td>\n",
       "      <td>0.407723</td>\n",
       "      <td>0.306762</td>\n",
       "      <td>0.345980</td>\n",
       "      <td>0.256153</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3500</td>\n",
       "      <td>0.781700</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.227793</td>\n",
       "      <td>0.506514</td>\n",
       "      <td>0.631859</td>\n",
       "      <td>0.120292</td>\n",
       "      <td>0.406238</td>\n",
       "      <td>0.305928</td>\n",
       "      <td>0.344895</td>\n",
       "      <td>0.256093</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4000</td>\n",
       "      <td>0.763900</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.225424</td>\n",
       "      <td>0.504935</td>\n",
       "      <td>0.637189</td>\n",
       "      <td>0.121674</td>\n",
       "      <td>0.411293</td>\n",
       "      <td>0.307302</td>\n",
       "      <td>0.344430</td>\n",
       "      <td>0.255817</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4500</td>\n",
       "      <td>0.758000</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.228583</td>\n",
       "      <td>0.504737</td>\n",
       "      <td>0.639755</td>\n",
       "      <td>0.121792</td>\n",
       "      <td>0.412599</td>\n",
       "      <td>0.308349</td>\n",
       "      <td>0.345929</td>\n",
       "      <td>0.256722</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4563</td>\n",
       "      <td>0.758000</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.228780</td>\n",
       "      <td>0.504145</td>\n",
       "      <td>0.639755</td>\n",
       "      <td>0.121753</td>\n",
       "      <td>0.412484</td>\n",
       "      <td>0.308339</td>\n",
       "      <td>0.346084</td>\n",
       "      <td>0.256745</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C. Evaluasi Model Finetuned (Best Checkpoint)...\n",
      "   [Fine] Hit@10: 0.5417 | MRR@10: 0.3299 | Recall@10: 0.2860\n",
      "\n",
      "\n",
      "============================== HASIL AKHIR SKRIPSI ==============================\n",
      "              Model        Type  Hit@1  Hit@5  Hit@10  MRR@10  NDCG@10  \\\n",
      "0  all-MiniLM-L6-v2    Baseline 0.2348 0.4420  0.5307  0.3230   0.2377   \n",
      "1  all-MiniLM-L6-v2  Fine-Tuned 0.2382 0.4516  0.5417  0.3299   0.2474   \n",
      "\n",
      "   Precision@10  Recall@10  \n",
      "0        0.1069     0.2715  \n",
      "1        0.1126     0.2860  \n",
      "\n",
      "✅ Hasil lengkap tersimpan di: hasil_retrieval_final_lengkap.csv\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import os\n",
    "import pandas as pd\n",
    "from sentence_transformers import SentenceTransformer, InputExample, losses, models, evaluation\n",
    "from torch.utils.data import DataLoader\n",
    "import torch\n",
    "import gc\n",
    "\n",
    "# --- CONFIG ---\n",
    "# Folder data hasil split sebelumnya\n",
    "DATA_DIR = \"final_dataset_retrieval_split\" \n",
    "OUTPUT_DIR = \"models-retrieval\"\n",
    "BATCH_SIZE = 64  # Batch besar = Bagus untuk Contrastive Learning\n",
    "EPOCHS = 3       # Model akan stop otomatis jika validasi tidak membaik\n",
    "\n",
    "# Daftar Model\n",
    "MODELS_TO_TEST = [\n",
    "    \"sentence-transformers/all-MiniLM-L6-v2\",\n",
    "    # \"sentence-transformers/all-mpnet-base-v2\" # Uncomment jika ingin tes MPNet\n",
    "]\n",
    "\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "# --- 1. LOAD DATA FUNCTION ---\n",
    "def load_json(filename):\n",
    "    print(f\"Loading {filename}...\")\n",
    "    path = os.path.join(DATA_DIR, filename)\n",
    "    with open(path, 'r', encoding='utf-8') as f:\n",
    "        return json.load(f)\n",
    "\n",
    "# Load Data Training\n",
    "train_raw = load_json(\"train_pairs.json\")\n",
    "\n",
    "# Load Data Validasi (Untuk pemantauan saat training)\n",
    "val_queries_raw = load_json(\"val_queries.json\")\n",
    "corpus_val_raw = load_json(\"corpus_val.json\")\n",
    "\n",
    "# Load Data Test (Untuk evaluasi akhir)\n",
    "test_queries_raw = load_json(\"test_queries.json\")\n",
    "corpus_test_raw = load_json(\"corpus_test.json\")\n",
    "\n",
    "# --- 2. HELPER UNTUK MEMBUAT EVALUATOR ---\n",
    "def create_evaluator(queries_raw, corpus_raw, name_prefix):\n",
    "    \"\"\"\n",
    "    Membuat object InformationRetrievalEvaluator.\n",
    "    name_prefix: 'val' atau 'test' (mempengaruhi nama key di hasil)\n",
    "    \"\"\"\n",
    "    print(f\"Menyiapkan Evaluator: {name_prefix.upper()}...\")\n",
    "    \n",
    "    # 1. Corpus Dictionary\n",
    "    corpus_dict = {item['id']: item['text'] for item in corpus_raw}\n",
    "    text_to_id_map = {item['text']: item['id'] for item in corpus_raw}\n",
    "    \n",
    "    # 2. Queries & Ground Truths\n",
    "    queries_dict = {}\n",
    "    relevant_docs = {}\n",
    "\n",
    "    for i, item in enumerate(queries_raw):\n",
    "        qid = f\"{name_prefix}_q_{i}\" # ID unik per split\n",
    "        queries_dict[qid] = item['query']\n",
    "        \n",
    "        ground_truth_ids = set()\n",
    "        for gt_text in item['ground_truths']:\n",
    "            found_id = text_to_id_map.get(gt_text)\n",
    "            if found_id:\n",
    "                ground_truth_ids.add(found_id)\n",
    "        \n",
    "        if ground_truth_ids:\n",
    "            relevant_docs[qid] = ground_truth_ids\n",
    "            \n",
    "    # 3. Return Evaluator Object\n",
    "    return evaluation.InformationRetrievalEvaluator(\n",
    "        queries_dict,\n",
    "        corpus_dict,\n",
    "        relevant_docs,\n",
    "        show_progress_bar=False, # Matikan progress bar evaluasi agar log training bersih\n",
    "        name=name_prefix,        # Prefix key: test_cosine_... atau val_cosine_...\n",
    "        mrr_at_k=[10],\n",
    "        ndcg_at_k=[10],\n",
    "        accuracy_at_k=[1, 5, 10],      # Hit Rate\n",
    "        precision_recall_at_k=[10]     # Precision & Recall\n",
    "    )\n",
    "\n",
    "# Buat dua evaluator\n",
    "val_evaluator = create_evaluator(val_queries_raw, corpus_val_raw, \"val\")\n",
    "test_evaluator = create_evaluator(test_queries_raw, corpus_test_raw, \"test\")\n",
    "\n",
    "# --- 3. FUNGSI EKSTRAKSI METRIK LENGKAP ---\n",
    "def extract_metrics(metrics_dict, model_name, run_type):\n",
    "    \"\"\"Mengambil semua metrik spesifik yang diminta user\"\"\"\n",
    "    \n",
    "    # Tentukan prefix key berdasarkan tipe run (test atau val)\n",
    "    # Default library: '{name}_cosine_{metric}@{k}'\n",
    "    prefix = \"test_\" if \"Baseline\" in run_type or \"Fine-Tuned\" in run_type else \"val_\"\n",
    "    \n",
    "    return {\n",
    "        \"Model\": model_name,\n",
    "        \"Type\": run_type,\n",
    "        \"Hit@1\": metrics_dict.get(f'{prefix}cosine_accuracy@1', 0),\n",
    "        \"Hit@5\": metrics_dict.get(f'{prefix}cosine_accuracy@5', 0),\n",
    "        \"Hit@10\": metrics_dict.get(f'{prefix}cosine_accuracy@10', 0),\n",
    "        \"MRR@10\": metrics_dict.get(f'{prefix}cosine_mrr@10', 0),\n",
    "        \"NDCG@10\": metrics_dict.get(f'{prefix}cosine_ndcg@10', 0),\n",
    "        \"Precision@10\": metrics_dict.get(f'{prefix}cosine_precision@10', 0),\n",
    "        \"Recall@10\": metrics_dict.get(f'{prefix}cosine_recall@10', 0)\n",
    "    }\n",
    "\n",
    "# --- 4. CORE EXPERIMENT LOOP ---\n",
    "results_table = []\n",
    "\n",
    "def run_experiment(model_name):\n",
    "    short_name = model_name.split(\"/\")[-1]\n",
    "    print(f\"\\n{'='*60}\\nEXPERIMENT: {short_name}\\n{'='*60}\")\n",
    "\n",
    "    # --- A. BASELINE EVALUATION (Zero-Shot) ---\n",
    "    print(f\"A. Evaluasi Baseline (Pre-trained)...\")\n",
    "    model_base = SentenceTransformer(model_name)\n",
    "    metrics_base = test_evaluator(model_base)\n",
    "    \n",
    "    res_base = extract_metrics(metrics_base, short_name, \"Baseline\")\n",
    "    results_table.append(res_base)\n",
    "    \n",
    "    print(f\"   [Base] Hit@10: {res_base['Hit@10']:.4f} | MRR@10: {res_base['MRR@10']:.4f} | Recall@10: {res_base['Recall@10']:.4f}\")\n",
    "    \n",
    "    del model_base\n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "\n",
    "    # --- B. FINE-TUNING ---\n",
    "    print(f\"B. Training (Fine-tuning)...\")\n",
    "    \n",
    "    # Setup Model\n",
    "    word_emb = models.Transformer(model_name, max_seq_length=256)\n",
    "    pooling = models.Pooling(word_emb.get_word_embedding_dimension())\n",
    "    model_ft = SentenceTransformer(modules=[word_emb, pooling])\n",
    "    \n",
    "    # DataLoader\n",
    "    train_ex = [InputExample(texts=[d['anchor'], d['positive']]) for d in train_raw]\n",
    "    train_dl = DataLoader(train_ex, shuffle=True, batch_size=BATCH_SIZE)\n",
    "    \n",
    "    # Loss Function\n",
    "    train_loss = losses.MultipleNegativesRankingLoss(model_ft)\n",
    "    \n",
    "    save_path = os.path.join(OUTPUT_DIR, f\"{short_name}-finetuned\")\n",
    "    \n",
    "    # Training dengan Validasi\n",
    "    model_ft.fit(\n",
    "        train_objectives=[(train_dl, train_loss)],\n",
    "        epochs=EPOCHS,\n",
    "        warmup_steps=int(len(train_dl) * 0.1),\n",
    "        optimizer_params={'lr': 2e-5},\n",
    "        use_amp=True,\n",
    "        show_progress_bar=True,\n",
    "        \n",
    "        # Validasi Integrasi\n",
    "        evaluator=val_evaluator,\n",
    "        evaluation_steps=500,     # Cek validasi tiap 500 steps\n",
    "        save_best_model=True,     # Simpan checkpoint terbaik berdasarkan validasi\n",
    "        output_path=save_path\n",
    "    )\n",
    "    \n",
    "    # --- C. FINAL EVALUATION (Best Model) ---\n",
    "    print(f\"C. Evaluasi Model Finetuned (Best Checkpoint)...\")\n",
    "    \n",
    "    # Load model terbaik dari hasil save_best_model\n",
    "    best_model = SentenceTransformer(save_path)\n",
    "    metrics_ft = test_evaluator(best_model)\n",
    "    \n",
    "    res_ft = extract_metrics(metrics_ft, short_name, \"Fine-Tuned\")\n",
    "    results_table.append(res_ft)\n",
    "    \n",
    "    print(f\"   [Fine] Hit@10: {res_ft['Hit@10']:.4f} | MRR@10: {res_ft['MRR@10']:.4f} | Recall@10: {res_ft['Recall@10']:.4f}\")\n",
    "\n",
    "    del model_ft\n",
    "    del best_model\n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "\n",
    "# --- EKSEKUSI ---\n",
    "for model in MODELS_TO_TEST:\n",
    "    run_experiment(model)\n",
    "\n",
    "# --- SAVE RESULT ---\n",
    "print(\"\\n\\n\" + \"=\"*30 + \" HASIL AKHIR SKRIPSI \" + \"=\"*30)\n",
    "df = pd.DataFrame(results_table)\n",
    "\n",
    "# Urutkan kolom sesuai permintaan\n",
    "cols = [\"Model\", \"Type\", \"Hit@1\", \"Hit@5\", \"Hit@10\", \"MRR@10\", \"NDCG@10\", \"Precision@10\", \"Recall@10\"]\n",
    "df = df[cols]\n",
    "\n",
    "# Print & Save\n",
    "pd.options.display.float_format = '{:,.4f}'.format\n",
    "print(df)\n",
    "\n",
    "csv_path = \"hasil_retrieval_final_lengkap.csv\"\n",
    "df.to_csv(csv_path, index=False)\n",
    "print(f\"\\n✅ Hasil lengkap tersimpan di: {csv_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "af01f0a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Iskandar\\Documents\\GitHub\\Tubes-NLP-PaperMatch\\ret_env\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading train_pairs.json...\n",
      "Loading test_queries.json...\n",
      "Loading corpus_test.json...\n",
      "Loading val_queries.json...\n",
      "Loading corpus_val.json...\n",
      "Menyiapkan Evaluator: VAL...\n",
      "Menyiapkan Evaluator: TEST...\n",
      "\n",
      "============================================================\n",
      "EXPERIMENT: all-mpnet-base-v2\n",
      "============================================================\n",
      "Evaluasi Baseline (Pre-trained)...\n",
      " [Base] Hit@10: 0.5447 | MRR@10: 0.3319\n",
      "Training (Fine-tuning)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                     \r"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='24324' max='24324' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [24324/24324 3:47:20, Epoch 2/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Val Cosine Accuracy@1</th>\n",
       "      <th>Val Cosine Accuracy@5</th>\n",
       "      <th>Val Cosine Accuracy@10</th>\n",
       "      <th>Val Cosine Precision@10</th>\n",
       "      <th>Val Cosine Recall@10</th>\n",
       "      <th>Val Cosine Ndcg@10</th>\n",
       "      <th>Val Cosine Mrr@10</th>\n",
       "      <th>Val Cosine Map@100</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.277100</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.227991</td>\n",
       "      <td>0.528030</td>\n",
       "      <td>0.644887</td>\n",
       "      <td>0.123826</td>\n",
       "      <td>0.422869</td>\n",
       "      <td>0.317109</td>\n",
       "      <td>0.351705</td>\n",
       "      <td>0.264804</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.282900</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.225622</td>\n",
       "      <td>0.516186</td>\n",
       "      <td>0.640742</td>\n",
       "      <td>0.122365</td>\n",
       "      <td>0.418806</td>\n",
       "      <td>0.313083</td>\n",
       "      <td>0.347654</td>\n",
       "      <td>0.259996</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>0.308800</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.221871</td>\n",
       "      <td>0.501382</td>\n",
       "      <td>0.618634</td>\n",
       "      <td>0.117687</td>\n",
       "      <td>0.400327</td>\n",
       "      <td>0.300920</td>\n",
       "      <td>0.337989</td>\n",
       "      <td>0.250532</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>0.268400</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.218121</td>\n",
       "      <td>0.498816</td>\n",
       "      <td>0.620608</td>\n",
       "      <td>0.117687</td>\n",
       "      <td>0.399945</td>\n",
       "      <td>0.297802</td>\n",
       "      <td>0.335014</td>\n",
       "      <td>0.246123</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2500</td>\n",
       "      <td>0.281700</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.221674</td>\n",
       "      <td>0.495065</td>\n",
       "      <td>0.618437</td>\n",
       "      <td>0.117884</td>\n",
       "      <td>0.399800</td>\n",
       "      <td>0.298461</td>\n",
       "      <td>0.335567</td>\n",
       "      <td>0.247712</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>0.265500</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.225424</td>\n",
       "      <td>0.495065</td>\n",
       "      <td>0.623766</td>\n",
       "      <td>0.117410</td>\n",
       "      <td>0.400940</td>\n",
       "      <td>0.300601</td>\n",
       "      <td>0.341318</td>\n",
       "      <td>0.249271</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3500</td>\n",
       "      <td>0.295100</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.219108</td>\n",
       "      <td>0.496250</td>\n",
       "      <td>0.617055</td>\n",
       "      <td>0.116818</td>\n",
       "      <td>0.395478</td>\n",
       "      <td>0.297041</td>\n",
       "      <td>0.335518</td>\n",
       "      <td>0.247651</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4000</td>\n",
       "      <td>0.241300</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.217331</td>\n",
       "      <td>0.495855</td>\n",
       "      <td>0.618634</td>\n",
       "      <td>0.116640</td>\n",
       "      <td>0.397736</td>\n",
       "      <td>0.296727</td>\n",
       "      <td>0.333537</td>\n",
       "      <td>0.246923</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4500</td>\n",
       "      <td>0.270300</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.222858</td>\n",
       "      <td>0.496842</td>\n",
       "      <td>0.617450</td>\n",
       "      <td>0.116088</td>\n",
       "      <td>0.396141</td>\n",
       "      <td>0.296368</td>\n",
       "      <td>0.337157</td>\n",
       "      <td>0.245625</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5000</td>\n",
       "      <td>0.241000</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.218121</td>\n",
       "      <td>0.490722</td>\n",
       "      <td>0.611923</td>\n",
       "      <td>0.114706</td>\n",
       "      <td>0.392136</td>\n",
       "      <td>0.292891</td>\n",
       "      <td>0.331954</td>\n",
       "      <td>0.242937</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5500</td>\n",
       "      <td>0.256200</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.220095</td>\n",
       "      <td>0.493881</td>\n",
       "      <td>0.617647</td>\n",
       "      <td>0.116542</td>\n",
       "      <td>0.396993</td>\n",
       "      <td>0.295663</td>\n",
       "      <td>0.335217</td>\n",
       "      <td>0.245244</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6000</td>\n",
       "      <td>0.265400</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.221279</td>\n",
       "      <td>0.497631</td>\n",
       "      <td>0.618042</td>\n",
       "      <td>0.116798</td>\n",
       "      <td>0.395884</td>\n",
       "      <td>0.296800</td>\n",
       "      <td>0.336108</td>\n",
       "      <td>0.246441</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6500</td>\n",
       "      <td>0.257700</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.222069</td>\n",
       "      <td>0.487367</td>\n",
       "      <td>0.610343</td>\n",
       "      <td>0.116463</td>\n",
       "      <td>0.394058</td>\n",
       "      <td>0.295708</td>\n",
       "      <td>0.334831</td>\n",
       "      <td>0.246371</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7000</td>\n",
       "      <td>0.240800</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.217331</td>\n",
       "      <td>0.491117</td>\n",
       "      <td>0.612120</td>\n",
       "      <td>0.115495</td>\n",
       "      <td>0.393782</td>\n",
       "      <td>0.293233</td>\n",
       "      <td>0.332789</td>\n",
       "      <td>0.243046</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7500</td>\n",
       "      <td>0.255900</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.216542</td>\n",
       "      <td>0.494078</td>\n",
       "      <td>0.612712</td>\n",
       "      <td>0.116443</td>\n",
       "      <td>0.393868</td>\n",
       "      <td>0.293996</td>\n",
       "      <td>0.332382</td>\n",
       "      <td>0.244157</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8000</td>\n",
       "      <td>0.224600</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.214370</td>\n",
       "      <td>0.494276</td>\n",
       "      <td>0.610936</td>\n",
       "      <td>0.117331</td>\n",
       "      <td>0.394281</td>\n",
       "      <td>0.295344</td>\n",
       "      <td>0.331533</td>\n",
       "      <td>0.246666</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8500</td>\n",
       "      <td>0.227900</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.215160</td>\n",
       "      <td>0.496644</td>\n",
       "      <td>0.618042</td>\n",
       "      <td>0.116719</td>\n",
       "      <td>0.395891</td>\n",
       "      <td>0.294538</td>\n",
       "      <td>0.331776</td>\n",
       "      <td>0.244143</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9000</td>\n",
       "      <td>0.221900</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.209238</td>\n",
       "      <td>0.489143</td>\n",
       "      <td>0.615278</td>\n",
       "      <td>0.113995</td>\n",
       "      <td>0.390258</td>\n",
       "      <td>0.288523</td>\n",
       "      <td>0.326429</td>\n",
       "      <td>0.238060</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9500</td>\n",
       "      <td>0.237400</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.221477</td>\n",
       "      <td>0.494670</td>\n",
       "      <td>0.622779</td>\n",
       "      <td>0.117450</td>\n",
       "      <td>0.399441</td>\n",
       "      <td>0.297647</td>\n",
       "      <td>0.336388</td>\n",
       "      <td>0.246962</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10000</td>\n",
       "      <td>0.225200</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.221871</td>\n",
       "      <td>0.495263</td>\n",
       "      <td>0.619226</td>\n",
       "      <td>0.118160</td>\n",
       "      <td>0.399897</td>\n",
       "      <td>0.298934</td>\n",
       "      <td>0.337631</td>\n",
       "      <td>0.248849</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10500</td>\n",
       "      <td>0.234800</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.214962</td>\n",
       "      <td>0.496842</td>\n",
       "      <td>0.624753</td>\n",
       "      <td>0.117983</td>\n",
       "      <td>0.400401</td>\n",
       "      <td>0.296635</td>\n",
       "      <td>0.333462</td>\n",
       "      <td>0.245801</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11000</td>\n",
       "      <td>0.234900</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.220095</td>\n",
       "      <td>0.504145</td>\n",
       "      <td>0.623964</td>\n",
       "      <td>0.118358</td>\n",
       "      <td>0.400871</td>\n",
       "      <td>0.299609</td>\n",
       "      <td>0.338396</td>\n",
       "      <td>0.249451</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11500</td>\n",
       "      <td>0.200900</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.220884</td>\n",
       "      <td>0.491117</td>\n",
       "      <td>0.621595</td>\n",
       "      <td>0.117173</td>\n",
       "      <td>0.399985</td>\n",
       "      <td>0.297044</td>\n",
       "      <td>0.335012</td>\n",
       "      <td>0.247054</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12000</td>\n",
       "      <td>0.218100</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.224635</td>\n",
       "      <td>0.494868</td>\n",
       "      <td>0.624556</td>\n",
       "      <td>0.118377</td>\n",
       "      <td>0.401705</td>\n",
       "      <td>0.300128</td>\n",
       "      <td>0.340186</td>\n",
       "      <td>0.249762</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12162</td>\n",
       "      <td>0.218100</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.224635</td>\n",
       "      <td>0.498816</td>\n",
       "      <td>0.627912</td>\n",
       "      <td>0.119226</td>\n",
       "      <td>0.404564</td>\n",
       "      <td>0.301995</td>\n",
       "      <td>0.341207</td>\n",
       "      <td>0.251009</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12500</td>\n",
       "      <td>0.198900</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.225819</td>\n",
       "      <td>0.496052</td>\n",
       "      <td>0.629688</td>\n",
       "      <td>0.119897</td>\n",
       "      <td>0.406713</td>\n",
       "      <td>0.302753</td>\n",
       "      <td>0.341108</td>\n",
       "      <td>0.251708</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13000</td>\n",
       "      <td>0.184500</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.222069</td>\n",
       "      <td>0.493881</td>\n",
       "      <td>0.622779</td>\n",
       "      <td>0.119266</td>\n",
       "      <td>0.402445</td>\n",
       "      <td>0.300933</td>\n",
       "      <td>0.337660</td>\n",
       "      <td>0.251020</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13500</td>\n",
       "      <td>0.160100</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.223648</td>\n",
       "      <td>0.497829</td>\n",
       "      <td>0.627122</td>\n",
       "      <td>0.119897</td>\n",
       "      <td>0.403847</td>\n",
       "      <td>0.301780</td>\n",
       "      <td>0.340695</td>\n",
       "      <td>0.251314</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14000</td>\n",
       "      <td>0.175400</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.223253</td>\n",
       "      <td>0.502764</td>\n",
       "      <td>0.629491</td>\n",
       "      <td>0.120253</td>\n",
       "      <td>0.405642</td>\n",
       "      <td>0.303292</td>\n",
       "      <td>0.340795</td>\n",
       "      <td>0.253595</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14500</td>\n",
       "      <td>0.167400</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.223648</td>\n",
       "      <td>0.496644</td>\n",
       "      <td>0.629688</td>\n",
       "      <td>0.120055</td>\n",
       "      <td>0.405137</td>\n",
       "      <td>0.302248</td>\n",
       "      <td>0.340309</td>\n",
       "      <td>0.251893</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15000</td>\n",
       "      <td>0.162100</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.224832</td>\n",
       "      <td>0.495263</td>\n",
       "      <td>0.623569</td>\n",
       "      <td>0.119582</td>\n",
       "      <td>0.402206</td>\n",
       "      <td>0.301249</td>\n",
       "      <td>0.339313</td>\n",
       "      <td>0.251757</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15500</td>\n",
       "      <td>0.175700</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.221674</td>\n",
       "      <td>0.496250</td>\n",
       "      <td>0.627122</td>\n",
       "      <td>0.119147</td>\n",
       "      <td>0.402623</td>\n",
       "      <td>0.301487</td>\n",
       "      <td>0.339462</td>\n",
       "      <td>0.252250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16000</td>\n",
       "      <td>0.157800</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.223253</td>\n",
       "      <td>0.503158</td>\n",
       "      <td>0.632254</td>\n",
       "      <td>0.119937</td>\n",
       "      <td>0.405360</td>\n",
       "      <td>0.303277</td>\n",
       "      <td>0.341208</td>\n",
       "      <td>0.253701</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16500</td>\n",
       "      <td>0.165800</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.221674</td>\n",
       "      <td>0.499210</td>\n",
       "      <td>0.624951</td>\n",
       "      <td>0.118417</td>\n",
       "      <td>0.401465</td>\n",
       "      <td>0.299349</td>\n",
       "      <td>0.336812</td>\n",
       "      <td>0.250210</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17000</td>\n",
       "      <td>0.170200</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.219897</td>\n",
       "      <td>0.503553</td>\n",
       "      <td>0.628899</td>\n",
       "      <td>0.120332</td>\n",
       "      <td>0.407611</td>\n",
       "      <td>0.303078</td>\n",
       "      <td>0.338053</td>\n",
       "      <td>0.253027</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17500</td>\n",
       "      <td>0.163300</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.219700</td>\n",
       "      <td>0.503553</td>\n",
       "      <td>0.634820</td>\n",
       "      <td>0.121141</td>\n",
       "      <td>0.410947</td>\n",
       "      <td>0.304112</td>\n",
       "      <td>0.339292</td>\n",
       "      <td>0.253345</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18000</td>\n",
       "      <td>0.161500</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.224043</td>\n",
       "      <td>0.501382</td>\n",
       "      <td>0.634426</td>\n",
       "      <td>0.122029</td>\n",
       "      <td>0.412487</td>\n",
       "      <td>0.306564</td>\n",
       "      <td>0.342092</td>\n",
       "      <td>0.256018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18500</td>\n",
       "      <td>0.149000</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.217726</td>\n",
       "      <td>0.495657</td>\n",
       "      <td>0.630478</td>\n",
       "      <td>0.121161</td>\n",
       "      <td>0.409221</td>\n",
       "      <td>0.302962</td>\n",
       "      <td>0.337318</td>\n",
       "      <td>0.252352</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19000</td>\n",
       "      <td>0.143600</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.221082</td>\n",
       "      <td>0.501777</td>\n",
       "      <td>0.632649</td>\n",
       "      <td>0.121279</td>\n",
       "      <td>0.410637</td>\n",
       "      <td>0.304135</td>\n",
       "      <td>0.339538</td>\n",
       "      <td>0.253218</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19500</td>\n",
       "      <td>0.143600</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.223845</td>\n",
       "      <td>0.497434</td>\n",
       "      <td>0.628306</td>\n",
       "      <td>0.121200</td>\n",
       "      <td>0.408459</td>\n",
       "      <td>0.304331</td>\n",
       "      <td>0.340183</td>\n",
       "      <td>0.254497</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20000</td>\n",
       "      <td>0.133200</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.222858</td>\n",
       "      <td>0.500987</td>\n",
       "      <td>0.627912</td>\n",
       "      <td>0.121299</td>\n",
       "      <td>0.408864</td>\n",
       "      <td>0.304798</td>\n",
       "      <td>0.340502</td>\n",
       "      <td>0.254934</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20500</td>\n",
       "      <td>0.139700</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.216739</td>\n",
       "      <td>0.498026</td>\n",
       "      <td>0.628899</td>\n",
       "      <td>0.120371</td>\n",
       "      <td>0.407443</td>\n",
       "      <td>0.301275</td>\n",
       "      <td>0.335978</td>\n",
       "      <td>0.251067</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21000</td>\n",
       "      <td>0.150400</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.223056</td>\n",
       "      <td>0.499803</td>\n",
       "      <td>0.635018</td>\n",
       "      <td>0.121891</td>\n",
       "      <td>0.411915</td>\n",
       "      <td>0.305813</td>\n",
       "      <td>0.341588</td>\n",
       "      <td>0.254857</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21500</td>\n",
       "      <td>0.153000</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.219305</td>\n",
       "      <td>0.497631</td>\n",
       "      <td>0.634820</td>\n",
       "      <td>0.120845</td>\n",
       "      <td>0.409113</td>\n",
       "      <td>0.303766</td>\n",
       "      <td>0.338643</td>\n",
       "      <td>0.254059</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22000</td>\n",
       "      <td>0.151500</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.219897</td>\n",
       "      <td>0.496052</td>\n",
       "      <td>0.633439</td>\n",
       "      <td>0.121161</td>\n",
       "      <td>0.408697</td>\n",
       "      <td>0.303440</td>\n",
       "      <td>0.338660</td>\n",
       "      <td>0.253673</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22500</td>\n",
       "      <td>0.138200</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.221082</td>\n",
       "      <td>0.499013</td>\n",
       "      <td>0.631859</td>\n",
       "      <td>0.121023</td>\n",
       "      <td>0.409678</td>\n",
       "      <td>0.303598</td>\n",
       "      <td>0.338561</td>\n",
       "      <td>0.253289</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23000</td>\n",
       "      <td>0.139700</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.218318</td>\n",
       "      <td>0.500395</td>\n",
       "      <td>0.638176</td>\n",
       "      <td>0.122108</td>\n",
       "      <td>0.413439</td>\n",
       "      <td>0.305188</td>\n",
       "      <td>0.338588</td>\n",
       "      <td>0.254025</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23500</td>\n",
       "      <td>0.152300</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.217923</td>\n",
       "      <td>0.500197</td>\n",
       "      <td>0.637979</td>\n",
       "      <td>0.122246</td>\n",
       "      <td>0.413373</td>\n",
       "      <td>0.305452</td>\n",
       "      <td>0.338821</td>\n",
       "      <td>0.254456</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24000</td>\n",
       "      <td>0.132500</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.219108</td>\n",
       "      <td>0.499803</td>\n",
       "      <td>0.639360</td>\n",
       "      <td>0.122661</td>\n",
       "      <td>0.414125</td>\n",
       "      <td>0.305984</td>\n",
       "      <td>0.339465</td>\n",
       "      <td>0.254719</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24324</td>\n",
       "      <td>0.132500</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.218910</td>\n",
       "      <td>0.500790</td>\n",
       "      <td>0.638768</td>\n",
       "      <td>0.122542</td>\n",
       "      <td>0.413962</td>\n",
       "      <td>0.305845</td>\n",
       "      <td>0.339403</td>\n",
       "      <td>0.254630</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluasi Model Finetuned (Best Checkpoint)...\n",
      " [Fine] Hit@10: 0.5513 | MRR@10: 0.3364\n",
      "\n",
      "\n",
      "============================== HASIL AKHIR SKRIPSI ==============================\n",
      "               Model        Type  Hit@1  Hit@5  Hit@10  MRR@10  NDCG@10  \\\n",
      "0  all-mpnet-base-v2    Baseline 0.2417 0.4530  0.5447  0.3319   0.2468   \n",
      "1  all-mpnet-base-v2  Fine-Tuned 0.2446 0.4598  0.5513  0.3364   0.2510   \n",
      "\n",
      "   Precision@10  Recall@10  \n",
      "0        0.1116     0.2835  \n",
      "1        0.1137     0.2886  \n",
      "\n",
      "✅ Hasil lengkap tersimpan di: hasil_retrieval_mpnet_final.csv\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import os\n",
    "import pandas as pd\n",
    "from sentence_transformers import SentenceTransformer, InputExample, losses, models, evaluation\n",
    "from torch.utils.data import DataLoader\n",
    "import torch\n",
    "import gc\n",
    "\n",
    "# --- CONFIG KHUSUS VRAM 6GB ---\n",
    "DATA_DIR = \"final_dataset_retrieval_split\"\n",
    "OUTPUT_DIR = \"models-retrieval-mpnet\"\n",
    "\n",
    "# Batch Size 8-10 adalah batas aman MPNet di 6GB VRAM.\n",
    "# Jika masih OOM, turunkan ke 6 atau 4.\n",
    "BATCH_SIZE = 8  \n",
    "\n",
    "EPOCHS = 2 # MPNet cepat pintar, 1 epoch biasanya cukup untuk data sitasi\n",
    "\n",
    "MODELS_TO_TEST = [\n",
    "    \"sentence-transformers/all-mpnet-base-v2\"\n",
    "]\n",
    "\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "# --- 1. LOAD DATA ---\n",
    "def load_json(filename):\n",
    "    print(f\"Loading {filename}...\")\n",
    "    path = os.path.join(DATA_DIR, filename)\n",
    "    with open(path, 'r', encoding='utf-8') as f:\n",
    "        return json.load(f)\n",
    "\n",
    "# Load Data\n",
    "train_raw = load_json(\"train_pairs.json\")\n",
    "test_queries_raw = load_json(\"test_queries.json\")\n",
    "corpus_test_raw = load_json(\"corpus_test.json\")\n",
    "val_queries_raw = load_json(\"val_queries.json\")\n",
    "corpus_val_raw = load_json(\"corpus_val.json\")\n",
    "\n",
    "# --- 2. SETUP EVALUATOR ---\n",
    "def create_evaluator(queries_raw, corpus_raw, name_prefix):\n",
    "    print(f\"Menyiapkan Evaluator: {name_prefix.upper()}...\")\n",
    "    corpus_dict = {item['id']: item['text'] for item in corpus_raw}\n",
    "    text_to_id_map = {item['text']: item['id'] for item in corpus_raw}\n",
    "    \n",
    "    queries_dict = {}\n",
    "    relevant_docs = {}\n",
    "\n",
    "    for i, item in enumerate(queries_raw):\n",
    "        qid = f\"{name_prefix}_q_{i}\"\n",
    "        queries_dict[qid] = item['query']\n",
    "        ground_truth_ids = set()\n",
    "        for gt_text in item['ground_truths']:\n",
    "            found_id = text_to_id_map.get(gt_text)\n",
    "            if found_id:\n",
    "                ground_truth_ids.add(found_id)\n",
    "        if ground_truth_ids:\n",
    "            relevant_docs[qid] = ground_truth_ids\n",
    "            \n",
    "    return evaluation.InformationRetrievalEvaluator(\n",
    "        queries_dict, corpus_dict, relevant_docs,\n",
    "        show_progress_bar=False, name=name_prefix,\n",
    "        mrr_at_k=[10], ndcg_at_k=[10], accuracy_at_k=[1, 5, 10], precision_recall_at_k=[10]\n",
    "    )\n",
    "\n",
    "val_evaluator = create_evaluator(val_queries_raw, corpus_val_raw, \"val\")\n",
    "test_evaluator = create_evaluator(test_queries_raw, corpus_test_raw, \"test\")\n",
    "\n",
    "# --- 3. METRIK ---\n",
    "results_table = []\n",
    "\n",
    "def extract_metrics(metrics_dict, model_name, run_type):\n",
    "    prefix = \"test_\" if \"Baseline\" in run_type or \"Fine-Tuned\" in run_type else \"val_\"\n",
    "    return {\n",
    "        \"Model\": model_name,\n",
    "        \"Type\": run_type,\n",
    "        \"Hit@1\": metrics_dict.get(f'{prefix}cosine_accuracy@1', 0),\n",
    "        \"Hit@5\": metrics_dict.get(f'{prefix}cosine_accuracy@5', 0),\n",
    "        \"Hit@10\": metrics_dict.get(f'{prefix}cosine_accuracy@10', 0),\n",
    "        \"MRR@10\": metrics_dict.get(f'{prefix}cosine_mrr@10', 0),\n",
    "        \"NDCG@10\": metrics_dict.get(f'{prefix}cosine_ndcg@10', 0),\n",
    "        \"Precision@10\": metrics_dict.get(f'{prefix}cosine_precision@10', 0),\n",
    "        \"Recall@10\": metrics_dict.get(f'{prefix}cosine_recall@10', 0)\n",
    "    }\n",
    "\n",
    "# --- 4. EXPERIMENT LOOP (HEMAT MEMORI) ---\n",
    "def run_experiment(model_name):\n",
    "    short_name = model_name.split(\"/\")[-1]\n",
    "    print(f\"\\n{'='*60}\\nEXPERIMENT: {short_name}\\n{'='*60}\")\n",
    "\n",
    "    # Bersihkan memori seagresif mungkin\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "    # A. BASELINE\n",
    "    print(f\"Evaluasi Baseline (Pre-trained)...\")\n",
    "    model_base = SentenceTransformer(model_name)\n",
    "    metrics_base = test_evaluator(model_base)\n",
    "    res_base = extract_metrics(metrics_base, short_name, \"Baseline\")\n",
    "    results_table.append(res_base)\n",
    "    print(f\" [Base] Hit@10: {res_base['Hit@10']:.4f} | MRR@10: {res_base['MRR@10']:.4f}\")\n",
    "    \n",
    "    del model_base\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "    # B. FINE-TUNING\n",
    "    print(f\"Training (Fine-tuning)...\")\n",
    "    \n",
    "    # 1. Batasi Panjang Sequence (Wajib untuk 6GB VRAM)\n",
    "    # 256 adalah batas aman. Jika masih OOM, turunkan ke 200.\n",
    "    word_emb = models.Transformer(model_name, max_seq_length=256)\n",
    "    pooling = models.Pooling(word_emb.get_word_embedding_dimension())\n",
    "    model_ft = SentenceTransformer(modules=[word_emb, pooling])\n",
    "    \n",
    "    train_ex = [InputExample(texts=[d['anchor'], d['positive']]) for d in train_raw]\n",
    "    train_dl = DataLoader(train_ex, shuffle=True, batch_size=BATCH_SIZE)\n",
    "    train_loss = losses.MultipleNegativesRankingLoss(model_ft)\n",
    "    \n",
    "    save_path = os.path.join(OUTPUT_DIR, f\"{short_name}-finetuned\")\n",
    "    \n",
    "    # 2. Fit dengan Trik Hemat Memori\n",
    "    model_ft.fit(\n",
    "        train_objectives=[(train_dl, train_loss)],\n",
    "        epochs=EPOCHS,\n",
    "        warmup_steps=int(len(train_dl) * 0.1),\n",
    "        \n",
    "        # Learning rate standar untuk MPNet\n",
    "        optimizer_params={'lr': 2e-5},\n",
    "        \n",
    "        # WAJIB: Mixed Precision (Hemat 40% VRAM)\n",
    "        use_amp=True,\n",
    "        \n",
    "        show_progress_bar=True,\n",
    "        evaluator=val_evaluator,\n",
    "        evaluation_steps=500,\n",
    "        save_best_model=True,\n",
    "        output_path=save_path\n",
    "    )\n",
    "    \n",
    "    # C. FINAL EVALUATION\n",
    "    print(f\"Evaluasi Model Finetuned (Best Checkpoint)...\")\n",
    "    \n",
    "    # Hapus model lama dari memori dulu sebelum load yang baru\n",
    "    del model_ft\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "    \n",
    "    best_model = SentenceTransformer(save_path)\n",
    "    metrics_ft = test_evaluator(best_model)\n",
    "    res_ft = extract_metrics(metrics_ft, short_name, \"Fine-Tuned\")\n",
    "    results_table.append(res_ft)\n",
    "    print(f\" [Fine] Hit@10: {res_ft['Hit@10']:.4f} | MRR@10: {res_ft['MRR@10']:.4f}\")\n",
    "\n",
    "    del best_model\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "# --- EKSEKUSI DENGAN ERROR HANDLING ---\n",
    "for model in MODELS_TO_TEST:\n",
    "    try:\n",
    "        run_experiment(model)\n",
    "    except RuntimeError as e:\n",
    "        if \"out of memory\" in str(e).lower():\n",
    "            print(f\"\\n❌ GPU OOM (Out of Memory)!\")\n",
    "            print(\"Saran: Turunkan BATCH_SIZE dari {BATCH_SIZE} menjadi {int(BATCH_SIZE/2)}.\")\n",
    "            print(\"Atau turunkan max_seq_length di word_emb menjadi 200.\")\n",
    "        else:\n",
    "            raise e\n",
    "\n",
    "# HASIL AKHIR\n",
    "print(\"\\n\\n\" + \"=\"*30 + \" HASIL AKHIR SKRIPSI \" + \"=\"*30)\n",
    "df = pd.DataFrame(results_table)\n",
    "cols = [\"Model\", \"Type\", \"Hit@1\", \"Hit@5\", \"Hit@10\", \"MRR@10\", \"NDCG@10\", \"Precision@10\", \"Recall@10\"]\n",
    "df = df[cols]\n",
    "\n",
    "pd.options.display.float_format = '{:,.4f}'.format\n",
    "print(df)\n",
    "\n",
    "csv_path = \"hasil_retrieval_mpnet_final.csv\"\n",
    "df.to_csv(csv_path, index=False)\n",
    "print(f\"\\n✅ Hasil lengkap tersimpan di: {csv_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7d208416",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading final_dataset_retrieval_split\\train_pairs.json...\n",
      "Loading final_dataset_retrieval_split\\corpus_train.json...\n",
      "Total Pasangan: 97292\n",
      "Total Corpus: 62989\n",
      "\n",
      ">>> Membuat Triplets dengan Random Negatives...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 97292/97292 [00:00<00:00, 707428.29it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✅ Selesai! File tersimpan di: final_dataset_retrieval_split\\train_triplets_random.json\n",
      "Contoh data: {'anchor': 'Author(s): Kuperberg, Greg; Thurston, Dylan P. | Abstract: We give a purely topological definition of the perturbative quantum invariants of links and 3-manifolds associated with Chern-Simons field theory. Our definition is as close as possible to one given by Kontsevich. We will also establish some basic properties of these invariants, in particular that they are universally finite type with respect to algebraically split surgery and with respect to Torelli surgery. Torelli surgery is a mutual generalization of blink surgery of Garoufalidis and Levine and clasper surgery of Habiro.', 'positive': 'This note is a sequel to our earlier paper of the same title [4] and describes invariants of rational homology 3-spheres associated to acyclic orthogonal local systems. Our work is in the spirit of the Axelrod–Singer papers [1], generalizes some of their results, and furnishes a new setting for the purely topological implications of their work.', 'negative': 'Segmentation of 3D images is a fundamental problem in biomedical image analysis. Deep learning (DL) approaches have achieved the state-of-the-art segmentation performance. To exploit the 3D contexts using neural networks, known DL segmentation methods, including 3D convolution, 2D convolution on the planes orthogonal to 2D slices, and LSTM in multiple directions, all suffer incompatibility with the highly anisotropic dimensions in common 3D biomedical images. In this paper, we propose a new DL framework for 3D image segmentation, based on a combination of a fully convolutional network (FCN) and a recurrent neural network (RNN), which are responsible for exploiting the intra-slice and inter-slice contexts, respectively. To our best knowledge, this is the first DL framework for 3D image segmentation that explicitly leverages 3D image anisotropism. Evaluating using a dataset from the ISBI Neuronal Structure Segmentation Challenge and in-house image stacks for 3D fungus segmentation, our approach achieves promising results, comparing to the known DL-based 3D segmentation approaches.'}\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import os\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "\n",
    "# --- CONFIG ---\n",
    "DATA_DIR = \"final_dataset_retrieval_split\"\n",
    "OUTPUT_FILE = \"train_triplets_random.json\"\n",
    "\n",
    "# --- LOAD DATA ---\n",
    "def load_json(filename):\n",
    "    path = os.path.join(DATA_DIR, filename)\n",
    "    print(f\"Loading {path}...\")\n",
    "    with open(path, 'r', encoding='utf-8') as f:\n",
    "        return json.load(f)\n",
    "\n",
    "train_pairs = load_json(\"train_pairs.json\")\n",
    "corpus_train = load_json(\"corpus_train.json\")\n",
    "\n",
    "# Ambil semua teks dari corpus untuk dijadikan kolam pengambilan acak\n",
    "corpus_texts = [item['text'] for item in corpus_train]\n",
    "\n",
    "print(f\"Total Pasangan: {len(train_pairs)}\")\n",
    "print(f\"Total Corpus: {len(corpus_texts)}\")\n",
    "\n",
    "# --- GENERATE RANDOM NEGATIVES ---\n",
    "print(\"\\n>>> Membuat Triplets dengan Random Negatives...\")\n",
    "triplets = []\n",
    "\n",
    "for item in tqdm(train_pairs):\n",
    "    anchor = item['anchor']\n",
    "    positive = item['positive']\n",
    "    \n",
    "    # Ambil 1 Negative secara Acak\n",
    "    while True:\n",
    "        negative = random.choice(corpus_texts)\n",
    "        \n",
    "        # Pastikan Negative tidak sama dengan Anchor maupun Positive\n",
    "        if negative != anchor and negative != positive:\n",
    "            break\n",
    "            \n",
    "    triplets.append({\n",
    "        \"anchor\": anchor,\n",
    "        \"positive\": positive,\n",
    "        \"negative\": negative\n",
    "    })\n",
    "\n",
    "# --- SIMPAN ---\n",
    "save_path = os.path.join(DATA_DIR, OUTPUT_FILE)\n",
    "with open(save_path, 'w', encoding='utf-8') as f:\n",
    "    json.dump(triplets, f, ensure_ascii=False, indent=4)\n",
    "\n",
    "print(f\"\\n✅ Selesai! File tersimpan di: {save_path}\")\n",
    "print(f\"Contoh data: {triplets[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9fd4e8ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Iskandar\\Documents\\GitHub\\Tubes-NLP-PaperMatch\\ret_env\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading train_triplets_random.json...\n",
      "Loading val_queries.json...\n",
      "Loading corpus_val.json...\n",
      "Loading test_queries.json...\n",
      "Loading corpus_test.json...\n",
      "Menyiapkan Evaluator: VAL...\n",
      "Menyiapkan Evaluator: TEST...\n",
      "\n",
      "============================================================\n",
      "EXPERIMENT: all-MiniLM-L6-v2 (Triplets)\n",
      "============================================================\n",
      "Evaluasi Baseline...\n",
      " [Base] Hit@10: 0.5307 | MRR@10: 0.3230\n",
      "Training (Fine-tuning with Triplets)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                             \r"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='4563' max='4563' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [4563/4563 33:05, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Val Cosine Accuracy@1</th>\n",
       "      <th>Val Cosine Accuracy@5</th>\n",
       "      <th>Val Cosine Accuracy@10</th>\n",
       "      <th>Val Cosine Precision@10</th>\n",
       "      <th>Val Cosine Recall@10</th>\n",
       "      <th>Val Cosine Ndcg@10</th>\n",
       "      <th>Val Cosine Mrr@10</th>\n",
       "      <th>Val Cosine Map@100</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>1.425600</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.222661</td>\n",
       "      <td>0.505922</td>\n",
       "      <td>0.627912</td>\n",
       "      <td>0.119266</td>\n",
       "      <td>0.402493</td>\n",
       "      <td>0.301585</td>\n",
       "      <td>0.339831</td>\n",
       "      <td>0.250103</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>1.320100</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.223450</td>\n",
       "      <td>0.511646</td>\n",
       "      <td>0.636597</td>\n",
       "      <td>0.120134</td>\n",
       "      <td>0.408043</td>\n",
       "      <td>0.305020</td>\n",
       "      <td>0.344333</td>\n",
       "      <td>0.253021</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>1.258400</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.223648</td>\n",
       "      <td>0.504145</td>\n",
       "      <td>0.635413</td>\n",
       "      <td>0.120371</td>\n",
       "      <td>0.407484</td>\n",
       "      <td>0.303710</td>\n",
       "      <td>0.342071</td>\n",
       "      <td>0.251774</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1521</td>\n",
       "      <td>1.258400</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.223648</td>\n",
       "      <td>0.505922</td>\n",
       "      <td>0.633241</td>\n",
       "      <td>0.119779</td>\n",
       "      <td>0.405764</td>\n",
       "      <td>0.303349</td>\n",
       "      <td>0.342276</td>\n",
       "      <td>0.252092</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>1.146400</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.224832</td>\n",
       "      <td>0.509080</td>\n",
       "      <td>0.641334</td>\n",
       "      <td>0.121931</td>\n",
       "      <td>0.412311</td>\n",
       "      <td>0.307114</td>\n",
       "      <td>0.344443</td>\n",
       "      <td>0.254344</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2500</td>\n",
       "      <td>1.148500</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.224635</td>\n",
       "      <td>0.508685</td>\n",
       "      <td>0.640347</td>\n",
       "      <td>0.121575</td>\n",
       "      <td>0.411153</td>\n",
       "      <td>0.306124</td>\n",
       "      <td>0.343862</td>\n",
       "      <td>0.253904</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>1.120900</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.227398</td>\n",
       "      <td>0.508488</td>\n",
       "      <td>0.637386</td>\n",
       "      <td>0.121141</td>\n",
       "      <td>0.407987</td>\n",
       "      <td>0.305982</td>\n",
       "      <td>0.346252</td>\n",
       "      <td>0.254727</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3042</td>\n",
       "      <td>1.120900</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.228188</td>\n",
       "      <td>0.508488</td>\n",
       "      <td>0.639953</td>\n",
       "      <td>0.121457</td>\n",
       "      <td>0.411575</td>\n",
       "      <td>0.307509</td>\n",
       "      <td>0.346690</td>\n",
       "      <td>0.255622</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3500</td>\n",
       "      <td>1.059800</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.226214</td>\n",
       "      <td>0.506119</td>\n",
       "      <td>0.638373</td>\n",
       "      <td>0.121141</td>\n",
       "      <td>0.408842</td>\n",
       "      <td>0.306045</td>\n",
       "      <td>0.344626</td>\n",
       "      <td>0.255208</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4000</td>\n",
       "      <td>1.042500</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.228188</td>\n",
       "      <td>0.510857</td>\n",
       "      <td>0.636794</td>\n",
       "      <td>0.121319</td>\n",
       "      <td>0.409269</td>\n",
       "      <td>0.306492</td>\n",
       "      <td>0.345992</td>\n",
       "      <td>0.255341</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4500</td>\n",
       "      <td>1.043300</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.226806</td>\n",
       "      <td>0.508883</td>\n",
       "      <td>0.638768</td>\n",
       "      <td>0.121259</td>\n",
       "      <td>0.409944</td>\n",
       "      <td>0.306290</td>\n",
       "      <td>0.345924</td>\n",
       "      <td>0.254985</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4563</td>\n",
       "      <td>1.043300</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.226609</td>\n",
       "      <td>0.508685</td>\n",
       "      <td>0.638373</td>\n",
       "      <td>0.121279</td>\n",
       "      <td>0.409928</td>\n",
       "      <td>0.306295</td>\n",
       "      <td>0.345791</td>\n",
       "      <td>0.254996</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluasi Model Finetuned (Best Checkpoint)...\n",
      " [Fine] Hit@10: 0.5453 | MRR@10: 0.3291\n",
      "\n",
      "\n",
      "============================== HASIL AKHIR LENGKAP ==============================\n",
      "              Model        Type  Hit@1  Hit@5  Hit@10  MRR@10  NDCG@10  \\\n",
      "0  all-MiniLM-L6-v2    Baseline 0.2348 0.4420  0.5307  0.3230   0.2377   \n",
      "1  all-MiniLM-L6-v2  Fine-Tuned 0.2370 0.4526  0.5453  0.3291   0.2466   \n",
      "\n",
      "   Precision@10  Recall@10  \n",
      "0        0.1069     0.2715  \n",
      "1        0.1123     0.2853  \n",
      "\n",
      "✅ Hasil lengkap tersimpan di: hasil_retrieval_minilm_triplets.csv\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import os\n",
    "import pandas as pd\n",
    "from sentence_transformers import SentenceTransformer, InputExample, losses, models, evaluation\n",
    "from torch.utils.data import DataLoader\n",
    "import torch\n",
    "import gc\n",
    "\n",
    "# --- CONFIG ---\n",
    "DATA_DIR = \"final_dataset_retrieval_split\" \n",
    "OUTPUT_DIR = \"models-retrieval-minilm-triplets\"\n",
    "\n",
    "# File Training (Gunakan yang Triplets/Random Negatives)\n",
    "TRAIN_FILE = \"train_triplets_random.json\" \n",
    "\n",
    "# MiniLM ringan, bisa pakai Batch Size besar\n",
    "BATCH_SIZE = 64  \n",
    "EPOCHS = 3       \n",
    "\n",
    "MODELS_TO_TEST = [\n",
    "    \"sentence-transformers/all-MiniLM-L6-v2\"\n",
    "]\n",
    "\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "# --- 1. LOAD DATA ---\n",
    "def load_json(filename):\n",
    "    print(f\"Loading {filename}...\")\n",
    "    path = os.path.join(DATA_DIR, filename)\n",
    "    with open(path, 'r', encoding='utf-8') as f:\n",
    "        return json.load(f)\n",
    "\n",
    "# Load Data Training (Triplets)\n",
    "train_raw = load_json(TRAIN_FILE)\n",
    "\n",
    "# Load Data Validasi & Test\n",
    "val_queries_raw = load_json(\"val_queries.json\")\n",
    "corpus_val_raw = load_json(\"corpus_val.json\")\n",
    "\n",
    "test_queries_raw = load_json(\"test_queries.json\")\n",
    "corpus_test_raw = load_json(\"corpus_test.json\")\n",
    "\n",
    "# --- 2. SETUP EVALUATOR ---\n",
    "def create_evaluator(queries_raw, corpus_raw, name_prefix):\n",
    "    print(f\"Menyiapkan Evaluator: {name_prefix.upper()}...\")\n",
    "    corpus_dict = {item['id']: item['text'] for item in corpus_raw}\n",
    "    text_to_id_map = {item['text']: item['id'] for item in corpus_raw}\n",
    "    \n",
    "    queries_dict = {}\n",
    "    relevant_docs = {}\n",
    "\n",
    "    for i, item in enumerate(queries_raw):\n",
    "        qid = f\"{name_prefix}_q_{i}\"\n",
    "        queries_dict[qid] = item['query']\n",
    "        ground_truth_ids = set()\n",
    "        for gt_text in item['ground_truths']:\n",
    "            found_id = text_to_id_map.get(gt_text)\n",
    "            if found_id:\n",
    "                ground_truth_ids.add(found_id)\n",
    "        if ground_truth_ids:\n",
    "            relevant_docs[qid] = ground_truth_ids\n",
    "            \n",
    "    return evaluation.InformationRetrievalEvaluator(\n",
    "        queries_dict, corpus_dict, relevant_docs,\n",
    "        show_progress_bar=False, name=name_prefix,\n",
    "        mrr_at_k=[10], ndcg_at_k=[10], accuracy_at_k=[1, 5, 10], precision_recall_at_k=[10]\n",
    "    )\n",
    "\n",
    "val_evaluator = create_evaluator(val_queries_raw, corpus_val_raw, \"val\")\n",
    "test_evaluator = create_evaluator(test_queries_raw, corpus_test_raw, \"test\")\n",
    "\n",
    "# --- 3. METRIK ---\n",
    "results_table = []\n",
    "\n",
    "def extract_metrics(metrics_dict, model_name, run_type):\n",
    "    prefix = \"test_\" if \"Baseline\" in run_type or \"Fine-Tuned\" in run_type else \"val_\"\n",
    "    return {\n",
    "        \"Model\": model_name,\n",
    "        \"Type\": run_type,\n",
    "        \"Hit@1\": metrics_dict.get(f'{prefix}cosine_accuracy@1', 0),\n",
    "        \"Hit@5\": metrics_dict.get(f'{prefix}cosine_accuracy@5', 0),\n",
    "        \"Hit@10\": metrics_dict.get(f'{prefix}cosine_accuracy@10', 0),\n",
    "        \"MRR@10\": metrics_dict.get(f'{prefix}cosine_mrr@10', 0),\n",
    "        \"NDCG@10\": metrics_dict.get(f'{prefix}cosine_ndcg@10', 0),\n",
    "        \"Precision@10\": metrics_dict.get(f'{prefix}cosine_precision@10', 0),\n",
    "        \"Recall@10\": metrics_dict.get(f'{prefix}cosine_recall@10', 0)\n",
    "    }\n",
    "\n",
    "# --- 4. EXPERIMENT LOOP ---\n",
    "def run_experiment(model_name):\n",
    "    short_name = model_name.split(\"/\")[-1]\n",
    "    print(f\"\\n{'='*60}\\nEXPERIMENT: {short_name} (Triplets)\\n{'='*60}\")\n",
    "\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "    # A. BASELINE\n",
    "    print(f\"Evaluasi Baseline...\")\n",
    "    model_base = SentenceTransformer(model_name)\n",
    "    metrics_base = test_evaluator(model_base)\n",
    "    res_base = extract_metrics(metrics_base, short_name, \"Baseline\")\n",
    "    results_table.append(res_base)\n",
    "    print(f\" [Base] Hit@10: {res_base['Hit@10']:.4f} | MRR@10: {res_base['MRR@10']:.4f}\")\n",
    "    \n",
    "    del model_base\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "    # B. FINE-TUNING\n",
    "    print(f\"Training (Fine-tuning with Triplets)...\")\n",
    "    \n",
    "    # Init Model (Max Length 512 agar baca abstrak full)\n",
    "    # MiniLM kuat handle 512 token\n",
    "    word_emb = models.Transformer(model_name, max_seq_length=216)\n",
    "    pooling = models.Pooling(word_emb.get_word_embedding_dimension())\n",
    "    model_ft = SentenceTransformer(modules=[word_emb, pooling])\n",
    "    \n",
    "    # --- PERUBAHAN UTAMA: FORMAT DATA TRIPLET ---\n",
    "    train_ex = []\n",
    "    for d in train_raw:\n",
    "        # Kita masukkan 3 kolom: [Anchor, Positive, Negative]\n",
    "        train_ex.append(InputExample(texts=[d['anchor'], d['positive'], d['negative']]))\n",
    "        \n",
    "    train_dl = DataLoader(train_ex, shuffle=True, batch_size=BATCH_SIZE)\n",
    "    \n",
    "    # Loss Function otomatis tahu kalau inputnya 3 kolom, dia pakai Triplet Logic\n",
    "    train_loss = losses.MultipleNegativesRankingLoss(model_ft)\n",
    "    \n",
    "    save_path = os.path.join(OUTPUT_DIR, f\"{short_name}-finetuned\")\n",
    "    \n",
    "    model_ft.fit(\n",
    "        train_objectives=[(train_dl, train_loss)],\n",
    "        epochs=EPOCHS,\n",
    "        warmup_steps=int(len(train_dl) * 0.1),\n",
    "        optimizer_params={'lr': 2e-5},\n",
    "        use_amp=True,\n",
    "        show_progress_bar=True,\n",
    "        evaluator=val_evaluator,\n",
    "        evaluation_steps=500,\n",
    "        save_best_model=True,\n",
    "        output_path=save_path\n",
    "    )\n",
    "    \n",
    "    # C. EVALUASI\n",
    "    print(f\"Evaluasi Model Finetuned (Best Checkpoint)...\")\n",
    "    \n",
    "    del model_ft\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "    \n",
    "    best_model = SentenceTransformer(save_path)\n",
    "    metrics_ft = test_evaluator(best_model)\n",
    "    res_ft = extract_metrics(metrics_ft, short_name, \"Fine-Tuned\")\n",
    "    results_table.append(res_ft)\n",
    "    print(f\" [Fine] Hit@10: {res_ft['Hit@10']:.4f} | MRR@10: {res_ft['MRR@10']:.4f}\")\n",
    "\n",
    "    del best_model\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "# EKSEKUSI\n",
    "for model in MODELS_TO_TEST:\n",
    "    run_experiment(model)\n",
    "\n",
    "# HASIL AKHIR\n",
    "print(\"\\n\\n\" + \"=\"*30 + \" HASIL AKHIR LENGKAP \" + \"=\"*30)\n",
    "df = pd.DataFrame(results_table)\n",
    "cols = [\"Model\", \"Type\", \"Hit@1\", \"Hit@5\", \"Hit@10\", \"MRR@10\", \"NDCG@10\", \"Precision@10\", \"Recall@10\"]\n",
    "df = df[cols]\n",
    "\n",
    "pd.options.display.float_format = '{:,.4f}'.format\n",
    "print(df)\n",
    "\n",
    "csv_path = \"hasil_retrieval_minilm_triplets.csv\"\n",
    "df.to_csv(csv_path, index=False)\n",
    "print(f\"\\n✅ Hasil lengkap tersimpan di: {csv_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1b6167ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading NLTK resources...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Iskandar\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\Iskandar\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\Iskandar\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Cleaning corpus_val.json...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 13612/13612 [00:08<00:00, 1551.56it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Tersimpan: final_dataset_retrieval_cleaned\\corpus_val.json\n",
      "\n",
      "Cleaning corpus_test.json...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 11568/11568 [00:05<00:00, 2220.90it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Tersimpan: final_dataset_retrieval_cleaned\\corpus_test.json\n",
      "\n",
      "Cleaning corpus_train.json...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 62989/62989 [00:20<00:00, 3077.51it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Tersimpan: final_dataset_retrieval_cleaned\\corpus_train.json\n",
      "\n",
      "Cleaning train_pairs.json...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 97292/97292 [01:30<00:00, 1075.14it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Tersimpan: final_dataset_retrieval_cleaned\\train_triplets_random.json\n",
      "\n",
      "Cleaning val_queries.json...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5066/5066 [00:07<00:00, 708.19it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Tersimpan: final_dataset_retrieval_cleaned\\val_queries.json\n",
      "\n",
      "Cleaning test_queries.json...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5093/5093 [00:08<00:00, 586.08it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Tersimpan: final_dataset_retrieval_cleaned\\test_queries.json\n",
      "\n",
      "🎉 SEMUA DATA SELESAI DIBERSIHKAN! Siap untuk Training.\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import os\n",
    "import re\n",
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from tqdm import tqdm\n",
    "\n",
    "# --- 1. SETUP CLEANING TOOLS ---\n",
    "print(\"Downloading NLTK resources...\")\n",
    "try:\n",
    "    nltk.data.find('corpora/stopwords')\n",
    "    nltk.data.find('corpora/wordnet')\n",
    "    nltk.data.find('corpora/omw-1.4')\n",
    "except LookupError:\n",
    "    nltk.download('stopwords')\n",
    "    nltk.download('wordnet')\n",
    "    nltk.download('omw-1.4')\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# Stopwords Custom + Bawaan\n",
    "custom_stopwords = set([\n",
    "    'based', 'proposed', 'using', 'paper', 'data', 'results', 'method', \n",
    "    'model', 'approach', 'analysis', 'study', 'performance', 'new', \n",
    "    'presented', 'show', 'demonstrate', 'investigate', 'we', 'our'\n",
    "])\n",
    "stop_words = set(stopwords.words('english')) | custom_stopwords\n",
    "\n",
    "def clean_text(text):\n",
    "    if not isinstance(text, str): return \"\"\n",
    "    \n",
    "    # 1. Lowercase\n",
    "    text = text.lower()\n",
    "    \n",
    "    # 2. Hapus karakter non-alphanumeric (sisakan spasi)\n",
    "    text = re.sub(r'[^a-z0-9\\s]', '', text)\n",
    "    \n",
    "    # 3. Tokenize (Split by space)\n",
    "    tokens = text.split()\n",
    "    \n",
    "    # 4. Remove Stopwords & Lemmatization\n",
    "    cleaned_tokens = [\n",
    "        lemmatizer.lemmatize(word) \n",
    "        for word in tokens \n",
    "        if word not in stop_words and len(word) > 2 # Hapus kata kependekan (1-2 huruf)\n",
    "    ]\n",
    "    \n",
    "    # 5. Join kembali\n",
    "    return \" \".join(cleaned_tokens)\n",
    "\n",
    "# --- 2. CONFIG IO ---\n",
    "INPUT_DIR = \"final_dataset_retrieval_split\"\n",
    "OUTPUT_DIR = \"final_dataset_retrieval_cleaned\"\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "def load_json(filename):\n",
    "    path = os.path.join(INPUT_DIR, filename)\n",
    "    with open(path, 'r', encoding='utf-8') as f:\n",
    "        return json.load(f)\n",
    "\n",
    "def save_json(data, filename):\n",
    "    path = os.path.join(OUTPUT_DIR, filename)\n",
    "    with open(path, 'w', encoding='utf-8') as f:\n",
    "        json.dump(data, f, ensure_ascii=False, indent=4)\n",
    "    print(f\"✅ Tersimpan: {path}\")\n",
    "\n",
    "# --- 3. PROSES CLEANING ---\n",
    "\n",
    "# A. Clean CORPUS (Val & Test)\n",
    "# PENTING: Corpus harus bersih, agar pencarian match dengan query yang bersih\n",
    "for corp_file in [\"corpus_val.json\", \"corpus_test.json\", \"corpus_train.json\"]:\n",
    "    if not os.path.exists(os.path.join(INPUT_DIR, corp_file)): continue\n",
    "    \n",
    "    print(f\"\\nCleaning {corp_file}...\")\n",
    "    raw_data = load_json(corp_file)\n",
    "    cleaned_data = []\n",
    "    \n",
    "    for item in tqdm(raw_data):\n",
    "        # ID jangan diubah! Hanya text yang dibersihkan\n",
    "        cleaned_text = clean_text(item['text'])\n",
    "        if cleaned_text:\n",
    "            cleaned_data.append({\"id\": item['id'], \"text\": cleaned_text})\n",
    "            \n",
    "    save_json(cleaned_data, corp_file)\n",
    "\n",
    "# B. Clean TRAIN PAIRS\n",
    "print(f\"\\nCleaning train_pairs.json...\")\n",
    "train_raw = load_json(\"train_triplets_random.json\")\n",
    "train_cleaned = []\n",
    "\n",
    "for item in tqdm(train_raw):\n",
    "    # Bersihkan Anchor & Positive\n",
    "    anc = clean_text(item['anchor'])\n",
    "    pos = clean_text(item['positive'])\n",
    "    neg = clean_text(item['negative'])  # Negative tidak dipakai di training sekarang\n",
    "    \n",
    "    if anc and pos:\n",
    "        train_cleaned.append({\"anchor\": anc, \"positive\": pos, \"negative\": neg})\n",
    "\n",
    "save_json(train_cleaned, \"train_triplets_random.json\")\n",
    "\n",
    "# C. Clean QUERIES (Val & Test)\n",
    "# PENTING: Ground Truths (teks jawaban) JUGA harus dibersihkan agar match dengan Corpus bersih\n",
    "for query_file in [\"val_queries.json\", \"test_queries.json\"]:\n",
    "    print(f\"\\nCleaning {query_file}...\")\n",
    "    q_raw = load_json(query_file)\n",
    "    q_cleaned = []\n",
    "    \n",
    "    for item in tqdm(q_raw):\n",
    "        q_text = clean_text(item['query'])\n",
    "        \n",
    "        # Bersihkan list ground truths juga!\n",
    "        gt_cleaned = [clean_text(gt) for gt in item['ground_truths']]\n",
    "        gt_cleaned = [gt for gt in gt_cleaned if gt] # Hapus yang kosong\n",
    "        \n",
    "        if q_text and gt_cleaned:\n",
    "            q_cleaned.append({\n",
    "                \"query\": q_text,\n",
    "                \"ground_truths\": gt_cleaned\n",
    "            })\n",
    "            \n",
    "    save_json(q_cleaned, query_file)\n",
    "\n",
    "print(\"\\n🎉 SEMUA DATA SELESAI DIBERSIHKAN! Siap untuk Training.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "50bf8a33",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Iskandar\\Documents\\GitHub\\Tubes-NLP-PaperMatch\\ret_env\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading train_triplets_random.json...\n",
      "Loading val_queries.json...\n",
      "Loading corpus_val.json...\n",
      "Loading test_queries.json...\n",
      "Loading corpus_test.json...\n",
      "Menyiapkan Evaluator: VAL...\n",
      "Menyiapkan Evaluator: TEST...\n",
      "\n",
      "============================================================\n",
      "EXPERIMENT: all-MiniLM-L6-v2 (Triplets)\n",
      "============================================================\n",
      "Evaluasi Baseline...\n",
      " [Base] Hit@10: 0.4962 | MRR@10: 0.2998\n",
      "Training (Fine-tuning with Triplets)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                     \r"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='4563' max='4563' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [4563/4563 2:10:37, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Val Cosine Accuracy@1</th>\n",
       "      <th>Val Cosine Accuracy@5</th>\n",
       "      <th>Val Cosine Accuracy@10</th>\n",
       "      <th>Val Cosine Precision@10</th>\n",
       "      <th>Val Cosine Recall@10</th>\n",
       "      <th>Val Cosine Ndcg@10</th>\n",
       "      <th>Val Cosine Mrr@10</th>\n",
       "      <th>Val Cosine Map@100</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>1.526800</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.216739</td>\n",
       "      <td>0.495263</td>\n",
       "      <td>0.617647</td>\n",
       "      <td>0.114686</td>\n",
       "      <td>0.389826</td>\n",
       "      <td>0.292851</td>\n",
       "      <td>0.334827</td>\n",
       "      <td>0.241964</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>1.397800</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.220292</td>\n",
       "      <td>0.498223</td>\n",
       "      <td>0.623569</td>\n",
       "      <td>0.115476</td>\n",
       "      <td>0.393255</td>\n",
       "      <td>0.294458</td>\n",
       "      <td>0.337229</td>\n",
       "      <td>0.243470</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>1.323200</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.218121</td>\n",
       "      <td>0.505330</td>\n",
       "      <td>0.623766</td>\n",
       "      <td>0.116403</td>\n",
       "      <td>0.393437</td>\n",
       "      <td>0.295295</td>\n",
       "      <td>0.337728</td>\n",
       "      <td>0.245066</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1521</td>\n",
       "      <td>1.323200</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.218910</td>\n",
       "      <td>0.500790</td>\n",
       "      <td>0.627319</td>\n",
       "      <td>0.116305</td>\n",
       "      <td>0.395835</td>\n",
       "      <td>0.296322</td>\n",
       "      <td>0.338302</td>\n",
       "      <td>0.245284</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>1.221200</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.221082</td>\n",
       "      <td>0.504540</td>\n",
       "      <td>0.624753</td>\n",
       "      <td>0.116463</td>\n",
       "      <td>0.393338</td>\n",
       "      <td>0.296374</td>\n",
       "      <td>0.339522</td>\n",
       "      <td>0.246323</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2500</td>\n",
       "      <td>1.191500</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.217923</td>\n",
       "      <td>0.505922</td>\n",
       "      <td>0.628504</td>\n",
       "      <td>0.117608</td>\n",
       "      <td>0.397122</td>\n",
       "      <td>0.297175</td>\n",
       "      <td>0.338689</td>\n",
       "      <td>0.245888</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>1.175600</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.219503</td>\n",
       "      <td>0.507501</td>\n",
       "      <td>0.629688</td>\n",
       "      <td>0.117075</td>\n",
       "      <td>0.396334</td>\n",
       "      <td>0.297338</td>\n",
       "      <td>0.339785</td>\n",
       "      <td>0.247057</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3042</td>\n",
       "      <td>1.175600</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.218516</td>\n",
       "      <td>0.506711</td>\n",
       "      <td>0.627122</td>\n",
       "      <td>0.117252</td>\n",
       "      <td>0.395736</td>\n",
       "      <td>0.296986</td>\n",
       "      <td>0.338839</td>\n",
       "      <td>0.246852</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3500</td>\n",
       "      <td>1.101900</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.220095</td>\n",
       "      <td>0.507896</td>\n",
       "      <td>0.628306</td>\n",
       "      <td>0.117193</td>\n",
       "      <td>0.396476</td>\n",
       "      <td>0.297298</td>\n",
       "      <td>0.339459</td>\n",
       "      <td>0.247037</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4000</td>\n",
       "      <td>1.108400</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.218713</td>\n",
       "      <td>0.506119</td>\n",
       "      <td>0.630872</td>\n",
       "      <td>0.117233</td>\n",
       "      <td>0.396516</td>\n",
       "      <td>0.296978</td>\n",
       "      <td>0.339449</td>\n",
       "      <td>0.246392</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4500</td>\n",
       "      <td>1.101900</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.220292</td>\n",
       "      <td>0.508291</td>\n",
       "      <td>0.629491</td>\n",
       "      <td>0.117035</td>\n",
       "      <td>0.396242</td>\n",
       "      <td>0.297447</td>\n",
       "      <td>0.340507</td>\n",
       "      <td>0.247273</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4563</td>\n",
       "      <td>1.101900</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.220095</td>\n",
       "      <td>0.507896</td>\n",
       "      <td>0.630280</td>\n",
       "      <td>0.117233</td>\n",
       "      <td>0.396864</td>\n",
       "      <td>0.297648</td>\n",
       "      <td>0.340432</td>\n",
       "      <td>0.247247</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluasi Model Finetuned (Best Checkpoint)...\n",
      " [Fine] Hit@10: 0.5380 | MRR@10: 0.3260\n",
      "\n",
      "\n",
      "============================== HASIL AKHIR LENGKAP ==============================\n",
      "              Model        Type  Hit@1  Hit@5  Hit@10  MRR@10  NDCG@10  \\\n",
      "0  all-MiniLM-L6-v2    Baseline 0.2148 0.4151  0.4962  0.2998   0.2154   \n",
      "1  all-MiniLM-L6-v2  Fine-Tuned 0.2348 0.4489  0.5380  0.3260   0.2436   \n",
      "\n",
      "   Precision@10  Recall@10  \n",
      "0        0.0963     0.2462  \n",
      "1        0.1106     0.2825  \n",
      "\n",
      "✅ Hasil lengkap tersimpan di: hasil_retrieval_minilm_triplets_cleaned.csv\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import os\n",
    "import pandas as pd\n",
    "from sentence_transformers import SentenceTransformer, InputExample, losses, models, evaluation\n",
    "from torch.utils.data import DataLoader\n",
    "import torch\n",
    "import gc\n",
    "\n",
    "# --- CONFIG ---\n",
    "DATA_DIR = \"final_dataset_retrieval_cleaned\" \n",
    "OUTPUT_DIR = \"models-retrieval-minilm-triplets\"\n",
    "\n",
    "# File Training (Gunakan yang Triplets/Random Negatives)\n",
    "TRAIN_FILE = \"train_triplets_random.json\" \n",
    "\n",
    "# MiniLM ringan, bisa pakai Batch Size besar\n",
    "BATCH_SIZE = 64  \n",
    "EPOCHS = 3       \n",
    "\n",
    "MODELS_TO_TEST = [\n",
    "    \"sentence-transformers/all-MiniLM-L6-v2\"\n",
    "]\n",
    "\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "# --- 1. LOAD DATA ---\n",
    "def load_json(filename):\n",
    "    print(f\"Loading {filename}...\")\n",
    "    path = os.path.join(DATA_DIR, filename)\n",
    "    with open(path, 'r', encoding='utf-8') as f:\n",
    "        return json.load(f)\n",
    "\n",
    "# Load Data Training (Triplets)\n",
    "train_raw = load_json(TRAIN_FILE)\n",
    "\n",
    "# Load Data Validasi & Test\n",
    "val_queries_raw = load_json(\"val_queries.json\")\n",
    "corpus_val_raw = load_json(\"corpus_val.json\")\n",
    "\n",
    "test_queries_raw = load_json(\"test_queries.json\")\n",
    "corpus_test_raw = load_json(\"corpus_test.json\")\n",
    "\n",
    "# --- 2. SETUP EVALUATOR ---\n",
    "def create_evaluator(queries_raw, corpus_raw, name_prefix):\n",
    "    print(f\"Menyiapkan Evaluator: {name_prefix.upper()}...\")\n",
    "    corpus_dict = {item['id']: item['text'] for item in corpus_raw}\n",
    "    text_to_id_map = {item['text']: item['id'] for item in corpus_raw}\n",
    "    \n",
    "    queries_dict = {}\n",
    "    relevant_docs = {}\n",
    "\n",
    "    for i, item in enumerate(queries_raw):\n",
    "        qid = f\"{name_prefix}_q_{i}\"\n",
    "        queries_dict[qid] = item['query']\n",
    "        ground_truth_ids = set()\n",
    "        for gt_text in item['ground_truths']:\n",
    "            found_id = text_to_id_map.get(gt_text)\n",
    "            if found_id:\n",
    "                ground_truth_ids.add(found_id)\n",
    "        if ground_truth_ids:\n",
    "            relevant_docs[qid] = ground_truth_ids\n",
    "            \n",
    "    return evaluation.InformationRetrievalEvaluator(\n",
    "        queries_dict, corpus_dict, relevant_docs,\n",
    "        show_progress_bar=False, name=name_prefix,\n",
    "        mrr_at_k=[10], ndcg_at_k=[10], accuracy_at_k=[1, 5, 10], precision_recall_at_k=[10]\n",
    "    )\n",
    "\n",
    "val_evaluator = create_evaluator(val_queries_raw, corpus_val_raw, \"val\")\n",
    "test_evaluator = create_evaluator(test_queries_raw, corpus_test_raw, \"test\")\n",
    "\n",
    "# --- 3. METRIK ---\n",
    "results_table = []\n",
    "\n",
    "def extract_metrics(metrics_dict, model_name, run_type):\n",
    "    prefix = \"test_\" if \"Baseline\" in run_type or \"Fine-Tuned\" in run_type else \"val_\"\n",
    "    return {\n",
    "        \"Model\": model_name,\n",
    "        \"Type\": run_type,\n",
    "        \"Hit@1\": metrics_dict.get(f'{prefix}cosine_accuracy@1', 0),\n",
    "        \"Hit@5\": metrics_dict.get(f'{prefix}cosine_accuracy@5', 0),\n",
    "        \"Hit@10\": metrics_dict.get(f'{prefix}cosine_accuracy@10', 0),\n",
    "        \"MRR@10\": metrics_dict.get(f'{prefix}cosine_mrr@10', 0),\n",
    "        \"NDCG@10\": metrics_dict.get(f'{prefix}cosine_ndcg@10', 0),\n",
    "        \"Precision@10\": metrics_dict.get(f'{prefix}cosine_precision@10', 0),\n",
    "        \"Recall@10\": metrics_dict.get(f'{prefix}cosine_recall@10', 0)\n",
    "    }\n",
    "\n",
    "# --- 4. EXPERIMENT LOOP ---\n",
    "def run_experiment(model_name):\n",
    "    short_name = model_name.split(\"/\")[-1]\n",
    "    print(f\"\\n{'='*60}\\nEXPERIMENT: {short_name} (Triplets)\\n{'='*60}\")\n",
    "\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "    # A. BASELINE\n",
    "    print(f\"Evaluasi Baseline...\")\n",
    "    model_base = SentenceTransformer(model_name)\n",
    "    metrics_base = test_evaluator(model_base)\n",
    "    res_base = extract_metrics(metrics_base, short_name, \"Baseline\")\n",
    "    results_table.append(res_base)\n",
    "    print(f\" [Base] Hit@10: {res_base['Hit@10']:.4f} | MRR@10: {res_base['MRR@10']:.4f}\")\n",
    "    \n",
    "    del model_base\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "    # B. FINE-TUNING\n",
    "    print(f\"Training (Fine-tuning with Triplets)...\")\n",
    "    \n",
    "    # Init Model (Max Length 512 agar baca abstrak full)\n",
    "    # MiniLM kuat handle 512 token\n",
    "    word_emb = models.Transformer(model_name, max_seq_length=216)\n",
    "    pooling = models.Pooling(word_emb.get_word_embedding_dimension())\n",
    "    model_ft = SentenceTransformer(modules=[word_emb, pooling])\n",
    "    \n",
    "    # --- PERUBAHAN UTAMA: FORMAT DATA TRIPLET ---\n",
    "    train_ex = []\n",
    "    for d in train_raw:\n",
    "        # Kita masukkan 3 kolom: [Anchor, Positive, Negative]\n",
    "        train_ex.append(InputExample(texts=[d['anchor'], d['positive'], d['negative']]))\n",
    "        \n",
    "    train_dl = DataLoader(train_ex, shuffle=True, batch_size=BATCH_SIZE)\n",
    "    \n",
    "    # Loss Function otomatis tahu kalau inputnya 3 kolom, dia pakai Triplet Logic\n",
    "    train_loss = losses.MultipleNegativesRankingLoss(model_ft)\n",
    "    \n",
    "    save_path = os.path.join(OUTPUT_DIR, f\"{short_name}-finetuned\")\n",
    "    \n",
    "    model_ft.fit(\n",
    "        train_objectives=[(train_dl, train_loss)],\n",
    "        epochs=EPOCHS,\n",
    "        warmup_steps=int(len(train_dl) * 0.1),\n",
    "        optimizer_params={'lr': 2e-5},\n",
    "        use_amp=True,\n",
    "        show_progress_bar=True,\n",
    "        evaluator=val_evaluator,\n",
    "        evaluation_steps=500,\n",
    "        save_best_model=True,\n",
    "        output_path=save_path\n",
    "    )\n",
    "    \n",
    "    # C. EVALUASI\n",
    "    print(f\"Evaluasi Model Finetuned (Best Checkpoint)...\")\n",
    "    \n",
    "    del model_ft\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "    \n",
    "    best_model = SentenceTransformer(save_path)\n",
    "    metrics_ft = test_evaluator(best_model)\n",
    "    res_ft = extract_metrics(metrics_ft, short_name, \"Fine-Tuned\")\n",
    "    results_table.append(res_ft)\n",
    "    print(f\" [Fine] Hit@10: {res_ft['Hit@10']:.4f} | MRR@10: {res_ft['MRR@10']:.4f}\")\n",
    "\n",
    "    del best_model\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "# EKSEKUSI\n",
    "for model in MODELS_TO_TEST:\n",
    "    run_experiment(model)\n",
    "\n",
    "# HASIL AKHIR\n",
    "print(\"\\n\\n\" + \"=\"*30 + \" HASIL AKHIR LENGKAP \" + \"=\"*30)\n",
    "df = pd.DataFrame(results_table)\n",
    "cols = [\"Model\", \"Type\", \"Hit@1\", \"Hit@5\", \"Hit@10\", \"MRR@10\", \"NDCG@10\", \"Precision@10\", \"Recall@10\"]\n",
    "df = df[cols]\n",
    "\n",
    "pd.options.display.float_format = '{:,.4f}'.format\n",
    "print(df)\n",
    "\n",
    "csv_path = \"hasil_retrieval_minilm_triplets_cleaned.csv\"\n",
    "df.to_csv(csv_path, index=False)\n",
    "print(f\"\\n✅ Hasil lengkap tersimpan di: {csv_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49c29dbf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Iskandar\\Documents\\GitHub\\Tubes-NLP-PaperMatch\\ret_env\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading train_triplets_random.json...\n",
      "Loading val_queries.json...\n",
      "Loading corpus_val.json...\n",
      "Loading test_queries.json...\n",
      "Loading corpus_test.json...\n",
      "Menyiapkan Evaluator: VAL...\n",
      "Menyiapkan Evaluator: TEST...\n",
      "\n",
      "============================================================\n",
      "EXPERIMENT: all-mpnet-base-v2 (Triplets)\n",
      "============================================================\n",
      "Evaluasi Baseline...\n",
      " [Base] Hit@10: 0.5087 | MRR@10: 0.3092\n",
      "Training (Fine-tuning with Triplets)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                     \r"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='6081' max='6081' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [6081/6081 17:03:44, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Val Cosine Accuracy@1</th>\n",
       "      <th>Val Cosine Accuracy@5</th>\n",
       "      <th>Val Cosine Accuracy@10</th>\n",
       "      <th>Val Cosine Precision@10</th>\n",
       "      <th>Val Cosine Recall@10</th>\n",
       "      <th>Val Cosine Ndcg@10</th>\n",
       "      <th>Val Cosine Mrr@10</th>\n",
       "      <th>Val Cosine Map@100</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.716500</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.224240</td>\n",
       "      <td>0.493486</td>\n",
       "      <td>0.623569</td>\n",
       "      <td>0.116088</td>\n",
       "      <td>0.398826</td>\n",
       "      <td>0.298515</td>\n",
       "      <td>0.339568</td>\n",
       "      <td>0.247492</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.705900</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.220095</td>\n",
       "      <td>0.495263</td>\n",
       "      <td>0.626925</td>\n",
       "      <td>0.117035</td>\n",
       "      <td>0.397223</td>\n",
       "      <td>0.296280</td>\n",
       "      <td>0.336403</td>\n",
       "      <td>0.245550</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>0.671900</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.228977</td>\n",
       "      <td>0.502566</td>\n",
       "      <td>0.630478</td>\n",
       "      <td>0.119029</td>\n",
       "      <td>0.404040</td>\n",
       "      <td>0.303910</td>\n",
       "      <td>0.344741</td>\n",
       "      <td>0.252438</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>0.611600</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.224635</td>\n",
       "      <td>0.505724</td>\n",
       "      <td>0.634228</td>\n",
       "      <td>0.119424</td>\n",
       "      <td>0.406716</td>\n",
       "      <td>0.302966</td>\n",
       "      <td>0.343942</td>\n",
       "      <td>0.250688</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2500</td>\n",
       "      <td>0.614600</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.223450</td>\n",
       "      <td>0.508685</td>\n",
       "      <td>0.632057</td>\n",
       "      <td>0.119049</td>\n",
       "      <td>0.406127</td>\n",
       "      <td>0.302871</td>\n",
       "      <td>0.343312</td>\n",
       "      <td>0.251267</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>0.576900</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.223648</td>\n",
       "      <td>0.501974</td>\n",
       "      <td>0.628899</td>\n",
       "      <td>0.118358</td>\n",
       "      <td>0.404006</td>\n",
       "      <td>0.301631</td>\n",
       "      <td>0.342080</td>\n",
       "      <td>0.250825</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3500</td>\n",
       "      <td>0.568800</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.226411</td>\n",
       "      <td>0.514607</td>\n",
       "      <td>0.633439</td>\n",
       "      <td>0.120549</td>\n",
       "      <td>0.406185</td>\n",
       "      <td>0.304729</td>\n",
       "      <td>0.345533</td>\n",
       "      <td>0.253602</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4000</td>\n",
       "      <td>0.555500</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.224437</td>\n",
       "      <td>0.501382</td>\n",
       "      <td>0.637386</td>\n",
       "      <td>0.120312</td>\n",
       "      <td>0.409071</td>\n",
       "      <td>0.303789</td>\n",
       "      <td>0.342345</td>\n",
       "      <td>0.251953</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4500</td>\n",
       "      <td>0.533500</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.230557</td>\n",
       "      <td>0.509080</td>\n",
       "      <td>0.639360</td>\n",
       "      <td>0.120529</td>\n",
       "      <td>0.410707</td>\n",
       "      <td>0.307850</td>\n",
       "      <td>0.348747</td>\n",
       "      <td>0.257280</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5000</td>\n",
       "      <td>0.539800</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.229175</td>\n",
       "      <td>0.507698</td>\n",
       "      <td>0.638571</td>\n",
       "      <td>0.120707</td>\n",
       "      <td>0.412128</td>\n",
       "      <td>0.307363</td>\n",
       "      <td>0.348229</td>\n",
       "      <td>0.256020</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5500</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.228780</td>\n",
       "      <td>0.511646</td>\n",
       "      <td>0.641532</td>\n",
       "      <td>0.121477</td>\n",
       "      <td>0.413369</td>\n",
       "      <td>0.308986</td>\n",
       "      <td>0.348516</td>\n",
       "      <td>0.257891</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6000</td>\n",
       "      <td>0.503200</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.226609</td>\n",
       "      <td>0.510659</td>\n",
       "      <td>0.640545</td>\n",
       "      <td>0.121200</td>\n",
       "      <td>0.412813</td>\n",
       "      <td>0.307426</td>\n",
       "      <td>0.347315</td>\n",
       "      <td>0.256042</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6081</td>\n",
       "      <td>0.503200</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.227004</td>\n",
       "      <td>0.511251</td>\n",
       "      <td>0.641927</td>\n",
       "      <td>0.121279</td>\n",
       "      <td>0.412984</td>\n",
       "      <td>0.307670</td>\n",
       "      <td>0.347664</td>\n",
       "      <td>0.256300</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluasi Model Finetuned (Best Checkpoint)...\n",
      " [Fine] Hit@10: 0.5376 | MRR@10: 0.3297\n",
      "\n",
      "\n",
      "============================== HASIL AKHIR LENGKAP ==============================\n",
      "               Model        Type  Hit@1  Hit@5  Hit@10  MRR@10  NDCG@10  \\\n",
      "0  all-mpnet-base-v2    Baseline 0.2229 0.4274  0.5087  0.3092   0.2252   \n",
      "1  all-mpnet-base-v2  Fine-Tuned 0.2374 0.4557  0.5376  0.3297   0.2461   \n",
      "\n",
      "   Precision@10  Recall@10  \n",
      "0        0.1010     0.2581  \n",
      "1        0.1111     0.2839  \n",
      "\n",
      "✅ Hasil lengkap tersimpan di: hasil_retrieval_mpnet_triplets_cleaned.csv\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import os\n",
    "import pandas as pd\n",
    "from sentence_transformers import SentenceTransformer, InputExample, losses, models, evaluation\n",
    "from torch.utils.data import DataLoader\n",
    "import torch\n",
    "import gc\n",
    "\n",
    "# --- CONFIG ---\n",
    "DATA_DIR = \"final_dataset_retrieval_cleaned\" \n",
    "OUTPUT_DIR = \"models-retrieval-minilm-triplets\"\n",
    "\n",
    "# File Training (Gunakan yang Triplets/Random Negatives)\n",
    "TRAIN_FILE = \"train_triplets_random.json\" \n",
    "\n",
    "# MiniLM ringan, bisa pakai Batch Size besar\n",
    "BATCH_SIZE = 16  \n",
    "EPOCHS = 1       \n",
    "\n",
    "MODELS_TO_TEST = [\n",
    "    \"sentence-transformers/all-mpnet-base-v2\"\n",
    "]\n",
    "\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "# --- 1. LOAD DATA ---\n",
    "def load_json(filename):\n",
    "    print(f\"Loading {filename}...\")\n",
    "    path = os.path.join(DATA_DIR, filename)\n",
    "    with open(path, 'r', encoding='utf-8') as f:\n",
    "        return json.load(f)\n",
    "\n",
    "# Load Data Training (Triplets)\n",
    "train_raw = load_json(TRAIN_FILE)\n",
    "\n",
    "# Load Data Validasi & Test\n",
    "val_queries_raw = load_json(\"val_queries.json\")\n",
    "corpus_val_raw = load_json(\"corpus_val.json\")\n",
    "\n",
    "test_queries_raw = load_json(\"test_queries.json\")\n",
    "corpus_test_raw = load_json(\"corpus_test.json\")\n",
    "\n",
    "# --- 2. SETUP EVALUATOR ---\n",
    "def create_evaluator(queries_raw, corpus_raw, name_prefix):\n",
    "    print(f\"Menyiapkan Evaluator: {name_prefix.upper()}...\")\n",
    "    corpus_dict = {item['id']: item['text'] for item in corpus_raw}\n",
    "    text_to_id_map = {item['text']: item['id'] for item in corpus_raw}\n",
    "    \n",
    "    queries_dict = {}\n",
    "    relevant_docs = {}\n",
    "\n",
    "    for i, item in enumerate(queries_raw):\n",
    "        qid = f\"{name_prefix}_q_{i}\"\n",
    "        queries_dict[qid] = item['query']\n",
    "        ground_truth_ids = set()\n",
    "        for gt_text in item['ground_truths']:\n",
    "            found_id = text_to_id_map.get(gt_text)\n",
    "            if found_id:\n",
    "                ground_truth_ids.add(found_id)\n",
    "        if ground_truth_ids:\n",
    "            relevant_docs[qid] = ground_truth_ids\n",
    "            \n",
    "    return evaluation.InformationRetrievalEvaluator(\n",
    "        queries_dict, corpus_dict, relevant_docs,\n",
    "        show_progress_bar=False, name=name_prefix,\n",
    "        mrr_at_k=[10], ndcg_at_k=[10], accuracy_at_k=[1, 5, 10], precision_recall_at_k=[10]\n",
    "    )\n",
    "\n",
    "val_evaluator = create_evaluator(val_queries_raw, corpus_val_raw, \"val\")\n",
    "test_evaluator = create_evaluator(test_queries_raw, corpus_test_raw, \"test\")\n",
    "\n",
    "# --- 3. METRIK ---\n",
    "results_table = []\n",
    "\n",
    "def extract_metrics(metrics_dict, model_name, run_type):\n",
    "    prefix = \"test_\" if \"Baseline\" in run_type or \"Fine-Tuned\" in run_type else \"val_\"\n",
    "    return {\n",
    "        \"Model\": model_name,\n",
    "        \"Type\": run_type,\n",
    "        \"Hit@1\": metrics_dict.get(f'{prefix}cosine_accuracy@1', 0),\n",
    "        \"Hit@5\": metrics_dict.get(f'{prefix}cosine_accuracy@5', 0),\n",
    "        \"Hit@10\": metrics_dict.get(f'{prefix}cosine_accuracy@10', 0),\n",
    "        \"MRR@10\": metrics_dict.get(f'{prefix}cosine_mrr@10', 0),\n",
    "        \"NDCG@10\": metrics_dict.get(f'{prefix}cosine_ndcg@10', 0),\n",
    "        \"Precision@10\": metrics_dict.get(f'{prefix}cosine_precision@10', 0),\n",
    "        \"Recall@10\": metrics_dict.get(f'{prefix}cosine_recall@10', 0)\n",
    "    }\n",
    "\n",
    "# --- 4. EXPERIMENT LOOP ---\n",
    "def run_experiment(model_name):\n",
    "    short_name = model_name.split(\"/\")[-1]\n",
    "    print(f\"\\n{'='*60}\\nEXPERIMENT: {short_name} (Triplets)\\n{'='*60}\")\n",
    "\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "    # A. BASELINE\n",
    "    print(f\"Evaluasi Baseline...\")\n",
    "    model_base = SentenceTransformer(model_name)\n",
    "    metrics_base = test_evaluator(model_base)\n",
    "    res_base = extract_metrics(metrics_base, short_name, \"Baseline\")\n",
    "    results_table.append(res_base)\n",
    "    print(f\" [Base] Hit@10: {res_base['Hit@10']:.4f} | MRR@10: {res_base['MRR@10']:.4f}\")\n",
    "    \n",
    "    del model_base\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "    # B. FINE-TUNING\n",
    "    print(f\"Training (Fine-tuning with Triplets)...\")\n",
    "    \n",
    "    # Init Model (Max Length 512 agar baca abstrak full)\n",
    "    # MiniLM kuat handle 512 token\n",
    "    word_emb = models.Transformer(model_name, max_seq_length=216)\n",
    "    pooling = models.Pooling(word_emb.get_word_embedding_dimension())\n",
    "    model_ft = SentenceTransformer(modules=[word_emb, pooling])\n",
    "    \n",
    "    # --- PERUBAHAN UTAMA: FORMAT DATA TRIPLET ---\n",
    "    train_ex = []\n",
    "    for d in train_raw:\n",
    "        # Kita masukkan 3 kolom: [Anchor, Positive, Negative]\n",
    "        train_ex.append(InputExample(texts=[d['anchor'], d['positive'], d['negative']]))\n",
    "        \n",
    "    train_dl = DataLoader(train_ex, shuffle=True, batch_size=BATCH_SIZE)\n",
    "    \n",
    "    # Loss Function otomatis tahu kalau inputnya 3 kolom, dia pakai Triplet Logic\n",
    "    train_loss = losses.MultipleNegativesRankingLoss(model_ft)\n",
    "    \n",
    "    save_path = os.path.join(OUTPUT_DIR, f\"{short_name}-finetuned\")\n",
    "    \n",
    "    model_ft.fit(\n",
    "        train_objectives=[(train_dl, train_loss)],\n",
    "        epochs=EPOCHS,\n",
    "        warmup_steps=int(len(train_dl) * 0.1),\n",
    "        optimizer_params={'lr': 2e-5},\n",
    "        use_amp=True,\n",
    "        show_progress_bar=True,\n",
    "        evaluator=val_evaluator,\n",
    "        evaluation_steps=500,\n",
    "        save_best_model=True,\n",
    "        output_path=save_path\n",
    "    )\n",
    "    \n",
    "    # C. EVALUASI\n",
    "    print(f\"Evaluasi Model Finetuned (Best Checkpoint)...\")\n",
    "    \n",
    "    del model_ft\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "    \n",
    "    best_model = SentenceTransformer(save_path)\n",
    "    metrics_ft = test_evaluator(best_model)\n",
    "    res_ft = extract_metrics(metrics_ft, short_name, \"Fine-Tuned\")\n",
    "    results_table.append(res_ft)\n",
    "    print(f\" [Fine] Hit@10: {res_ft['Hit@10']:.4f} | MRR@10: {res_ft['MRR@10']:.4f}\")\n",
    "\n",
    "    del best_model\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "# EKSEKUSI\n",
    "for model in MODELS_TO_TEST:\n",
    "    run_experiment(model)\n",
    "\n",
    "# HASIL AKHIR\n",
    "print(\"\\n\\n\" + \"=\"*30 + \" HASIL AKHIR LENGKAP \" + \"=\"*30)\n",
    "df = pd.DataFrame(results_table)\n",
    "cols = [\"Model\", \"Type\", \"Hit@1\", \"Hit@5\", \"Hit@10\", \"MRR@10\", \"NDCG@10\", \"Precision@10\", \"Recall@10\"]\n",
    "df = df[cols]\n",
    "\n",
    "pd.options.display.float_format = '{:,.4f}'.format\n",
    "print(df)\n",
    "\n",
    "csv_path = \"hasil_retrieval_mpnet_triplets_cleaned.csv\"\n",
    "df.to_csv(csv_path, index=False)\n",
    "print(f\"\\n✅ Hasil lengkap tersimpan di: {csv_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6e4195cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Iskandar\\Documents\\GitHub\\Tubes-NLP-PaperMatch\\ret_env\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading train_triplets_random.json...\n",
      "Loading val_queries.json...\n",
      "Loading corpus_val.json...\n",
      "Loading test_queries.json...\n",
      "Loading corpus_test.json...\n",
      "Menyiapkan Evaluator: VAL...\n",
      "Menyiapkan Evaluator: TEST...\n",
      "\n",
      "============================================================\n",
      "EXPERIMENT: snowflake-arctic-embed-xs (Triplets)\n",
      "============================================================\n",
      "Evaluasi Baseline...\n",
      " [Base] Hit@10: 0.4765 | MRR@10: 0.2949\n",
      "Training (Fine-tuning with Triplets)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertModel were not initialized from the model checkpoint at Snowflake/snowflake-arctic-embed-xs and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "                                                                     \r"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='4563' max='4563' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [4563/4563 1:03:50, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Val Cosine Accuracy@1</th>\n",
       "      <th>Val Cosine Accuracy@5</th>\n",
       "      <th>Val Cosine Accuracy@10</th>\n",
       "      <th>Val Cosine Precision@10</th>\n",
       "      <th>Val Cosine Recall@10</th>\n",
       "      <th>Val Cosine Ndcg@10</th>\n",
       "      <th>Val Cosine Mrr@10</th>\n",
       "      <th>Val Cosine Map@100</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>2.042700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>1.650000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>1.554000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1521</td>\n",
       "      <td>1.554000</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.212396</td>\n",
       "      <td>0.474931</td>\n",
       "      <td>0.597710</td>\n",
       "      <td>0.110422</td>\n",
       "      <td>0.371832</td>\n",
       "      <td>0.279819</td>\n",
       "      <td>0.323464</td>\n",
       "      <td>0.229532</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>1.454200</td>\n",
       "      <td>No Log</td>\n",
       "      <td>No Log</td>\n",
       "      <td>No Log</td>\n",
       "      <td>No Log</td>\n",
       "      <td>No Log</td>\n",
       "      <td>No Log</td>\n",
       "      <td>No Log</td>\n",
       "      <td>No Log</td>\n",
       "      <td>No Log</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2500</td>\n",
       "      <td>1.412100</td>\n",
       "      <td>No Log</td>\n",
       "      <td>No Log</td>\n",
       "      <td>No Log</td>\n",
       "      <td>No Log</td>\n",
       "      <td>No Log</td>\n",
       "      <td>No Log</td>\n",
       "      <td>No Log</td>\n",
       "      <td>No Log</td>\n",
       "      <td>No Log</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>1.380000</td>\n",
       "      <td>No Log</td>\n",
       "      <td>No Log</td>\n",
       "      <td>No Log</td>\n",
       "      <td>No Log</td>\n",
       "      <td>No Log</td>\n",
       "      <td>No Log</td>\n",
       "      <td>No Log</td>\n",
       "      <td>No Log</td>\n",
       "      <td>No Log</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3042</td>\n",
       "      <td>1.380000</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.214568</td>\n",
       "      <td>0.480458</td>\n",
       "      <td>0.599487</td>\n",
       "      <td>0.111153</td>\n",
       "      <td>0.373200</td>\n",
       "      <td>0.283003</td>\n",
       "      <td>0.327443</td>\n",
       "      <td>0.234204</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3500</td>\n",
       "      <td>1.323600</td>\n",
       "      <td>No Log</td>\n",
       "      <td>No Log</td>\n",
       "      <td>No Log</td>\n",
       "      <td>No Log</td>\n",
       "      <td>No Log</td>\n",
       "      <td>No Log</td>\n",
       "      <td>No Log</td>\n",
       "      <td>No Log</td>\n",
       "      <td>No Log</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4000</td>\n",
       "      <td>1.311700</td>\n",
       "      <td>No Log</td>\n",
       "      <td>No Log</td>\n",
       "      <td>No Log</td>\n",
       "      <td>No Log</td>\n",
       "      <td>No Log</td>\n",
       "      <td>No Log</td>\n",
       "      <td>No Log</td>\n",
       "      <td>No Log</td>\n",
       "      <td>No Log</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4500</td>\n",
       "      <td>1.302000</td>\n",
       "      <td>No Log</td>\n",
       "      <td>No Log</td>\n",
       "      <td>No Log</td>\n",
       "      <td>No Log</td>\n",
       "      <td>No Log</td>\n",
       "      <td>No Log</td>\n",
       "      <td>No Log</td>\n",
       "      <td>No Log</td>\n",
       "      <td>No Log</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4563</td>\n",
       "      <td>1.302000</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.214568</td>\n",
       "      <td>0.484011</td>\n",
       "      <td>0.599092</td>\n",
       "      <td>0.111212</td>\n",
       "      <td>0.372595</td>\n",
       "      <td>0.282360</td>\n",
       "      <td>0.326996</td>\n",
       "      <td>0.233793</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluasi Model Finetuned (Best Checkpoint)...\n",
      " [Fine] Hit@10: 0.5299 | MRR@10: 0.3215\n",
      "\n",
      "\n",
      "============================== HASIL AKHIR LENGKAP ==============================\n",
      "                       Model        Type  Hit@1  Hit@5  Hit@10  MRR@10  \\\n",
      "0  snowflake-arctic-embed-xs    Baseline 0.2158 0.4039  0.4765  0.2949   \n",
      "1  snowflake-arctic-embed-xs  Fine-Tuned 0.2303 0.4443  0.5299  0.3215   \n",
      "\n",
      "   NDCG@10  Precision@10  Recall@10  \n",
      "0   0.2066        0.0899     0.2301  \n",
      "1   0.2390        0.1075     0.2748  \n",
      "\n",
      "✅ Hasil lengkap tersimpan di: hasil_retrieval_snowflake_triplets_cleaned.csv\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import os\n",
    "import pandas as pd\n",
    "from sentence_transformers import SentenceTransformer, InputExample, losses, models, evaluation\n",
    "from torch.utils.data import DataLoader\n",
    "import torch\n",
    "import gc\n",
    "\n",
    "# --- CONFIG ---\n",
    "DATA_DIR = \"final_dataset_retrieval_cleaned\" \n",
    "OUTPUT_DIR = \"models-retrieval-minilm-triplets\"\n",
    "\n",
    "# File Training (Gunakan yang Triplets/Random Negatives)\n",
    "TRAIN_FILE = \"train_triplets_random.json\" \n",
    "\n",
    "# MiniLM ringan, bisa pakai Batch Size besar\n",
    "BATCH_SIZE = 64  \n",
    "EPOCHS = 3       \n",
    "\n",
    "MODELS_TO_TEST = [\n",
    "    \"Snowflake/snowflake-arctic-embed-xs\"\n",
    "]\n",
    "\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "# --- 1. LOAD DATA ---\n",
    "def load_json(filename):\n",
    "    print(f\"Loading {filename}...\")\n",
    "    path = os.path.join(DATA_DIR, filename)\n",
    "    with open(path, 'r', encoding='utf-8') as f:\n",
    "        return json.load(f)\n",
    "\n",
    "# Load Data Training (Triplets)\n",
    "train_raw = load_json(TRAIN_FILE)\n",
    "\n",
    "# Load Data Validasi & Test\n",
    "val_queries_raw = load_json(\"val_queries.json\")\n",
    "corpus_val_raw = load_json(\"corpus_val.json\")\n",
    "\n",
    "test_queries_raw = load_json(\"test_queries.json\")\n",
    "corpus_test_raw = load_json(\"corpus_test.json\")\n",
    "\n",
    "# --- 2. SETUP EVALUATOR ---\n",
    "def create_evaluator(queries_raw, corpus_raw, name_prefix):\n",
    "    print(f\"Menyiapkan Evaluator: {name_prefix.upper()}...\")\n",
    "    corpus_dict = {item['id']: item['text'] for item in corpus_raw}\n",
    "    text_to_id_map = {item['text']: item['id'] for item in corpus_raw}\n",
    "    \n",
    "    queries_dict = {}\n",
    "    relevant_docs = {}\n",
    "\n",
    "    for i, item in enumerate(queries_raw):\n",
    "        qid = f\"{name_prefix}_q_{i}\"\n",
    "        queries_dict[qid] = item['query']\n",
    "        ground_truth_ids = set()\n",
    "        for gt_text in item['ground_truths']:\n",
    "            found_id = text_to_id_map.get(gt_text)\n",
    "            if found_id:\n",
    "                ground_truth_ids.add(found_id)\n",
    "        if ground_truth_ids:\n",
    "            relevant_docs[qid] = ground_truth_ids\n",
    "            \n",
    "    return evaluation.InformationRetrievalEvaluator(\n",
    "        queries_dict, corpus_dict, relevant_docs,\n",
    "        show_progress_bar=False, name=name_prefix,\n",
    "        mrr_at_k=[10], ndcg_at_k=[10], accuracy_at_k=[1, 5, 10], precision_recall_at_k=[10]\n",
    "    )\n",
    "\n",
    "val_evaluator = create_evaluator(val_queries_raw, corpus_val_raw, \"val\")\n",
    "test_evaluator = create_evaluator(test_queries_raw, corpus_test_raw, \"test\")\n",
    "\n",
    "# --- 3. METRIK ---\n",
    "results_table = []\n",
    "\n",
    "def extract_metrics(metrics_dict, model_name, run_type):\n",
    "    prefix = \"test_\" if \"Baseline\" in run_type or \"Fine-Tuned\" in run_type else \"val_\"\n",
    "    return {\n",
    "        \"Model\": model_name,\n",
    "        \"Type\": run_type,\n",
    "        \"Hit@1\": metrics_dict.get(f'{prefix}cosine_accuracy@1', 0),\n",
    "        \"Hit@5\": metrics_dict.get(f'{prefix}cosine_accuracy@5', 0),\n",
    "        \"Hit@10\": metrics_dict.get(f'{prefix}cosine_accuracy@10', 0),\n",
    "        \"MRR@10\": metrics_dict.get(f'{prefix}cosine_mrr@10', 0),\n",
    "        \"NDCG@10\": metrics_dict.get(f'{prefix}cosine_ndcg@10', 0),\n",
    "        \"Precision@10\": metrics_dict.get(f'{prefix}cosine_precision@10', 0),\n",
    "        \"Recall@10\": metrics_dict.get(f'{prefix}cosine_recall@10', 0)\n",
    "    }\n",
    "\n",
    "# --- 4. EXPERIMENT LOOP ---\n",
    "def run_experiment(model_name):\n",
    "    short_name = model_name.split(\"/\")[-1]\n",
    "    print(f\"\\n{'='*60}\\nEXPERIMENT: {short_name} (Triplets)\\n{'='*60}\")\n",
    "\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "    # A. BASELINE\n",
    "    print(f\"Evaluasi Baseline...\")\n",
    "    model_base = SentenceTransformer(model_name)\n",
    "    metrics_base = test_evaluator(model_base)\n",
    "    res_base = extract_metrics(metrics_base, short_name, \"Baseline\")\n",
    "    results_table.append(res_base)\n",
    "    print(f\" [Base] Hit@10: {res_base['Hit@10']:.4f} | MRR@10: {res_base['MRR@10']:.4f}\")\n",
    "    \n",
    "    del model_base\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "    # B. FINE-TUNING\n",
    "    print(f\"Training (Fine-tuning with Triplets)...\")\n",
    "    \n",
    "    # Init Model (Max Length 512 agar baca abstrak full)\n",
    "    # MiniLM kuat handle 512 token\n",
    "    word_emb = models.Transformer(model_name, max_seq_length=216)\n",
    "    pooling = models.Pooling(word_emb.get_word_embedding_dimension())\n",
    "    model_ft = SentenceTransformer(modules=[word_emb, pooling])\n",
    "    \n",
    "    # --- PERUBAHAN UTAMA: FORMAT DATA TRIPLET ---\n",
    "    train_ex = []\n",
    "    for d in train_raw:\n",
    "        # Kita masukkan 3 kolom: [Anchor, Positive, Negative]\n",
    "        train_ex.append(InputExample(texts=[d['anchor'], d['positive'], d['negative']]))\n",
    "        \n",
    "    train_dl = DataLoader(train_ex, shuffle=True, batch_size=BATCH_SIZE)\n",
    "    \n",
    "    # Loss Function otomatis tahu kalau inputnya 3 kolom, dia pakai Triplet Logic\n",
    "    train_loss = losses.MultipleNegativesRankingLoss(model_ft)\n",
    "    \n",
    "    save_path = os.path.join(OUTPUT_DIR, f\"{short_name}-finetuned\")\n",
    "    \n",
    "    model_ft.fit(\n",
    "        train_objectives=[(train_dl, train_loss)],\n",
    "        epochs=EPOCHS,\n",
    "        warmup_steps=int(len(train_dl) * 0.1),\n",
    "        optimizer_params={'lr': 2e-5},\n",
    "        use_amp=True,\n",
    "        show_progress_bar=True,\n",
    "        evaluator=val_evaluator,\n",
    "        save_best_model=True,\n",
    "        output_path=save_path\n",
    "    )\n",
    "    \n",
    "    # C. EVALUASI\n",
    "    print(f\"Evaluasi Model Finetuned (Best Checkpoint)...\")\n",
    "    \n",
    "    del model_ft\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "    \n",
    "    best_model = SentenceTransformer(save_path)\n",
    "    metrics_ft = test_evaluator(best_model)\n",
    "    res_ft = extract_metrics(metrics_ft, short_name, \"Fine-Tuned\")\n",
    "    results_table.append(res_ft)\n",
    "    print(f\" [Fine] Hit@10: {res_ft['Hit@10']:.4f} | MRR@10: {res_ft['MRR@10']:.4f}\")\n",
    "\n",
    "    del best_model\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "# EKSEKUSI\n",
    "for model in MODELS_TO_TEST:\n",
    "    run_experiment(model)\n",
    "\n",
    "# HASIL AKHIR\n",
    "print(\"\\n\\n\" + \"=\"*30 + \" HASIL AKHIR LENGKAP \" + \"=\"*30)\n",
    "df = pd.DataFrame(results_table)\n",
    "cols = [\"Model\", \"Type\", \"Hit@1\", \"Hit@5\", \"Hit@10\", \"MRR@10\", \"NDCG@10\", \"Precision@10\", \"Recall@10\"]\n",
    "df = df[cols]\n",
    "\n",
    "pd.options.display.float_format = '{:,.4f}'.format\n",
    "print(df)\n",
    "\n",
    "csv_path = \"hasil_retrieval_snowflake_triplets_cleaned.csv\"\n",
    "df.to_csv(csv_path, index=False)\n",
    "print(f\"\\n✅ Hasil lengkap tersimpan di: {csv_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ea9f9d9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ret_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
