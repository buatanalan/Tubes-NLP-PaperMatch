{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "aa251e49",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Iskandar\\Documents\\GitHub\\Tubes-NLP-PaperMatch\\.venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memuat dataset dari processed_multixscience_data...\n",
      "‚úÖ Dataset berhasil dimuat!\n",
      "Dataset({\n",
      "    features: ['input_text', 'related_work'],\n",
      "    num_rows: 30369\n",
      "})\n",
      "\n",
      "Contoh Kolom Dataset: ['input_text', 'related_work']\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from datasets import load_from_disk\n",
    "\n",
    "data_path = \"processed_multixscience_data\"\n",
    "\n",
    "print(f\"Memuat dataset dari {data_path}...\")\n",
    "try:\n",
    "    tokenized_datasets = load_from_disk(data_path)\n",
    "    print(\"‚úÖ Dataset berhasil dimuat!\")\n",
    "    print(tokenized_datasets)\n",
    "except FileNotFoundError:\n",
    "    print(\"‚ùå Error: Path salah. Cek kembali lokasi folder di panel 'Input' sebelah kanan.\")\n",
    "\n",
    "print(\"\\nContoh Kolom Dataset:\", tokenized_datasets.column_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c61563a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# EOS_TOKEN biasanya diperlukan agar model tahu kapan harus berhenti menulis\n",
    "# Jika pakai Unsloth, biasanya sudah otomatis, tapi definisikan manual untuk aman.\n",
    "EOS_TOKEN = \"<|eot_id|>\"\n",
    "\n",
    "def format_prompt_llama3(examples):\n",
    "    # Ambil list data dari batch\n",
    "    inputs = examples[\"input_text\"]       # Context (Abstract + Refs)\n",
    "    outputs = examples[\"related_work\"]    # Ground Truth (Target)\n",
    "\n",
    "    prompts = []\n",
    "\n",
    "    # System Prompt: Instruksi peran untuk AI\n",
    "    system_msg = (\n",
    "        \"You are an academic writing assistant. \"\n",
    "        \"Write a 'Related Work' section based on the provided text. \"\n",
    "        \"The input contains the Current Abstract followed by References (marked with @cite_n). \"\n",
    "        \"Synthesize these references and highlight the novelty of the Current Abstract.\"\n",
    "    )\n",
    "\n",
    "    for input_text, output_text in zip(inputs, outputs):\n",
    "        # Struktur Llama 3 Instruct Resmi\n",
    "        text = f\"\"\"<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
    "\n",
    "{system_msg}<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
    "\n",
    "{input_text}<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
    "\n",
    "{output_text}{EOS_TOKEN}\"\"\"\n",
    "\n",
    "        prompts.append(text)\n",
    "\n",
    "    # Kembalikan dalam kolom baru bernama 'text' (biasanya SFTTrainer mencari kolom ini)\n",
    "    return { \"text\": prompts }\n",
    "\n",
    "# --- CARA PAKAI ---\n",
    "# Asumsi 'tokenized_datasets' adalah HuggingFace Dataset yang sudah Anda load\n",
    "# Terapkan fungsi format_prompt_llama3 ke seluruh dataset\n",
    "formatted_datasets = tokenized_datasets.map(format_prompt_llama3, batched=True)\n",
    "# print(formatted_datasets['text'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1a81689b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch: 2.5.1+cu121, Device: cuda\n",
      "Sedang meload model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Iskandar\\Documents\\GitHub\\Tubes-NLP-PaperMatch\\.venv\\Lib\\site-packages\\transformers\\quantizers\\auto.py:239: UserWarning: You passed `quantization_config` or equivalent parameters to `from_pretrained` but the model you're loading already has a `quantization_config` attribute. The `quantization_config` from the model will be used.\n",
      "  warnings.warn(warning_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "‚úÖ Model Llama 3 (Windows Native) siap! Berikut parameter yang akan dilatih:\n",
      "trainable params: 1,703,936 || all params: 1,237,518,336 || trainable%: 0.1377\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import (\n",
    "    AutoTokenizer, \n",
    "    AutoModelForCausalLM, \n",
    "    BitsAndBytesConfig\n",
    ")\n",
    "from peft import (\n",
    "    LoraConfig, \n",
    "    get_peft_model, \n",
    "    prepare_model_for_kbit_training\n",
    ")\n",
    "\n",
    "# 1. Cek Device\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"PyTorch: {torch.__version__}, Device: {device}\")\n",
    "\n",
    "# 2. Konfigurasi Model & Quantization (Pengganti load_in_4bit=True di Unsloth)\n",
    "model_id = \"unsloth/Llama-3.2-1B-Instruct-bnb-4bit\" # Kita tetap pakai model ini karena sudah optimized\n",
    "\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,           # Aktifkan 4-bit loading\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",   # Tipe quantisasi standar Llama\n",
    "    bnb_4bit_compute_dtype=torch.float16, # Gunakan float16 untuk komputasi di GPU\n",
    ")\n",
    "\n",
    "print(\"Sedang meload model...\")\n",
    "\n",
    "# 3. Load Model Base\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_id,\n",
    "    quantization_config=bnb_config,\n",
    "    device_map=\"auto\",           # Otomatis sebar ke GPU\n",
    "    use_cache=False              # False saat training untuk hemat VRAM\n",
    ")\n",
    "\n",
    "# 4. Load Tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "tokenizer.pad_token = tokenizer.eos_token # Fix untuk Llama 3\n",
    "tokenizer.padding_side = \"right\"\n",
    "\n",
    "# 5. Persiapan Model untuk Training 4-bit\n",
    "model = prepare_model_for_kbit_training(model)\n",
    "\n",
    "# 6. Konfigurasi LoRA (Pengganti FastLanguageModel.get_peft_model)\n",
    "peft_config = LoraConfig(\n",
    "    r=16,                        # Rank LoRA (sama seperti kode Anda)\n",
    "    lora_alpha=16,\n",
    "    target_modules=[\"q_proj\", \"v_proj\"], # Bisa ditambah: \"k_proj\", \"o_proj\", \"gate_proj\", \"up_proj\", \"down_proj\"\n",
    "    lora_dropout=0.05,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\",\n",
    ")\n",
    "\n",
    "# 7. Pasang Adapter LoRA ke Model\n",
    "model = get_peft_model(model, peft_config)\n",
    "\n",
    "print(\"\\n‚úÖ Model Llama 3 (Windows Native) siap! Berikut parameter yang akan dilatih:\")\n",
    "model.print_trainable_parameters()\n",
    "\n",
    "# --- BATAS KODE INISIALISASI ---\n",
    "# Di bawah ini nanti Anda bisa lanjut ke kode Training (SFTTrainer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "be27b96d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚è≥ Sedang men-tokenisasi dataset...\n",
      "‚úÖ Tokenisasi Selesai!\n",
      "Contoh data: dict_keys(['input_ids', 'attention_mask', 'labels'])\n"
     ]
    }
   ],
   "source": [
    "from transformers import DataCollatorForLanguageModeling\n",
    "\n",
    "# 1. Pastikan Tokenizer sudah diset padding-nya (PENTING buat Llama 3)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.padding_side = \"right\" # Fix untuk fp16\n",
    "\n",
    "# 2. Fungsi Mapping\n",
    "def tokenize_function(examples):\n",
    "    # Ambil teks dari kolom \"text\"\n",
    "    outputs = tokenizer(\n",
    "        examples[\"text\"],\n",
    "        truncation=True,\n",
    "        max_length=1024, # Sesuaikan panjang konteks\n",
    "        padding=False    # Kita padding nanti di DataCollator biar hemat memori\n",
    "    )\n",
    "    # Untuk Causal LM (text generation), labels = input_ids\n",
    "    outputs[\"labels\"] = outputs[\"input_ids\"].copy()\n",
    "    return outputs\n",
    "\n",
    "# 3. Terapkan ke Dataset\n",
    "print(\"‚è≥ Sedang men-tokenisasi dataset...\")\n",
    "tokenized_dataset = formatted_datasets.map(\n",
    "    tokenize_function,\n",
    "    batched=True,\n",
    "    remove_columns=formatted_datasets.column_names # Hapus kolom teks asli agar tidak error masuk model\n",
    ")\n",
    "\n",
    "print(\"‚úÖ Tokenisasi Selesai!\")\n",
    "print(f\"Contoh data: {tokenized_dataset[0].keys()}\") \n",
    "# Harusnya muncul: dict_keys(['input_ids', 'attention_mask', 'labels'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "74337948",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 30369/30369 [00:15<00:00, 2015.32 examples/s]\n",
      "c:\\Users\\Iskandar\\Documents\\GitHub\\Tubes-NLP-PaperMatch\\.venv\\Lib\\site-packages\\trl\\trainer\\sft_trainer.py:323: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `SFTTrainer.__init__`. Use `processing_class` instead.\n",
      "  super().__init__(\n",
      "c:\\Users\\Iskandar\\Documents\\GitHub\\Tubes-NLP-PaperMatch\\.venv\\Lib\\site-packages\\torch\\_dynamo\\eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3797' max='3797' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [3797/3797 7:36:47, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>2.744100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>2.480000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>2.469200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>2.471700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>250</td>\n",
       "      <td>2.440600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>2.450200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>350</td>\n",
       "      <td>2.429700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>2.447800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>450</td>\n",
       "      <td>2.433600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>2.433300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>550</td>\n",
       "      <td>2.426300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>2.423600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>650</td>\n",
       "      <td>2.445100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>700</td>\n",
       "      <td>2.415400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>750</td>\n",
       "      <td>2.413800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>2.406300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>850</td>\n",
       "      <td>2.412600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>900</td>\n",
       "      <td>2.392700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>950</td>\n",
       "      <td>2.412600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>2.405300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1050</td>\n",
       "      <td>2.379100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1100</td>\n",
       "      <td>2.400700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1150</td>\n",
       "      <td>2.397800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1200</td>\n",
       "      <td>2.403400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1250</td>\n",
       "      <td>2.394500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1300</td>\n",
       "      <td>2.403500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1350</td>\n",
       "      <td>2.395800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1400</td>\n",
       "      <td>2.385800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1450</td>\n",
       "      <td>2.390500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>2.402800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1550</td>\n",
       "      <td>2.413300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1600</td>\n",
       "      <td>2.372600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1650</td>\n",
       "      <td>2.413200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1700</td>\n",
       "      <td>2.391100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1750</td>\n",
       "      <td>2.382700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1800</td>\n",
       "      <td>2.382300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1850</td>\n",
       "      <td>2.389300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1900</td>\n",
       "      <td>2.379500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1950</td>\n",
       "      <td>2.395300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>2.382300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2050</td>\n",
       "      <td>2.376300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2100</td>\n",
       "      <td>2.393700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2150</td>\n",
       "      <td>2.375600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2200</td>\n",
       "      <td>2.388100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2250</td>\n",
       "      <td>2.379000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2300</td>\n",
       "      <td>2.391000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2350</td>\n",
       "      <td>2.373800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2400</td>\n",
       "      <td>2.372700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2450</td>\n",
       "      <td>2.385000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2500</td>\n",
       "      <td>2.377800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2550</td>\n",
       "      <td>2.371000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2600</td>\n",
       "      <td>2.384400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2650</td>\n",
       "      <td>2.385100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2700</td>\n",
       "      <td>2.385900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2750</td>\n",
       "      <td>2.389000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2800</td>\n",
       "      <td>2.388100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2850</td>\n",
       "      <td>2.380200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2900</td>\n",
       "      <td>2.363200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2950</td>\n",
       "      <td>2.376300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>2.381300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3050</td>\n",
       "      <td>2.368000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3100</td>\n",
       "      <td>2.382200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3150</td>\n",
       "      <td>2.359100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3200</td>\n",
       "      <td>2.368900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3250</td>\n",
       "      <td>2.366700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3300</td>\n",
       "      <td>2.370100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3350</td>\n",
       "      <td>2.371700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3400</td>\n",
       "      <td>2.364900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3450</td>\n",
       "      <td>2.362700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3500</td>\n",
       "      <td>2.380000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3550</td>\n",
       "      <td>2.373700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3600</td>\n",
       "      <td>2.364100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3650</td>\n",
       "      <td>2.373400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3700</td>\n",
       "      <td>2.371200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3750</td>\n",
       "      <td>2.368600</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=3797, training_loss=2.3992343789061215, metrics={'train_runtime': 27412.2762, 'train_samples_per_second': 1.108, 'train_steps_per_second': 0.139, 'total_flos': 1.4058449996064768e+17, 'train_loss': 2.3992343789061215, 'epoch': 1.0})"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import accelerate\n",
    "from trl import SFTTrainer\n",
    "from transformers import TrainingArguments\n",
    "\n",
    "# Define Training Arguments\n",
    "training_args = TrainingArguments(\n",
    "    per_device_train_batch_size = 2, # Batch size sesuai VRAM Anda\n",
    "    gradient_accumulation_steps = 4, # Jumlah step akumulasi gradien\n",
    "    warmup_steps = 5, # Warmup steps awal\n",
    "    # max_steps = 60, # Total step training (sesuaikan dengan budget Anda)\n",
    "    learning_rate = 2e-4, # Learning rate optimal\n",
    "    fp16 = not torch.cuda.is_bf16_supported(), # Menggunakan fp16 jika bf16 tidak didukung\n",
    "    bf16 = torch.cuda.is_bf16_supported(), # Menggunakan bf16 jika didukung (lebih baik)\n",
    "    logging_steps = 50, # Logging setiap 1 step\n",
    "    optim = \"adamw_8bit\", # Optimizer\n",
    "    weight_decay = 0.01, # Weight decay\n",
    "    lr_scheduler_type = \"linear\", # Linear scheduler\n",
    "    num_train_epochs=1,\n",
    "    seed = 3407, # Random seed\n",
    "    output_dir = \"outputs\", # Direktori output\n",
    "    save_strategy=\"epoch\",\n",
    "    push_to_hub=False,\n",
    "    report_to=[]\n",
    ")\n",
    "\n",
    "# Trainer\n",
    "trainer = SFTTrainer(\n",
    "    model = model,\n",
    "    tokenizer = tokenizer,\n",
    "    train_dataset = formatted_datasets, # Menggunakan dataset yang sudah diformat\n",
    "    dataset_text_field = \"text\", # Menunjukkan kolom 'text' sebagai input utama\n",
    "    max_seq_length = 1024,\n",
    "    args = training_args,\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1b81820b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model set to evaluation mode.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 10/10 [00:00<00:00, 404.36 examples/s]\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "model.eval()\n",
    "print(\"Model set to evaluation mode.\")\n",
    "\n",
    "eval_dataset = formatted_datasets.select(range(10))\n",
    "\n",
    "def format_prompt_llama3_val(examples):\n",
    "    # Ambil list data dari batch\n",
    "    inputs = examples[\"input_text\"]       # Context (Abstract + Refs)\n",
    "    outputs = examples[\"related_work\"]    # Ground Truth (Target)\n",
    "\n",
    "    prompts = []\n",
    "\n",
    "    # System Prompt: Instruksi peran untuk AI\n",
    "    system_msg = (\n",
    "        \"You are an academic writing assistant. \"\n",
    "        \"Write a 'Related Work' section based on the provided text. \"\n",
    "        \"The input contains the Current Abstract followed by References (marked with @cite_n). \"\n",
    "        \"Synthesize these references and highlight the novelty of the Current Abstract.\"\n",
    "    )\n",
    "\n",
    "    for input_text, output_text in zip(inputs, outputs):\n",
    "        # Struktur Llama 3 Instruct Resmi\n",
    "        text = f\"\"\"<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
    "\n",
    "{system_msg}<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
    "\n",
    "{input_text}<|eot_id|><|start_header_id|>assistant<|end_header_id|>\"\"\"\n",
    "\n",
    "        prompts.append(text)\n",
    "\n",
    "    # Kembalikan dalam kolom baru bernama 'text' (biasanya SFTTrainer mencari kolom ini)\n",
    "    return { \"text\": prompts }\n",
    "\n",
    "eval_dataset = eval_dataset.map(format_prompt_llama3_val, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "318c7db0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected 10 samples for evaluation.\n",
      "Generating predictions...\n",
      "Author(s): Kuperberg, Greg; Thurston, Dylan P. | Abstract: We give a purely topological definition of the perturbative quantum invariants of links and 3-manifolds associated with Chern-Simons field theory. Our definition is as close as possible to one given by Kontsevich. We will also establish some basic properties of these invariants, in particular that they are universally finite type with respect to algebraically split surgery and with respect to Torelli surgery. Torelli surgery is a mutual generalization of blink surgery of Garoufalidis and Levine and clasper surgery of Habiro. <doc-sep> @cite_0 This note is a sequel to our earlier paper of the same title [4] and describes invariants of rational homology 3-spheres associated to acyclic orthogonal local systems. Our work is in the spirit of the Axelrod‚ÄìSinger papers [1], generalizes some of their results, and furnishes a new setting for the purely topological implications of their work. <doc-sep> @cite_1 Recently, Mullins calculated the Casson-Walker invariant of the 2-fold cyclic branched cover of an oriented link in S^3 in terms of its Jones polynomial and its signature, under the assumption that the 2-fold branched cover is a rational homology 3-sphere. Using elementary principles, we provide a similar calculation for the general case. In addition, we calculate the LMO invariant of\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[10]\u001b[39m\u001b[32m, line 38\u001b[39m\n\u001b[32m     36\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mGenerating predictions...\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     37\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m i, sample \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(eval_dataset):\n\u001b[32m---> \u001b[39m\u001b[32m38\u001b[39m     generated_text = \u001b[43mgenerate_related_work\u001b[49m\u001b[43m(\u001b[49m\u001b[43msample\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43minput_text\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     39\u001b[39m     predictions.append(generated_text)\n\u001b[32m     40\u001b[39m     original_texts.append(sample[\u001b[33m\"\u001b[39m\u001b[33mrelated_work\u001b[39m\u001b[33m\"\u001b[39m])\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[10]\u001b[39m\u001b[32m, line 21\u001b[39m, in \u001b[36mgenerate_related_work\u001b[39m\u001b[34m(input_text)\u001b[39m\n\u001b[32m     19\u001b[39m \u001b[38;5;66;03m# 2. Jalankan Generate\u001b[39;00m\n\u001b[32m     20\u001b[39m   \u001b[38;5;28;01mwith\u001b[39;00m torch.no_grad():\n\u001b[32m---> \u001b[39m\u001b[32m21\u001b[39m       outputs = \u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     22\u001b[39m \u001b[43m          \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m                         \u001b[49m\u001b[38;5;66;43;03m# Unpack input_ids & attention_mask\u001b[39;49;00m\n\u001b[32m     23\u001b[39m \u001b[43m          \u001b[49m\u001b[38;5;66;43;03m# **generation_kwargs,              # Unpack config di atas\u001b[39;49;00m\n\u001b[32m     24\u001b[39m \u001b[43m          \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m     25\u001b[39m \u001b[43m          \u001b[49m\u001b[43meos_token_id\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[43m.\u001b[49m\u001b[43meos_token_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     26\u001b[39m \u001b[43m          \u001b[49m\u001b[43mpad_token_id\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mpad_token_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;66;43;03m# Gunakan pad token yang benar\u001b[39;49;00m\n\u001b[32m     27\u001b[39m \u001b[43m      \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     29\u001b[39m   generated_tokens = outputs[\u001b[32m0\u001b[39m][inputs[\u001b[33m\"\u001b[39m\u001b[33minput_ids\u001b[39m\u001b[33m\"\u001b[39m].shape[\u001b[32m1\u001b[39m]:]\n\u001b[32m     30\u001b[39m   decoded_output = tokenizer.decode(generated_tokens, skip_special_tokens=\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Iskandar\\Documents\\GitHub\\Tubes-NLP-PaperMatch\\.venv\\Lib\\site-packages\\peft\\peft_model.py:2048\u001b[39m, in \u001b[36mPeftModelForCausalLM.generate\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   2046\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m._enable_peft_forward_hooks(*args, **kwargs):\n\u001b[32m   2047\u001b[39m         kwargs = {k: v \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m kwargs.items() \u001b[38;5;28;01mif\u001b[39;00m k \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.special_peft_forward_args}\n\u001b[32m-> \u001b[39m\u001b[32m2048\u001b[39m         outputs = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mbase_model\u001b[49m\u001b[43m.\u001b[49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2049\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   2050\u001b[39m     outputs = \u001b[38;5;28mself\u001b[39m.base_model.generate(**kwargs)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Iskandar\\Documents\\GitHub\\Tubes-NLP-PaperMatch\\.venv\\Lib\\site-packages\\torch\\utils\\_contextlib.py:116\u001b[39m, in \u001b[36mcontext_decorator.<locals>.decorate_context\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    113\u001b[39m \u001b[38;5;129m@functools\u001b[39m.wraps(func)\n\u001b[32m    114\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mdecorate_context\u001b[39m(*args, **kwargs):\n\u001b[32m    115\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[32m--> \u001b[39m\u001b[32m116\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Iskandar\\Documents\\GitHub\\Tubes-NLP-PaperMatch\\.venv\\Lib\\site-packages\\transformers\\generation\\utils.py:2564\u001b[39m, in \u001b[36mGenerationMixin.generate\u001b[39m\u001b[34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, negative_prompt_ids, negative_prompt_attention_mask, use_model_defaults, custom_generate, **kwargs)\u001b[39m\n\u001b[32m   2561\u001b[39m model_kwargs[\u001b[33m\"\u001b[39m\u001b[33muse_cache\u001b[39m\u001b[33m\"\u001b[39m] = generation_config.use_cache\n\u001b[32m   2563\u001b[39m \u001b[38;5;66;03m# 9. Call generation mode\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m2564\u001b[39m result = \u001b[43mdecoding_method\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2565\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m   2566\u001b[39m \u001b[43m    \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2567\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlogits_processor\u001b[49m\u001b[43m=\u001b[49m\u001b[43mprepared_logits_processor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2568\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstopping_criteria\u001b[49m\u001b[43m=\u001b[49m\u001b[43mprepared_stopping_criteria\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2569\u001b[39m \u001b[43m    \u001b[49m\u001b[43mgeneration_config\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgeneration_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2570\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mgeneration_mode_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2571\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mmodel_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2572\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2574\u001b[39m \u001b[38;5;66;03m# Convert to legacy cache format if requested\u001b[39;00m\n\u001b[32m   2575\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[32m   2576\u001b[39m     generation_config.return_legacy_cache \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m   2577\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(result, \u001b[33m\"\u001b[39m\u001b[33mpast_key_values\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m   2578\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(result.past_key_values, \u001b[33m\"\u001b[39m\u001b[33mto_legacy_cache\u001b[39m\u001b[33m\"\u001b[39m) \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   2579\u001b[39m ):\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Iskandar\\Documents\\GitHub\\Tubes-NLP-PaperMatch\\.venv\\Lib\\site-packages\\transformers\\generation\\utils.py:2779\u001b[39m, in \u001b[36mGenerationMixin._sample\u001b[39m\u001b[34m(self, input_ids, logits_processor, stopping_criteria, generation_config, synced_gpus, streamer, **model_kwargs)\u001b[39m\n\u001b[32m   2776\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   2777\u001b[39m     is_prefill = \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m2779\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28mself\u001b[39m._has_unfinished_sequences(this_peer_finished, synced_gpus, device=input_ids.device):\n\u001b[32m   2780\u001b[39m     \u001b[38;5;66;03m# prepare model inputs\u001b[39;00m\n\u001b[32m   2781\u001b[39m     model_inputs = \u001b[38;5;28mself\u001b[39m.prepare_inputs_for_generation(input_ids, **model_kwargs)\n\u001b[32m   2783\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m is_prefill:\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "### 5. Evaluate Model Performance\n",
    "\n",
    "# After training, we evaluate the model's performance on a held-out validation set. We generate predictions for a subset of the validation data and compare them against the original 'Related Work' sections using various metrics like ROUGE, BERTScore, and length analysis.\n",
    "\n",
    "\n",
    "\n",
    "print(f\"Selected {len(eval_dataset)} samples for evaluation.\")\n",
    "\n",
    "# 3. Define the generation function\n",
    "def generate_related_work(input_text):\n",
    "    # Ensure pad_token_id is set for the tokenizer to prevent reorder_cache error\n",
    "    if tokenizer.pad_token_id is None:\n",
    "        tokenizer.pad_token_id = tokenizer.eos_token_id\n",
    "\n",
    "    print(input_text)\n",
    "    inputs = tokenizer(input_text, return_tensors=\"pt\", add_special_tokens=False)\n",
    "    inputs = {k: v.to(model.device) for k, v in inputs.items()}\n",
    "\n",
    "  # 2. Jalankan Generate\n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            **inputs,                         # Unpack input_ids & attention_mask\n",
    "            # **generation_kwargs,              # Unpack config di atas\n",
    "            use_cache=True,\n",
    "            eos_token_id=tokenizer.eos_token_id,\n",
    "            pad_token_id=tokenizer.pad_token_id, # Gunakan pad token yang benar\n",
    "        )\n",
    "\n",
    "    generated_tokens = outputs[0][inputs[\"input_ids\"].shape[1]:]\n",
    "    decoded_output = tokenizer.decode(generated_tokens, skip_special_tokens=True)\n",
    "\n",
    "    return decoded_output.strip()\n",
    "\n",
    "predictions = []\n",
    "original_texts = []\n",
    "print(\"Generating predictions...\")\n",
    "for i, sample in enumerate(eval_dataset):\n",
    "    generated_text = generate_related_work(sample[\"input_text\"])\n",
    "    predictions.append(generated_text)\n",
    "    original_texts.append(sample[\"related_work\"])\n",
    "    if (i + 1) % 10 == 0:\n",
    "        print(f\"Generated prediction for {i + 1}/{len(eval_dataset)} samples.\")\n",
    "\n",
    "print(\"Prediction generation complete.\")\n",
    "print(f\"Generated {len(predictions)} predictions.\")\n",
    "\n",
    "print(\"\\n--- First Generated Prediction ---\")\n",
    "print(predictions[0])\n",
    "print(\"\\n--- First Original Related Work ---\")\n",
    "print(original_texts[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "81aa1fc1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved to summarization/related_works_generation_model\n"
     ]
    }
   ],
   "source": [
    "# presaved_model = model.merge_and_unload()\n",
    "\n",
    "save_path = \"summarization/related_works_generation_model\"\n",
    "\n",
    "os.makedirs(save_path, exist_ok=True)\n",
    "# Save full merged model\n",
    "presaved_model.save_pretrained(save_path)\n",
    "tokenizer.save_pretrained(save_path)\n",
    "\n",
    "print(f\"Model saved to {save_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a93748a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install evaluate rouge-score bert-score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3db2877",
   "metadata": {},
   "outputs": [],
   "source": [
    "### 6. Calculate Comprehensive Evaluation Metrics\n",
    "\n",
    "#This section defines a function to compute ROUGE scores (for lexical similarity), BERTScore (for semantic similarity), and analyze the length of generated texts compared to the references. These metrics provide a holistic view of the model's generation quality.\n",
    "\n",
    "def calculate_comprehensive_metrics(predictions, references):\n",
    "    \"\"\"\n",
    "    Menghitung ROUGE, BERTScore, dan Rasio Panjang.\n",
    "\n",
    "    Args:\n",
    "        predictions (list): List string hasil output model.\n",
    "        references (list): List string kunci jawaban asli (ground truth).\n",
    "\n",
    "    Returns:\n",
    "        dict: Dictionary berisi semua skor evaluasi.\n",
    "    \"\"\"\n",
    "\n",
    "    print(f\"üìä Memulai Evaluasi untuk {len(predictions)} sampel data...\")\n",
    "    results = {}\n",
    "\n",
    "    # --- 1. ROUGE SCORE (Lexical / Kata) ---\n",
    "    print(\"‚è≥ Menghitung ROUGE...\")\n",
    "    rouge_metric = evaluate.load(\"rouge\")\n",
    "    rouge_scores = rouge_metric.compute(\n",
    "        predictions=predictions,\n",
    "        references=references,\n",
    "        use_stemmer=True # Penting untuk bahasa Inggris\n",
    "    )\n",
    "    # Konversi ke Persen (0-100)\n",
    "    results['ROUGE-1'] = round(rouge_scores['rouge1'] * 100, 2)\n",
    "    results['ROUGE-2'] = round(rouge_scores['rouge2'] * 100, 2)\n",
    "    results['ROUGE-L'] = round(rouge_scores['rougeL'] * 100, 2)\n",
    "\n",
    "    # --- 2. BERTSCORE (Semantic / Makna) ---\n",
    "    print(\"‚è≥ Menghitung BERTScore (Mungkin butuh waktu & download model)...\")\n",
    "    bertscore_metric = evaluate.load(\"bertscore\")\n",
    "    # Gunakan batch_size agar tidak OOM\n",
    "    bert_scores = bertscore_metric.compute(\n",
    "        predictions=predictions,\n",
    "        references=references,\n",
    "        lang=\"en\",\n",
    "        batch_size=16\n",
    "    )\n",
    "    # Kita ambil rata-rata F1 Score dari semua data\n",
    "    results['BERTScore-F1'] = round(np.mean(bert_scores['f1']) * 100, 2)\n",
    "    results['BERTScore-Precision'] = round(np.mean(bert_scores['precision']) * 100, 2)\n",
    "    results['BERTScore-Recall'] = round(np.mean(bert_scores['recall']) * 100, 2)\n",
    "\n",
    "    # --- 3. LENGTH ANALYSIS (Analisis Panjang) ---\n",
    "    print(\"‚è≥ Menghitung Statistik Panjang Teks...\")\n",
    "    pred_lens = [len(p.split()) for p in predictions]\n",
    "    ref_lens = [len(r.split()) for r in references]\n",
    "\n",
    "    avg_pred_len = np.mean(pred_lens)\n",
    "    avg_ref_len = np.mean(ref_lens)\n",
    "    length_ratio = (avg_pred_len / avg_ref_len) * 100\n",
    "\n",
    "    results['Avg Gen Length'] = round(avg_pred_len, 1)\n",
    "    results['Avg Ref Length'] = round(avg_ref_len, 1)\n",
    "    results['Length Ratio (%)'] = round(length_ratio, 2)\n",
    "\n",
    "    return results\n",
    "\n",
    "# Jalankan Fungsi\n",
    "final_metrics = calculate_comprehensive_metrics(predictions, original_texts)\n",
    "\n",
    "# Tampilkan Hasil Rapih\n",
    "print(\"\\n\" + \"=\"*40)\n",
    "print(\"      LAPORAN HASIL EVALUASI AKHIR      \")\n",
    "print(\"=\"*40)\n",
    "for metric, score in final_metrics.items():\n",
    "    print(f\"{metric:<20} : {score}\")\n",
    "print(\"=\"*40)\n",
    "\n",
    "# Interpretasi Singkat\n",
    "print(\"\\n--- Interpretasi Cepat ---\")\n",
    "if final_metrics['BERTScore-F1'] > 85:\n",
    "    print(\"Kualitas Makna SANGAT BAIK (Mirip manusia).\")\n",
    "elif final_metrics['BERTScore-F1'] > 80:\n",
    "    print(\"Kualitas Makna CUKUP BAIK.\")\n",
    "else:\n",
    "    print(\"Kualitas Makna KURANG (Model mungkin halusinasi/tidak nyambung).\")\n",
    "\n",
    "if final_metrics['Length Ratio (%)'] < 80:\n",
    "    print(\"WARNING: Output model terlalu pendek dibanding referensi asli.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
