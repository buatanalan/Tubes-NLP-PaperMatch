{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a6cef97b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Iskandar\\Documents\\GitHub\\Tubes-NLP-PaperMatch\\venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset Train loaded: Dataset({\n",
      "    features: ['final_text', 'label'],\n",
      "    num_rows: 86941\n",
      "})\n",
      "Dataset Test loaded: Dataset({\n",
      "    features: ['final_text', 'label'],\n",
      "    num_rows: 21736\n",
      "})\n",
      "Label Mapping loaded: {'astro-ph': 0, 'cond-mat': 1, 'cs': 2, 'econ': 3, 'eess': 4, 'gr-qc': 5, 'hep-ex': 6, 'hep-lat': 7, 'hep-ph': 8, 'hep-th': 9, 'math': 10, 'math-ph': 11, 'nlin': 12, 'nucl-ex': 13, 'nucl-th': 14, 'physics': 15, 'q-bio': 16, 'q-fin': 17, 'quant-ph': 18, 'stat': 19}\n",
      "Class Weights loaded (re-casted): {0: 0.9724944071588367, 1: 0.7553518679409209, 2: 0.19485633600788918, 3: 12.673615160349854, 4: 1.8127814845704755, 5: 6.792265625, 6: 6.792265625, 7: 6.792265625, 8: 6.813557993730408, 9: 6.792265625, 10: 0.2343423180592992, 11: 6.792265625, 12: 1.552517857142857, 13: 6.802895148669797, 14: 6.792265625, 15: 0.3266003005259204, 16: 0.8681945276612743, 17: 1.1803013847407005, 18: 6.802895148669797, 19: 1.69806640625}\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from datasets import load_from_disk\n",
    "\n",
    "dataset_train_loaded = load_from_disk(\"./topic_classification/train_dataset\")\n",
    "dataset_test_loaded = load_from_disk(\"./topic_classification/test_dataset\")\n",
    "\n",
    "print(\"Dataset Train loaded:\", dataset_train_loaded)\n",
    "print(\"Dataset Test loaded:\", dataset_test_loaded)\n",
    "\n",
    "with open(\"label_mapping.json\", \"r\") as f:\n",
    "    label_mapping_loaded = json.load(f)\n",
    "\n",
    "print(\"Label Mapping loaded:\", label_mapping_loaded)\n",
    "\n",
    "with open(\"class_weight.json\", \"r\") as f:\n",
    "    class_weight_loaded = json.load(f)\n",
    "\n",
    "class_weight_dict = {int(k): v for k, v in class_weight_loaded.items()}\n",
    "\n",
    "print(\"Class Weights loaded (re-casted):\", class_weight_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c20f0ada",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "5 Data Pertama (Train)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>final_text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ante forecast outcome interpreted counterfactu...</td>\n",
       "      <td>17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>gaming customizing individual character create...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>framework european research project meteomet l...</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>software product quality defined feature chara...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>optimizing communication imperative large scal...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          final_text  label\n",
       "0  ante forecast outcome interpreted counterfactu...     17\n",
       "1  gaming customizing individual character create...      2\n",
       "2  framework european research project meteomet l...     15\n",
       "3  software product quality defined feature chara...      2\n",
       "4  optimizing communication imperative large scal...      2"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "5 Data Pertama (Test)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>final_text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>work propose use dropout bayesian estimator in...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>given simple polygon [eq] consisting [eq] vert...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>pattern stored within pre trained deep neural ...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ride sharing service gaining popularity crucia...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>perform experiment phase simulation ring netwo...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          final_text  label\n",
       "0  work propose use dropout bayesian estimator in...      4\n",
       "1  given simple polygon [eq] consisting [eq] vert...      2\n",
       "2  pattern stored within pre trained deep neural ...      2\n",
       "3  ride sharing service gaining popularity crucia...      2\n",
       "4  perform experiment phase simulation ring netwo...      1"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "print(\"\\n5 Data Pertama (Train)\")\n",
    "df_sample_train = pd.DataFrame(dataset_train_loaded[:5])\n",
    "display(df_sample_train)\n",
    "\n",
    "print(\"\\n5 Data Pertama (Test)\")\n",
    "df_sample_test = pd.DataFrame(dataset_test_loaded[:5])\n",
    "display(df_sample_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "67442d71",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tipe kolom label sekarang: ClassLabel(names=['0', '1', '2', '3', '4', '5', '6', '7', '8', '9', '10', '11', '12', '13', '14', '15', '16', '17', '18', '19'])\n",
      "==============================\n",
      "Original Train: 86941\n",
      "New Train (80%): 69552\n",
      "New Val   (20%): 17389\n",
      "Original Test : 21736\n",
      "==============================\n"
     ]
    }
   ],
   "source": [
    "from datasets import ClassLabel\n",
    "\n",
    "num_classes = len(label_mapping_loaded)\n",
    "\n",
    "dataset_train_loaded = dataset_train_loaded.cast_column(\n",
    "    \"label\", \n",
    "    ClassLabel(num_classes=num_classes)\n",
    ")\n",
    "\n",
    "print(\"Tipe kolom label sekarang:\", dataset_train_loaded.features['label'])\n",
    "\n",
    "split_result = dataset_train_loaded.train_test_split(\n",
    "    test_size=0.2, \n",
    "    seed=42, \n",
    "    stratify_by_column=\"label\"\n",
    ")\n",
    "\n",
    "dataset_train_final = split_result['train']\n",
    "dataset_val_final = split_result['test']\n",
    "\n",
    "print(\"=\"*30)\n",
    "print(f\"Original Train: {len(dataset_train_loaded)}\")\n",
    "print(f\"New Train (80%): {len(dataset_train_final)}\")\n",
    "print(f\"New Val   (20%): {len(dataset_val_final)}\")\n",
    "print(f\"Original Test : {len(dataset_test_loaded)}\")\n",
    "print(\"=\"*30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a8b36fe4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==============================\n",
      "STATUS DATASET (MODE DEBUG)\n",
      "==============================\n",
      "Train Size : 1000\n",
      "Val Size   : 200\n",
      "Test Size : 200\n",
      "==============================\n"
     ]
    }
   ],
   "source": [
    "n_train_limit = min(1000, len(dataset_train_final))\n",
    "dataset_train_final_samples = dataset_train_final.select(range(n_train_limit))\n",
    "\n",
    "n_val_limit = min(200, len(dataset_val_final))\n",
    "dataset_val_final_samples = dataset_val_final.select(range(n_val_limit))\n",
    "\n",
    "n_test_limit = min(200, len(dataset_val_final))\n",
    "dataset_test_final_samples = dataset_test_loaded.select(range(n_val_limit))\n",
    "\n",
    "print(\"\\n\" + \"=\"*30)\n",
    "print(\"STATUS DATASET (MODE DEBUG)\")\n",
    "print(\"=\"*30)\n",
    "print(f\"Train Size : {len(dataset_train_final_samples)}\")\n",
    "print(f\"Val Size   : {len(dataset_val_final_samples)}\")\n",
    "print(f\"Test Size : {len(dataset_test_final_samples)}\")\n",
    "print(\"=\"*30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ad9a8d74",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from datasets import Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8a50fb98",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Iskandar\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "dd110bd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = dataset_train_final.to_pandas()\n",
    "df_val = dataset_val_final.to_pandas()\n",
    "df_test = dataset_test_loaded.to_pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fe666e19",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_and_tokenize(text):\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'[^a-z\\s]', ' ', text)\n",
    "    tokens = word_tokenize(text)\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "231f61a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train['tokens'] = df_train['final_text'].apply(clean_and_tokenize)\n",
    "df_val['tokens'] = df_val['final_text'].apply(clean_and_tokenize)\n",
    "df_test['tokens'] = df_test['final_text'].apply(clean_and_tokenize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c9ee9629",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_classes = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f98e6606",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec, FastText, KeyedVectors\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import TruncatedSVD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d2b43e66",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = df_train['tokens'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8de82cc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Word2Vec\n",
    "w2v_model = Word2Vec(sentences, vector_size=300, window=5, workers=4, epochs=10, min_count=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a4c72961",
   "metadata": {},
   "outputs": [],
   "source": [
    "# FastText\n",
    "ft_custom_model = FastText(sentences, vector_size=300, window=5, workers=4, epochs=10, min_count=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "cda5ad12",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FastText Pretrained loaded.\n"
     ]
    }
   ],
   "source": [
    "# FastText Pretrained\n",
    "try:\n",
    "    ft_pretrained = KeyedVectors.load_word2vec_format('cc.en.300.vec', binary=False) \n",
    "    print(\"FastText Pretrained loaded.\")\n",
    "except Exception as e:\n",
    "    print(f\"Gagal load FastText Pretrained: {e}. Menggunakan custom model sebagai fallback.\")\n",
    "    ft_pretrained = ft_custom_model.wv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "78ba3148",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GloVe loaded. Found 400000 words.\n"
     ]
    }
   ],
   "source": [
    "# GloVe\n",
    "glove_embeddings = {}\n",
    "try:\n",
    "    with open('./glove.6B.100d.txt', encoding=\"utf8\") as f:\n",
    "        for line in f:\n",
    "            values = line.split()\n",
    "            word = values[0]\n",
    "            coefs = np.asarray(values[1:], dtype='float32')\n",
    "            glove_embeddings[word] = coefs\n",
    "    print(f\"GloVe loaded. Found {len(glove_embeddings)} words.\")\n",
    "except FileNotFoundError:\n",
    "    print(\"File GloVe tidak ditemukan. Pastikan path benar.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "628412e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vectorizing TF-IDF + SVD...\n",
      "Shape TF-IDF SVD: (69552, 2000)\n"
     ]
    }
   ],
   "source": [
    "# TF-IDF + SVD\n",
    "print(\"Vectorizing TF-IDF + SVD...\")\n",
    "tfidf = TfidfVectorizer(max_features=100000)\n",
    "X_train_tfidf = tfidf.fit_transform(df_train['final_text'])\n",
    "X_val_tfidf = tfidf.transform(df_val['final_text'])\n",
    "\n",
    "svd = TruncatedSVD(n_components=2000, random_state=42)\n",
    "X_train_svd = svd.fit_transform(X_train_tfidf)\n",
    "X_val_svd = svd.transform(X_val_tfidf)\n",
    "print(f\"Shape TF-IDF SVD: {X_train_svd.shape}\")\n",
    "\n",
    "X_train_svd = X_train_svd.astype(np.float32)\n",
    "X_val_svd = X_val_svd.astype(np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8168127a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "max_words = 200000\n",
    "max_len = 200\n",
    "\n",
    "# tokenizer = Tokenizer(num_words=max_words)\n",
    "# tokenizer.fit_on_texts(df_train['final_text'])\n",
    "# word_index = tokenizer.word_index\n",
    "\n",
    "# Simpan tokenizer\n",
    "# with open(\"tokenizer_sl.pkl\", \"wb\") as f:\n",
    "#     pickle.dump(tokenizer, f)\n",
    "\n",
    "with open(\"tokenizer_sl.pkl\", \"rb\") as f:\n",
    "    tokenizer = pickle.load(f)\n",
    " \n",
    "word_index = tokenizer.word_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d72cef84",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_seq = tokenizer.texts_to_sequences(df_train['tokens'])\n",
    "X_val_seq = tokenizer.texts_to_sequences(df_val['tokens'])\n",
    "X_test_seq = tokenizer.texts_to_sequences(df_test['tokens'])\n",
    "\n",
    "X_train_pad = pad_sequences(X_train_seq, maxlen=max_len)\n",
    "X_val_pad = pad_sequences(X_val_seq, maxlen=max_len)\n",
    "X_test_pad = pad_sequences(X_test_seq, maxlen=max_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f27e1269",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Contoh input asli: ['bound', 'quasiparticle', 'negatively', 'charged', 'trions']\n",
      "Contoh input angka: [116, 3443, 4401, 1362, 23950]\n"
     ]
    }
   ],
   "source": [
    "print(f\"Contoh input asli: {df_train['tokens'].iloc[0][:5]}\")\n",
    "print(f\"Contoh input angka: {X_train_seq[0][:5]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c12e27f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = df_train['label'].values\n",
    "y_val = df_val['label'].values\n",
    "y_test = df_test['label'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "0a9a1534",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Creating W2V Matrix...\n",
      "Embedding coverage: 54072/88208 words found.\n",
      "\n",
      "Creating FastText Custom Matrix...\n",
      "Embedding coverage: 88208/88208 words found.\n",
      "\n",
      "Creating FastText Pretrained Matrix...\n",
      "Embedding coverage: 51634/88208 words found.\n",
      "\n",
      "Creating GloVe Matrix...\n",
      "Embedding coverage: 48928/88208 words found.\n"
     ]
    }
   ],
   "source": [
    "def create_embedding_matrix(word_index, embedding_source, dim, type='gensim'):\n",
    "    vocab_size = len(word_index) + 1\n",
    "    matrix = np.zeros((vocab_size, dim))\n",
    "    hits = 0\n",
    "    \n",
    "    for word, i in word_index.items():\n",
    "        if i >= max_words: continue\n",
    "        \n",
    "        try:\n",
    "            if type == 'gensim':\n",
    "                if word in embedding_source.wv: \n",
    "                    matrix[i] = embedding_source.wv[word]\n",
    "                    hits += 1\n",
    "            elif type == 'keyedvectors':\n",
    "                if word in embedding_source:\n",
    "                    matrix[i] = embedding_source[word]\n",
    "                    hits += 1\n",
    "            elif type == 'dict':\n",
    "                embedding_vector = embedding_source.get(word)\n",
    "                if embedding_vector is not None:\n",
    "                    matrix[i] = embedding_vector\n",
    "                    hits += 1\n",
    "        except:\n",
    "            continue\n",
    "            \n",
    "    print(f\"Embedding coverage: {hits}/{len(word_index)} words found.\")\n",
    "    return matrix\n",
    "\n",
    "print(\"\\nCreating W2V Matrix...\")\n",
    "matrix_w2v = create_embedding_matrix(word_index, w2v_model, 300, type='gensim')\n",
    "\n",
    "print(\"\\nCreating FastText Custom Matrix...\")\n",
    "matrix_ft_custom = create_embedding_matrix(word_index, ft_custom_model, 300, type='gensim')\n",
    "\n",
    "print(\"\\nCreating FastText Pretrained Matrix...\")\n",
    "matrix_ft_pre = create_embedding_matrix(word_index, ft_pretrained, 300, type='keyedvectors')\n",
    "\n",
    "print(\"\\nCreating GloVe Matrix...\")\n",
    "matrix_glove = create_embedding_matrix(word_index, glove_embeddings, 100, type='dict')\n",
    "\n",
    "embedding_configs = {\n",
    "    \"Word2Vec_300\": (matrix_w2v, 300),\n",
    "    \"FT_Custom_300\": (matrix_ft_custom, 300),\n",
    "    \"FT_Pre_300\": (matrix_ft_pre, 300),\n",
    "    \"GloVe_100\": (matrix_glove, 100)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "3b700517",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class weights dictionary: {0: np.float64(0.9724832214765101), 1: np.float64(0.7553431798436142), 2: np.float64(0.19485627836611194), 3: np.float64(12.645818181818182), 4: np.float64(1.813138686131387), 5: np.float64(6.7921875), 6: np.float64(6.7921875), 7: np.float64(6.7921875), 8: np.float64(6.818823529411764), 9: np.float64(6.7921875), 10: np.float64(0.23433962264150943), 11: np.float64(6.7921875), 12: np.float64(1.5525), 13: np.float64(6.8054794520547945), 14: np.float64(6.7921875), 15: np.float64(0.32659654395191584), 16: np.float64(0.8680978532201697), 17: np.float64(1.1804480651731162), 18: np.float64(6.8054794520547945), 19: np.float64(1.698046875)}\n",
      "target names: ['0', '1', '2', '3', '4', '5', '6', '7', '8', '9', '10', '11', '12', '13', '14', '15', '16', '17', '18', '19']\n",
      "Mulai Training Loop untuk 2 Embeddings x 4 Models...\n",
      "\n",
      "========== Training CNN_Word2Vec_300 ==========\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Iskandar\\Documents\\GitHub\\Tubes-NLP-PaperMatch\\venv\\Lib\\site-packages\\keras\\src\\layers\\core\\embedding.py:97: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "\u001b[1m1087/1087\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m291s\u001b[0m 266ms/step - accuracy: 0.4403 - loss: 1.6749 - val_accuracy: 0.5611 - val_loss: 1.3053\n",
      "Epoch 2/30\n",
      "\u001b[1m1087/1087\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m290s\u001b[0m 266ms/step - accuracy: 0.5679 - loss: 1.1490 - val_accuracy: 0.6016 - val_loss: 1.2158\n",
      "Epoch 3/30\n",
      "\u001b[1m1087/1087\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m285s\u001b[0m 262ms/step - accuracy: 0.6137 - loss: 0.9602 - val_accuracy: 0.5787 - val_loss: 1.1918\n",
      "Epoch 4/30\n",
      "\u001b[1m1087/1087\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m286s\u001b[0m 263ms/step - accuracy: 0.6487 - loss: 0.8045 - val_accuracy: 0.6293 - val_loss: 1.0868\n",
      "Epoch 5/30\n",
      "\u001b[1m1087/1087\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m286s\u001b[0m 263ms/step - accuracy: 0.6802 - loss: 0.6629 - val_accuracy: 0.6337 - val_loss: 1.1001\n",
      "Epoch 6/30\n",
      "\u001b[1m1087/1087\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m287s\u001b[0m 264ms/step - accuracy: 0.7037 - loss: 0.5595 - val_accuracy: 0.6753 - val_loss: 1.0183\n",
      "Epoch 7/30\n",
      "\u001b[1m1087/1087\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m286s\u001b[0m 263ms/step - accuracy: 0.7281 - loss: 0.4901 - val_accuracy: 0.6875 - val_loss: 0.9952\n",
      "Epoch 8/30\n",
      "\u001b[1m1087/1087\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m287s\u001b[0m 264ms/step - accuracy: 0.7530 - loss: 0.4280 - val_accuracy: 0.6565 - val_loss: 1.1189\n",
      "Epoch 9/30\n",
      "\u001b[1m1087/1087\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m290s\u001b[0m 267ms/step - accuracy: 0.7666 - loss: 0.3799 - val_accuracy: 0.6929 - val_loss: 1.0286\n",
      "Epoch 10/30\n",
      "\u001b[1m1087/1087\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m287s\u001b[0m 264ms/step - accuracy: 0.7915 - loss: 0.3354 - val_accuracy: 0.6814 - val_loss: 1.1197\n",
      "Epoch 11/30\n",
      "\u001b[1m1087/1087\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m285s\u001b[0m 262ms/step - accuracy: 0.8031 - loss: 0.3097 - val_accuracy: 0.7051 - val_loss: 1.0628\n",
      "Epoch 12/30\n",
      "\u001b[1m1087/1087\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m286s\u001b[0m 263ms/step - accuracy: 0.8208 - loss: 0.2889 - val_accuracy: 0.7038 - val_loss: 1.1254\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved to models_baru/CNN_Word2Vec_300.h5\n",
      "\u001b[1m544/544\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 10ms/step\n",
      "Accuracy: 0.6875\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.80      0.88      0.84       894\n",
      "           1       0.65      0.76      0.70      1151\n",
      "           2       0.85      0.70      0.77      4462\n",
      "           3       0.18      0.29      0.22        68\n",
      "           4       0.31      0.63      0.41       480\n",
      "           5       0.41      0.66      0.51       128\n",
      "           6       0.53      0.84      0.65       128\n",
      "           7       0.82      0.80      0.81       128\n",
      "           8       0.66      0.63      0.65       128\n",
      "           9       0.46      0.71      0.55       128\n",
      "          10       0.89      0.78      0.83      3710\n",
      "          11       0.09      0.37      0.15       128\n",
      "          12       0.42      0.72      0.53       560\n",
      "          13       0.49      0.60      0.54       128\n",
      "          14       0.54      0.67      0.60       128\n",
      "          15       0.78      0.39      0.52      2662\n",
      "          16       0.69      0.79      0.74      1001\n",
      "          17       0.84      0.90      0.87       737\n",
      "          18       0.34      0.66      0.45       128\n",
      "          19       0.33      0.60      0.43       512\n",
      "\n",
      "    accuracy                           0.69     17389\n",
      "   macro avg       0.55      0.67      0.59     17389\n",
      "weighted avg       0.75      0.69      0.70     17389\n",
      "\n",
      "\n",
      "========== Training RNN_Word2Vec_300 ==========\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Iskandar\\Documents\\GitHub\\Tubes-NLP-PaperMatch\\venv\\Lib\\site-packages\\keras\\src\\layers\\core\\embedding.py:97: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "\u001b[1m1087/1087\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m329s\u001b[0m 300ms/step - accuracy: 0.2360 - loss: 2.4169 - val_accuracy: 0.3506 - val_loss: 1.9611\n",
      "Epoch 2/30\n",
      "\u001b[1m1087/1087\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m332s\u001b[0m 305ms/step - accuracy: 0.3185 - loss: 2.0392 - val_accuracy: 0.2897 - val_loss: 2.0230\n",
      "Epoch 3/30\n",
      "\u001b[1m1087/1087\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m324s\u001b[0m 298ms/step - accuracy: 0.3553 - loss: 1.9168 - val_accuracy: 0.3525 - val_loss: 1.8323\n",
      "Epoch 4/30\n",
      "\u001b[1m1087/1087\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m321s\u001b[0m 295ms/step - accuracy: 0.3483 - loss: 1.9542 - val_accuracy: 0.3639 - val_loss: 1.8133\n",
      "Epoch 5/30\n",
      "\u001b[1m1087/1087\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m323s\u001b[0m 297ms/step - accuracy: 0.3666 - loss: 1.8788 - val_accuracy: 0.3289 - val_loss: 1.9406\n",
      "Epoch 6/30\n",
      "\u001b[1m1087/1087\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m322s\u001b[0m 296ms/step - accuracy: 0.3690 - loss: 1.8304 - val_accuracy: 0.3002 - val_loss: 1.9722\n",
      "Epoch 7/30\n",
      "\u001b[1m1087/1087\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m320s\u001b[0m 295ms/step - accuracy: 0.3753 - loss: 1.7752 - val_accuracy: 0.2816 - val_loss: 2.3337\n",
      "Epoch 8/30\n",
      "\u001b[1m1087/1087\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m324s\u001b[0m 298ms/step - accuracy: 0.3339 - loss: 1.9380 - val_accuracy: 0.3473 - val_loss: 1.9302\n",
      "Epoch 9/30\n",
      "\u001b[1m1087/1087\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m327s\u001b[0m 301ms/step - accuracy: 0.3388 - loss: 1.8357 - val_accuracy: 0.2854 - val_loss: 2.0917\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved to models_baru/RNN_Word2Vec_300.h5\n",
      "\u001b[1m544/544\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 24ms/step\n",
      "Accuracy: 0.3639\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.59      0.41      0.49       894\n",
      "           1       0.51      0.37      0.43      1151\n",
      "           2       0.65      0.17      0.27      4462\n",
      "           3       0.03      0.25      0.05        68\n",
      "           4       0.10      0.64      0.17       480\n",
      "           5       0.08      0.65      0.14       128\n",
      "           6       0.17      0.60      0.26       128\n",
      "           7       0.11      0.10      0.11       128\n",
      "           8       0.08      0.27      0.13       128\n",
      "           9       0.12      0.11      0.12       128\n",
      "          10       0.68      0.79      0.73      3710\n",
      "          11       0.05      0.09      0.06       128\n",
      "          12       0.24      0.28      0.26       560\n",
      "          13       0.10      0.09      0.10       128\n",
      "          14       0.13      0.11      0.12       128\n",
      "          15       0.56      0.01      0.01      2662\n",
      "          16       0.31      0.65      0.42      1001\n",
      "          17       0.71      0.40      0.51       737\n",
      "          18       0.13      0.38      0.19       128\n",
      "          19       0.16      0.23      0.19       512\n",
      "\n",
      "    accuracy                           0.36     17389\n",
      "   macro avg       0.28      0.33      0.24     17389\n",
      "weighted avg       0.53      0.36      0.35     17389\n",
      "\n",
      "\n",
      "========== Training LSTM_Word2Vec_300 ==========\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Iskandar\\Documents\\GitHub\\Tubes-NLP-PaperMatch\\venv\\Lib\\site-packages\\keras\\src\\layers\\core\\embedding.py:97: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "\u001b[1m1087/1087\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m384s\u001b[0m 351ms/step - accuracy: 0.4469 - loss: 1.7769 - val_accuracy: 0.5924 - val_loss: 1.1744\n",
      "Epoch 2/30\n",
      "\u001b[1m1087/1087\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m391s\u001b[0m 360ms/step - accuracy: 0.6021 - loss: 1.1060 - val_accuracy: 0.6590 - val_loss: 1.0086\n",
      "Epoch 3/30\n",
      "\u001b[1m1087/1087\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m389s\u001b[0m 358ms/step - accuracy: 0.6687 - loss: 0.8468 - val_accuracy: 0.6789 - val_loss: 0.9745\n",
      "Epoch 4/30\n",
      "\u001b[1m1087/1087\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m397s\u001b[0m 365ms/step - accuracy: 0.7235 - loss: 0.6463 - val_accuracy: 0.6858 - val_loss: 0.9770\n",
      "Epoch 5/30\n",
      "\u001b[1m1087/1087\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m389s\u001b[0m 357ms/step - accuracy: 0.7743 - loss: 0.4796 - val_accuracy: 0.7041 - val_loss: 0.9557\n",
      "Epoch 6/30\n",
      "\u001b[1m1087/1087\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m392s\u001b[0m 361ms/step - accuracy: 0.8182 - loss: 0.3534 - val_accuracy: 0.7133 - val_loss: 0.9922\n",
      "Epoch 7/30\n",
      "\u001b[1m1087/1087\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m390s\u001b[0m 359ms/step - accuracy: 0.8504 - loss: 0.2764 - val_accuracy: 0.7279 - val_loss: 1.0092\n",
      "Epoch 8/30\n",
      "\u001b[1m1087/1087\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m396s\u001b[0m 365ms/step - accuracy: 0.8775 - loss: 0.2103 - val_accuracy: 0.7178 - val_loss: 1.1186\n",
      "Epoch 9/30\n",
      "\u001b[1m1087/1087\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m392s\u001b[0m 361ms/step - accuracy: 0.8935 - loss: 0.1840 - val_accuracy: 0.7374 - val_loss: 1.1611\n",
      "Epoch 10/30\n",
      "\u001b[1m1087/1087\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m390s\u001b[0m 359ms/step - accuracy: 0.9119 - loss: 0.1509 - val_accuracy: 0.7424 - val_loss: 1.2327\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved to models_baru/LSTM_Word2Vec_300.h5\n",
      "\u001b[1m544/544\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 32ms/step\n",
      "Accuracy: 0.7041\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.84      0.87      0.86       894\n",
      "           1       0.64      0.77      0.70      1151\n",
      "           2       0.87      0.69      0.77      4462\n",
      "           3       0.11      0.47      0.18        68\n",
      "           4       0.33      0.65      0.44       480\n",
      "           5       0.42      0.68      0.52       128\n",
      "           6       0.63      0.80      0.71       128\n",
      "           7       0.83      0.80      0.81       128\n",
      "           8       0.68      0.67      0.68       128\n",
      "           9       0.46      0.70      0.56       128\n",
      "          10       0.92      0.77      0.84      3710\n",
      "          11       0.13      0.46      0.20       128\n",
      "          12       0.52      0.69      0.59       560\n",
      "          13       0.49      0.75      0.59       128\n",
      "          14       0.57      0.60      0.58       128\n",
      "          15       0.75      0.51      0.60      2662\n",
      "          16       0.67      0.84      0.74      1001\n",
      "          17       0.86      0.82      0.84       737\n",
      "          18       0.32      0.74      0.44       128\n",
      "          19       0.39      0.62      0.48       512\n",
      "\n",
      "    accuracy                           0.70     17389\n",
      "   macro avg       0.57      0.70      0.61     17389\n",
      "weighted avg       0.77      0.70      0.72     17389\n",
      "\n",
      "\n",
      "========== Training ANN_Word2Vec_300 ==========\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Iskandar\\Documents\\GitHub\\Tubes-NLP-PaperMatch\\venv\\Lib\\site-packages\\keras\\src\\layers\\core\\embedding.py:97: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "\u001b[1m1087/1087\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m398s\u001b[0m 364ms/step - accuracy: 0.3798 - loss: 2.0731 - val_accuracy: 0.5667 - val_loss: 1.3221\n",
      "Epoch 2/30\n",
      "\u001b[1m1087/1087\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m395s\u001b[0m 363ms/step - accuracy: 0.4885 - loss: 1.5520 - val_accuracy: 0.5523 - val_loss: 1.3101\n",
      "Epoch 3/30\n",
      "\u001b[1m1087/1087\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m396s\u001b[0m 365ms/step - accuracy: 0.5572 - loss: 1.2677 - val_accuracy: 0.6692 - val_loss: 1.0828\n",
      "Epoch 4/30\n",
      "\u001b[1m1087/1087\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m398s\u001b[0m 367ms/step - accuracy: 0.6042 - loss: 1.0490 - val_accuracy: 0.6374 - val_loss: 1.1361\n",
      "Epoch 5/30\n",
      "\u001b[1m1087/1087\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m396s\u001b[0m 364ms/step - accuracy: 0.6497 - loss: 0.9082 - val_accuracy: 0.6604 - val_loss: 1.0709\n",
      "Epoch 6/30\n",
      "\u001b[1m1087/1087\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m396s\u001b[0m 364ms/step - accuracy: 0.6830 - loss: 0.7836 - val_accuracy: 0.6827 - val_loss: 1.0059\n",
      "Epoch 7/30\n",
      "\u001b[1m1087/1087\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m396s\u001b[0m 365ms/step - accuracy: 0.7116 - loss: 0.7340 - val_accuracy: 0.6743 - val_loss: 1.0276\n",
      "Epoch 8/30\n",
      "\u001b[1m1087/1087\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m401s\u001b[0m 369ms/step - accuracy: 0.7363 - loss: 0.6495 - val_accuracy: 0.6913 - val_loss: 0.9967\n",
      "Epoch 9/30\n",
      "\u001b[1m1087/1087\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m399s\u001b[0m 367ms/step - accuracy: 0.7537 - loss: 0.5968 - val_accuracy: 0.7134 - val_loss: 0.9746\n",
      "Epoch 10/30\n",
      "\u001b[1m1087/1087\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m395s\u001b[0m 364ms/step - accuracy: 0.7805 - loss: 0.5244 - val_accuracy: 0.7255 - val_loss: 0.9717\n",
      "Epoch 11/30\n",
      "\u001b[1m1087/1087\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m396s\u001b[0m 364ms/step - accuracy: 0.8012 - loss: 0.4698 - val_accuracy: 0.7218 - val_loss: 1.0141\n",
      "Epoch 12/30\n",
      "\u001b[1m1087/1087\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m398s\u001b[0m 366ms/step - accuracy: 0.8158 - loss: 0.4613 - val_accuracy: 0.7297 - val_loss: 1.0231\n",
      "Epoch 13/30\n",
      "\u001b[1m1087/1087\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m398s\u001b[0m 366ms/step - accuracy: 0.8222 - loss: 0.4525 - val_accuracy: 0.7203 - val_loss: 1.0419\n",
      "Epoch 14/30\n",
      "\u001b[1m1087/1087\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m398s\u001b[0m 366ms/step - accuracy: 0.8394 - loss: 0.4119 - val_accuracy: 0.7256 - val_loss: 1.0754\n",
      "Epoch 15/30\n",
      "\u001b[1m1087/1087\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m396s\u001b[0m 365ms/step - accuracy: 0.8522 - loss: 0.3864 - val_accuracy: 0.7293 - val_loss: 1.0571\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved to models_baru/ANN_Word2Vec_300.h5\n",
      "\u001b[1m544/544\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 8ms/step\n",
      "Accuracy: 0.7255\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.84      0.85      0.85       894\n",
      "           1       0.73      0.66      0.70      1151\n",
      "           2       0.84      0.74      0.79      4462\n",
      "           3       0.15      0.06      0.09        68\n",
      "           4       0.33      0.55      0.42       480\n",
      "           5       0.45      0.72      0.55       128\n",
      "           6       0.63      0.77      0.70       128\n",
      "           7       0.71      0.81      0.76       128\n",
      "           8       0.68      0.38      0.48       128\n",
      "           9       0.58      0.41      0.48       128\n",
      "          10       0.88      0.84      0.86      3710\n",
      "          11       0.09      0.01      0.01       128\n",
      "          12       0.47      0.65      0.54       560\n",
      "          13       0.56      0.59      0.57       128\n",
      "          14       0.54      0.48      0.51       128\n",
      "          15       0.64      0.64      0.64      2662\n",
      "          16       0.75      0.74      0.75      1001\n",
      "          17       0.86      0.89      0.88       737\n",
      "          18       0.39      0.51      0.44       128\n",
      "          19       0.38      0.64      0.48       512\n",
      "\n",
      "    accuracy                           0.73     17389\n",
      "   macro avg       0.58      0.60      0.57     17389\n",
      "weighted avg       0.74      0.73      0.73     17389\n",
      "\n",
      "\n",
      "========== Training CNN_GloVe_100 ==========\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Iskandar\\Documents\\GitHub\\Tubes-NLP-PaperMatch\\venv\\Lib\\site-packages\\keras\\src\\layers\\core\\embedding.py:97: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "\u001b[1m1087/1087\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m107s\u001b[0m 97ms/step - accuracy: 0.3340 - loss: 2.0145 - val_accuracy: 0.5065 - val_loss: 1.4144\n",
      "Epoch 2/30\n",
      "\u001b[1m1087/1087\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m106s\u001b[0m 98ms/step - accuracy: 0.5487 - loss: 1.2556 - val_accuracy: 0.5597 - val_loss: 1.2898\n",
      "Epoch 3/30\n",
      "\u001b[1m1087/1087\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m106s\u001b[0m 98ms/step - accuracy: 0.6222 - loss: 0.9595 - val_accuracy: 0.6226 - val_loss: 1.1366\n",
      "Epoch 4/30\n",
      "\u001b[1m1087/1087\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m108s\u001b[0m 99ms/step - accuracy: 0.6715 - loss: 0.7584 - val_accuracy: 0.6605 - val_loss: 1.0602\n",
      "Epoch 5/30\n",
      "\u001b[1m1087/1087\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m107s\u001b[0m 99ms/step - accuracy: 0.7175 - loss: 0.5839 - val_accuracy: 0.6507 - val_loss: 1.1032\n",
      "Epoch 6/30\n",
      "\u001b[1m1087/1087\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m107s\u001b[0m 99ms/step - accuracy: 0.7527 - loss: 0.4687 - val_accuracy: 0.6767 - val_loss: 1.0605\n",
      "Epoch 7/30\n",
      "\u001b[1m1087/1087\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m107s\u001b[0m 99ms/step - accuracy: 0.7828 - loss: 0.3755 - val_accuracy: 0.6501 - val_loss: 1.2387\n",
      "Epoch 8/30\n",
      "\u001b[1m1087/1087\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m107s\u001b[0m 99ms/step - accuracy: 0.8052 - loss: 0.3302 - val_accuracy: 0.6826 - val_loss: 1.1613\n",
      "Epoch 9/30\n",
      "\u001b[1m1087/1087\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m107s\u001b[0m 98ms/step - accuracy: 0.8267 - loss: 0.2870 - val_accuracy: 0.6812 - val_loss: 1.2410\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved to models_baru/CNN_GloVe_100.h5\n",
      "\u001b[1m544/544\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 5ms/step\n",
      "Accuracy: 0.6605\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.72      0.91      0.81       894\n",
      "           1       0.75      0.53      0.62      1151\n",
      "           2       0.82      0.69      0.75      4462\n",
      "           3       0.13      0.38      0.20        68\n",
      "           4       0.26      0.62      0.37       480\n",
      "           5       0.38      0.81      0.52       128\n",
      "           6       0.71      0.70      0.70       128\n",
      "           7       0.72      0.84      0.78       128\n",
      "           8       0.57      0.66      0.61       128\n",
      "           9       0.43      0.64      0.51       128\n",
      "          10       0.92      0.74      0.82      3710\n",
      "          11       0.11      0.23      0.15       128\n",
      "          12       0.48      0.61      0.54       560\n",
      "          13       0.41      0.80      0.54       128\n",
      "          14       0.42      0.53      0.47       128\n",
      "          15       0.77      0.37      0.50      2662\n",
      "          16       0.60      0.80      0.69      1001\n",
      "          17       0.82      0.87      0.85       737\n",
      "          18       0.37      0.71      0.49       128\n",
      "          19       0.26      0.74      0.39       512\n",
      "\n",
      "    accuracy                           0.66     17389\n",
      "   macro avg       0.53      0.66      0.56     17389\n",
      "weighted avg       0.74      0.66      0.68     17389\n",
      "\n",
      "\n",
      "========== Training RNN_GloVe_100 ==========\n",
      "Epoch 1/30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Iskandar\\Documents\\GitHub\\Tubes-NLP-PaperMatch\\venv\\Lib\\site-packages\\keras\\src\\layers\\core\\embedding.py:97: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1087/1087\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m130s\u001b[0m 118ms/step - accuracy: 0.1093 - loss: 2.8219 - val_accuracy: 0.1165 - val_loss: 2.6403\n",
      "Epoch 2/30\n",
      "\u001b[1m1087/1087\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m139s\u001b[0m 128ms/step - accuracy: 0.1863 - loss: 2.4998 - val_accuracy: 0.2453 - val_loss: 2.2958\n",
      "Epoch 3/30\n",
      "\u001b[1m1087/1087\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m141s\u001b[0m 129ms/step - accuracy: 0.1478 - loss: 2.5509 - val_accuracy: 0.0649 - val_loss: 2.8252\n",
      "Epoch 4/30\n",
      "\u001b[1m1087/1087\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m142s\u001b[0m 131ms/step - accuracy: 0.0641 - loss: 2.8219 - val_accuracy: 0.1175 - val_loss: 2.7337\n",
      "Epoch 5/30\n",
      "\u001b[1m1087/1087\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m142s\u001b[0m 131ms/step - accuracy: 0.0832 - loss: 2.6147 - val_accuracy: 0.1056 - val_loss: 2.7437\n",
      "Epoch 6/30\n",
      "\u001b[1m1087/1087\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m142s\u001b[0m 131ms/step - accuracy: 0.0878 - loss: 2.5924 - val_accuracy: 0.1378 - val_loss: 2.5262\n",
      "Epoch 7/30\n",
      "\u001b[1m1087/1087\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m142s\u001b[0m 131ms/step - accuracy: 0.1693 - loss: 2.3576 - val_accuracy: 0.2051 - val_loss: 2.3492\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved to models_baru/RNN_GloVe_100.h5\n",
      "\u001b[1m544/544\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 15ms/step\n",
      "Accuracy: 0.2453\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.21      0.14      0.16       894\n",
      "           1       0.33      0.03      0.06      1151\n",
      "           2       0.55      0.04      0.08      4462\n",
      "           3       0.01      0.38      0.02        68\n",
      "           4       0.10      0.22      0.13       480\n",
      "           5       0.04      0.03      0.04       128\n",
      "           6       0.09      0.20      0.13       128\n",
      "           7       0.07      0.09      0.08       128\n",
      "           8       0.03      0.11      0.05       128\n",
      "           9       0.09      0.22      0.13       128\n",
      "          10       0.67      0.71      0.69      3710\n",
      "          11       0.02      0.02      0.02       128\n",
      "          12       0.14      0.03      0.05       560\n",
      "          13       0.04      0.64      0.07       128\n",
      "          14       0.02      0.05      0.03       128\n",
      "          15       0.32      0.03      0.06      2662\n",
      "          16       0.38      0.34      0.36      1001\n",
      "          17       0.27      0.47      0.34       737\n",
      "          18       0.08      0.30      0.12       128\n",
      "          19       0.07      0.29      0.11       512\n",
      "\n",
      "    accuracy                           0.25     17389\n",
      "   macro avg       0.18      0.22      0.14     17389\n",
      "weighted avg       0.41      0.25      0.24     17389\n",
      "\n",
      "\n",
      "========== Training LSTM_GloVe_100 ==========\n",
      "Epoch 1/30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Iskandar\\Documents\\GitHub\\Tubes-NLP-PaperMatch\\venv\\Lib\\site-packages\\keras\\src\\layers\\core\\embedding.py:97: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1087/1087\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m228s\u001b[0m 204ms/step - accuracy: 0.3278 - loss: 2.1327 - val_accuracy: 0.4860 - val_loss: 1.4828\n",
      "Epoch 2/30\n",
      "\u001b[1m1087/1087\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m246s\u001b[0m 226ms/step - accuracy: 0.5503 - loss: 1.3241 - val_accuracy: 0.6459 - val_loss: 1.1264\n",
      "Epoch 3/30\n",
      "\u001b[1m1087/1087\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m229s\u001b[0m 210ms/step - accuracy: 0.6692 - loss: 0.9347 - val_accuracy: 0.6868 - val_loss: 0.9957\n",
      "Epoch 4/30\n",
      "\u001b[1m1087/1087\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m230s\u001b[0m 211ms/step - accuracy: 0.7489 - loss: 0.6776 - val_accuracy: 0.6843 - val_loss: 0.9978\n",
      "Epoch 5/30\n",
      "\u001b[1m1087/1087\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m211s\u001b[0m 194ms/step - accuracy: 0.8017 - loss: 0.4945 - val_accuracy: 0.7153 - val_loss: 1.0087\n",
      "Epoch 6/30\n",
      "\u001b[1m1087/1087\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m230s\u001b[0m 211ms/step - accuracy: 0.8431 - loss: 0.3691 - val_accuracy: 0.7157 - val_loss: 1.0419\n",
      "Epoch 7/30\n",
      "\u001b[1m1087/1087\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m213s\u001b[0m 196ms/step - accuracy: 0.8698 - loss: 0.2836 - val_accuracy: 0.7002 - val_loss: 1.1426\n",
      "Epoch 8/30\n",
      "\u001b[1m1087/1087\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m221s\u001b[0m 203ms/step - accuracy: 0.8926 - loss: 0.2234 - val_accuracy: 0.7075 - val_loss: 1.2942\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved to models_baru/LSTM_GloVe_100.h5\n",
      "\u001b[1m544/544\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 27ms/step\n",
      "Accuracy: 0.6868\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.83      0.87      0.85       894\n",
      "           1       0.68      0.66      0.67      1151\n",
      "           2       0.86      0.69      0.76      4462\n",
      "           3       0.08      0.66      0.14        68\n",
      "           4       0.32      0.67      0.44       480\n",
      "           5       0.42      0.75      0.54       128\n",
      "           6       0.63      0.76      0.69       128\n",
      "           7       0.73      0.87      0.79       128\n",
      "           8       0.54      0.58      0.56       128\n",
      "           9       0.57      0.67      0.61       128\n",
      "          10       0.90      0.80      0.85      3710\n",
      "          11       0.14      0.42      0.21       128\n",
      "          12       0.46      0.70      0.56       560\n",
      "          13       0.50      0.49      0.50       128\n",
      "          14       0.49      0.55      0.52       128\n",
      "          15       0.81      0.41      0.55      2662\n",
      "          16       0.69      0.81      0.75      1001\n",
      "          17       0.84      0.89      0.87       737\n",
      "          18       0.24      0.80      0.36       128\n",
      "          19       0.35      0.55      0.43       512\n",
      "\n",
      "    accuracy                           0.69     17389\n",
      "   macro avg       0.55      0.68      0.58     17389\n",
      "weighted avg       0.76      0.69      0.71     17389\n",
      "\n",
      "\n",
      "========== Training ANN_GloVe_100 ==========\n",
      "Epoch 1/30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Iskandar\\Documents\\GitHub\\Tubes-NLP-PaperMatch\\venv\\Lib\\site-packages\\keras\\src\\layers\\core\\embedding.py:97: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1087/1087\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m137s\u001b[0m 125ms/step - accuracy: 0.2216 - loss: 2.4829 - val_accuracy: 0.3376 - val_loss: 1.8269\n",
      "Epoch 2/30\n",
      "\u001b[1m1087/1087\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m138s\u001b[0m 127ms/step - accuracy: 0.3402 - loss: 1.8649 - val_accuracy: 0.5115 - val_loss: 1.4365\n",
      "Epoch 3/30\n",
      "\u001b[1m1087/1087\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m138s\u001b[0m 127ms/step - accuracy: 0.4215 - loss: 1.5468 - val_accuracy: 0.5521 - val_loss: 1.3405\n",
      "Epoch 4/30\n",
      "\u001b[1m1087/1087\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m139s\u001b[0m 128ms/step - accuracy: 0.4853 - loss: 1.2891 - val_accuracy: 0.5740 - val_loss: 1.1994\n",
      "Epoch 5/30\n",
      "\u001b[1m1087/1087\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m137s\u001b[0m 126ms/step - accuracy: 0.5408 - loss: 1.1022 - val_accuracy: 0.6206 - val_loss: 1.1096\n",
      "Epoch 6/30\n",
      "\u001b[1m1087/1087\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m137s\u001b[0m 126ms/step - accuracy: 0.5773 - loss: 0.9670 - val_accuracy: 0.6236 - val_loss: 1.0826\n",
      "Epoch 7/30\n",
      "\u001b[1m1087/1087\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m136s\u001b[0m 126ms/step - accuracy: 0.6187 - loss: 0.8417 - val_accuracy: 0.6312 - val_loss: 1.0840\n",
      "Epoch 8/30\n",
      "\u001b[1m1087/1087\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m136s\u001b[0m 125ms/step - accuracy: 0.6623 - loss: 0.7251 - val_accuracy: 0.6314 - val_loss: 1.0918\n",
      "Epoch 9/30\n",
      "\u001b[1m1087/1087\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m138s\u001b[0m 127ms/step - accuracy: 0.6890 - loss: 0.6382 - val_accuracy: 0.6663 - val_loss: 1.0101\n",
      "Epoch 10/30\n",
      "\u001b[1m1087/1087\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m136s\u001b[0m 125ms/step - accuracy: 0.7207 - loss: 0.5753 - val_accuracy: 0.6846 - val_loss: 0.9898\n",
      "Epoch 11/30\n",
      "\u001b[1m1087/1087\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m137s\u001b[0m 126ms/step - accuracy: 0.7397 - loss: 0.5209 - val_accuracy: 0.6764 - val_loss: 1.0415\n",
      "Epoch 12/30\n",
      "\u001b[1m1087/1087\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m136s\u001b[0m 125ms/step - accuracy: 0.7606 - loss: 0.4827 - val_accuracy: 0.6960 - val_loss: 0.9940\n",
      "Epoch 13/30\n",
      "\u001b[1m1087/1087\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m137s\u001b[0m 126ms/step - accuracy: 0.7819 - loss: 0.4281 - val_accuracy: 0.6881 - val_loss: 1.0292\n",
      "Epoch 14/30\n",
      "\u001b[1m1087/1087\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m137s\u001b[0m 126ms/step - accuracy: 0.7918 - loss: 0.4202 - val_accuracy: 0.7018 - val_loss: 1.0481\n",
      "Epoch 15/30\n",
      "\u001b[1m1087/1087\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m138s\u001b[0m 127ms/step - accuracy: 0.8136 - loss: 0.3511 - val_accuracy: 0.7089 - val_loss: 1.0406\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved to models_baru/ANN_GloVe_100.h5\n",
      "\u001b[1m544/544\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step\n",
      "Accuracy: 0.6846\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.82      0.85      0.83       894\n",
      "           1       0.56      0.75      0.64      1151\n",
      "           2       0.85      0.66      0.75      4462\n",
      "           3       0.42      0.16      0.23        68\n",
      "           4       0.27      0.58      0.37       480\n",
      "           5       0.44      0.62      0.52       128\n",
      "           6       0.76      0.66      0.70       128\n",
      "           7       0.81      0.68      0.74       128\n",
      "           8       0.54      0.49      0.52       128\n",
      "           9       0.45      0.55      0.50       128\n",
      "          10       0.87      0.83      0.85      3710\n",
      "          11       0.18      0.19      0.18       128\n",
      "          12       0.36      0.65      0.46       560\n",
      "          13       0.43      0.68      0.53       128\n",
      "          14       0.50      0.52      0.51       128\n",
      "          15       0.65      0.47      0.55      2662\n",
      "          16       0.75      0.73      0.74      1001\n",
      "          17       0.84      0.88      0.86       737\n",
      "          18       0.33      0.62      0.43       128\n",
      "          19       0.39      0.60      0.47       512\n",
      "\n",
      "    accuracy                           0.68     17389\n",
      "   macro avg       0.56      0.61      0.57     17389\n",
      "weighted avg       0.73      0.68      0.70     17389\n",
      "\n",
      "\n",
      "========================================\n",
      "FINAL RESULTS SUMMARY\n",
      "========================================\n",
      "ANN_Word2Vec_300: 0.7255\n",
      "LSTM_Word2Vec_300: 0.7041\n",
      "CNN_Word2Vec_300: 0.6875\n",
      "LSTM_GloVe_100: 0.6868\n",
      "ANN_GloVe_100: 0.6846\n",
      "CNN_GloVe_100: 0.6605\n",
      "RNN_Word2Vec_300: 0.3639\n",
      "RNN_GloVe_100: 0.2453\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, Conv1D, GlobalMaxPooling1D, LSTM, SimpleRNN, Dense, Dropout, Bidirectional, Flatten\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "\n",
    "if not os.path.exists(\"models_baru\"):\n",
    "    os.makedirs(\"models_baru\")\n",
    "\n",
    "class_weights = compute_class_weight(\n",
    "    class_weight='balanced',\n",
    "    classes=np.unique(y_train),\n",
    "    y=y_train\n",
    ")\n",
    "\n",
    "class_weights_dict = dict(enumerate(class_weights))\n",
    "print(\"Class weights dictionary:\", class_weights_dict)\n",
    "\n",
    "# Dictionary matriks embedding untuk loop\n",
    "embedding_dict = {\n",
    "    \"Word2Vec_300\": matrix_w2v,\n",
    "    # \"FastText_Custom_300\": matrix_ft_custom,\n",
    "    # \"FastText_Pre_300\": matrix_ft_pre,\n",
    "    \"GloVe_100\": matrix_glove\n",
    "}\n",
    "\n",
    "def build_cnn(embedding_matrix, max_len, num_classes):\n",
    "    model = Sequential()\n",
    "    model.add(Embedding(input_dim=embedding_matrix.shape[0],\n",
    "                        output_dim=embedding_matrix.shape[1],\n",
    "                        weights=[embedding_matrix],\n",
    "                        input_length=max_len,\n",
    "                        trainable=True))\n",
    "    model.add(Conv1D(128, 5, activation='relu'))\n",
    "    model.add(Conv1D(64, 3, activation='relu'))\n",
    "    model.add(GlobalMaxPooling1D())\n",
    "    model.add(Dense(128, activation='relu'))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(64, activation='relu'))\n",
    "    model.add(Dense(num_classes, activation='softmax'))\n",
    "    model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "def build_rnn(embedding_matrix, max_len, num_classes):\n",
    "    model = Sequential()\n",
    "    model.add(Embedding(input_dim=embedding_matrix.shape[0],\n",
    "                        output_dim=embedding_matrix.shape[1],\n",
    "                        weights=[embedding_matrix],\n",
    "                        input_length=max_len,\n",
    "                        trainable=True))\n",
    "    model.add(Bidirectional(SimpleRNN(128, return_sequences=False)))\n",
    "    model.add(Dense(64, activation='relu'))\n",
    "    model.add(Dropout(0.3))\n",
    "    model.add(Dense(num_classes, activation='softmax'))\n",
    "    model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "def build_lstm(embedding_matrix, max_len, num_classes):\n",
    "    model = Sequential()\n",
    "    model.add(Embedding(input_dim=embedding_matrix.shape[0],\n",
    "                        output_dim=embedding_matrix.shape[1],\n",
    "                        weights=[embedding_matrix],\n",
    "                        input_length=max_len,\n",
    "                        trainable=True))\n",
    "    model.add(Bidirectional(LSTM(64, return_sequences=False))) \n",
    "    model.add(Dense(64, activation='relu'))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(num_classes, activation='softmax'))\n",
    "    model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "def build_ann(embedding_matrix, max_len, num_classes):\n",
    "    model = Sequential()\n",
    "    model.add(Embedding(input_dim=embedding_matrix.shape[0],\n",
    "                        output_dim=embedding_matrix.shape[1],\n",
    "                        weights=[embedding_matrix],\n",
    "                        input_length=max_len,\n",
    "                        trainable=True))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(256, activation='relu'))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(128, activation='relu'))\n",
    "    model.add(Dropout(0.3))\n",
    "    model.add(Dense(64, activation='relu'))\n",
    "    model.add(Dense(num_classes, activation='softmax'))\n",
    "    model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy']) \n",
    "    return model\n",
    "\n",
    "model_functions = {\n",
    "    \"CNN\": build_cnn,\n",
    "    \"RNN\": build_rnn,\n",
    "    \"LSTM\": build_lstm,\n",
    "    \"ANN\": build_ann\n",
    "}\n",
    "\n",
    "results = {}\n",
    "\n",
    "try:\n",
    "    target_names = dataset_train_final.features['label'].names\n",
    "except:\n",
    "    target_names = [str(i) for i in range(num_classes)]\n",
    "\n",
    "print('target names:', target_names)\n",
    "\n",
    "print(f\"Mulai Training Loop untuk {len(embedding_dict)} Embeddings x {len(model_functions)} Models...\")\n",
    "\n",
    "for emb_name, emb_matrix in embedding_dict.items():\n",
    "    for model_name, model_func in model_functions.items():\n",
    "\n",
    "        early_stop = EarlyStopping(\n",
    "            monitor='val_loss',\n",
    "            patience=5,\n",
    "            restore_best_weights=True\n",
    "        )\n",
    "        \n",
    "        full_name = f\"{model_name}_{emb_name}\"\n",
    "        print(f\"\\n{'='*10} Training {full_name} {'='*10}\")\n",
    "        \n",
    "        # Build Model\n",
    "        model = model_func(emb_matrix, max_len, num_classes)\n",
    "        \n",
    "        # Train\n",
    "        history = model.fit(\n",
    "            X_train_pad, y_train,\n",
    "            epochs=30, \n",
    "            batch_size=64,\n",
    "            validation_data=(X_val_pad, y_val),\n",
    "            class_weight=class_weights_dict,\n",
    "            callbacks=[early_stop],\n",
    "            verbose=1 \n",
    "        )\n",
    "        \n",
    "        # Save Model\n",
    "        model_path = f\"models_baru/{full_name}.h5\"\n",
    "        model.save(model_path)\n",
    "        print(f\"Model saved to {model_path}\")\n",
    "\n",
    "        # Predict & Evaluate\n",
    "        y_pred_prob = model.predict(X_val_pad)\n",
    "        y_pred = np.argmax(y_pred_prob, axis=1)\n",
    "\n",
    "        acc = accuracy_score(y_val, y_pred)\n",
    "        report = classification_report(\n",
    "            y_val, \n",
    "            y_pred, \n",
    "            labels=range(len(target_names)),\n",
    "            target_names=target_names, \n",
    "            zero_division=0\n",
    "        )\n",
    "\n",
    "        results[full_name] = {\"accuracy\": acc, \"report\": report}\n",
    "        print(f\"Accuracy: {acc:.4f}\")\n",
    "        print(report)\n",
    "\n",
    "print(\"\\n\" + \"=\"*40)\n",
    "print(\"FINAL RESULTS SUMMARY\")\n",
    "print(\"=\"*40)\n",
    "sorted_results = sorted(results.items(), key=lambda x: x[1]['accuracy'], reverse=True)\n",
    "\n",
    "for name, res in sorted_results:\n",
    "    print(f\"{name}: {res['accuracy']:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "a467b2d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing saved models...\n",
      "Loading ANN_FastText_Custom_300.h5 ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m680/680\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 8ms/step\n",
      "Loaded model ANN_FastText_Custom_300.h5 accuracy: 0.6978744939271255\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.85      0.81      0.83      1117\n",
      "           1       0.60      0.72      0.66      1439\n",
      "           2       0.84      0.72      0.78      5578\n",
      "           3       0.33      0.06      0.10        86\n",
      "           4       0.35      0.52      0.42       599\n",
      "           5       0.42      0.59      0.49       160\n",
      "           6       0.61      0.82      0.70       160\n",
      "           7       0.65      0.79      0.71       160\n",
      "           8       0.57      0.34      0.42       160\n",
      "           9       0.41      0.68      0.51       160\n",
      "          10       0.89      0.81      0.84      4637\n",
      "          11       0.13      0.16      0.15       160\n",
      "          12       0.41      0.58      0.48       700\n",
      "          13       0.47      0.56      0.51       160\n",
      "          14       0.42      0.41      0.41       160\n",
      "          15       0.65      0.54      0.59      3327\n",
      "          16       0.63      0.76      0.69      1252\n",
      "          17       0.83      0.89      0.86       921\n",
      "          18       0.49      0.33      0.39       160\n",
      "          19       0.37      0.68      0.48       640\n",
      "\n",
      "    accuracy                           0.70     21736\n",
      "   macro avg       0.55      0.59      0.55     21736\n",
      "weighted avg       0.73      0.70      0.71     21736\n",
      "\n",
      "==================================================\n",
      "Loading ANN_FastText_Pre_300.h5 ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m680/680\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 8ms/step\n",
      "Loaded model ANN_FastText_Pre_300.h5 accuracy: 0.6525579683474421\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.83      0.83      0.83      1117\n",
      "           1       0.63      0.75      0.68      1439\n",
      "           2       0.87      0.59      0.71      5578\n",
      "           3       0.25      0.31      0.28        86\n",
      "           4       0.26      0.64      0.37       599\n",
      "           5       0.40      0.69      0.50       160\n",
      "           6       0.74      0.72      0.73       160\n",
      "           7       0.77      0.79      0.78       160\n",
      "           8       0.58      0.56      0.57       160\n",
      "           9       0.55      0.59      0.57       160\n",
      "          10       0.90      0.73      0.81      4637\n",
      "          11       0.09      0.52      0.15       160\n",
      "          12       0.36      0.69      0.48       700\n",
      "          13       0.33      0.73      0.45       160\n",
      "          14       0.49      0.53      0.50       160\n",
      "          15       0.74      0.44      0.55      3327\n",
      "          16       0.67      0.77      0.72      1252\n",
      "          17       0.81      0.90      0.85       921\n",
      "          18       0.33      0.53      0.41       160\n",
      "          19       0.33      0.67      0.44       640\n",
      "\n",
      "    accuracy                           0.65     21736\n",
      "   macro avg       0.55      0.65      0.57     21736\n",
      "weighted avg       0.75      0.65      0.68     21736\n",
      "\n",
      "==================================================\n",
      "Loading ANN_GloVe_100.h5 ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m680/680\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step\n",
      "Loaded model ANN_GloVe_100.h5 accuracy: 0.6800699300699301\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.81      0.84      0.82      1117\n",
      "           1       0.56      0.76      0.64      1439\n",
      "           2       0.85      0.65      0.74      5578\n",
      "           3       0.31      0.16      0.21        86\n",
      "           4       0.27      0.59      0.37       599\n",
      "           5       0.46      0.67      0.54       160\n",
      "           6       0.80      0.67      0.73       160\n",
      "           7       0.80      0.66      0.72       160\n",
      "           8       0.57      0.52      0.54       160\n",
      "           9       0.52      0.66      0.58       160\n",
      "          10       0.86      0.83      0.85      4637\n",
      "          11       0.19      0.17      0.18       160\n",
      "          12       0.36      0.65      0.46       700\n",
      "          13       0.43      0.68      0.53       160\n",
      "          14       0.49      0.49      0.49       160\n",
      "          15       0.66      0.47      0.55      3327\n",
      "          16       0.73      0.71      0.72      1252\n",
      "          17       0.86      0.88      0.87       921\n",
      "          18       0.30      0.53      0.38       160\n",
      "          19       0.37      0.59      0.46       640\n",
      "\n",
      "    accuracy                           0.68     21736\n",
      "   macro avg       0.56      0.61      0.57     21736\n",
      "weighted avg       0.72      0.68      0.69     21736\n",
      "\n",
      "==================================================\n",
      "Loading ANN_Word2Vec_300.h5 ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m680/680\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 9ms/step\n",
      "Loaded model ANN_Word2Vec_300.h5 accuracy: 0.7188535149061465\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.84      0.84      0.84      1117\n",
      "           1       0.73      0.63      0.68      1439\n",
      "           2       0.84      0.74      0.79      5578\n",
      "           3       0.12      0.03      0.05        86\n",
      "           4       0.34      0.54      0.41       599\n",
      "           5       0.42      0.66      0.51       160\n",
      "           6       0.61      0.76      0.68       160\n",
      "           7       0.74      0.78      0.76       160\n",
      "           8       0.64      0.43      0.52       160\n",
      "           9       0.58      0.44      0.50       160\n",
      "          10       0.88      0.84      0.86      4637\n",
      "          11       0.33      0.02      0.04       160\n",
      "          12       0.44      0.67      0.53       700\n",
      "          13       0.52      0.49      0.50       160\n",
      "          14       0.53      0.49      0.51       160\n",
      "          15       0.63      0.63      0.63      3327\n",
      "          16       0.73      0.70      0.72      1252\n",
      "          17       0.85      0.88      0.86       921\n",
      "          18       0.42      0.46      0.44       160\n",
      "          19       0.39      0.68      0.50       640\n",
      "\n",
      "    accuracy                           0.72     21736\n",
      "   macro avg       0.58      0.59      0.57     21736\n",
      "weighted avg       0.74      0.72      0.72     21736\n",
      "\n",
      "==================================================\n",
      "Loading CNN_FastText_Custom_300.h5 ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m680/680\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 9ms/step\n",
      "Loaded model CNN_FastText_Custom_300.h5 accuracy: 0.6740430622009569\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.81      0.84      0.83      1117\n",
      "           1       0.63      0.72      0.67      1439\n",
      "           2       0.86      0.65      0.74      5578\n",
      "           3       0.14      0.38      0.21        86\n",
      "           4       0.28      0.67      0.39       599\n",
      "           5       0.38      0.74      0.50       160\n",
      "           6       0.53      0.87      0.66       160\n",
      "           7       0.69      0.88      0.78       160\n",
      "           8       0.56      0.55      0.56       160\n",
      "           9       0.57      0.57      0.57       160\n",
      "          10       0.90      0.78      0.84      4637\n",
      "          11       0.14      0.39      0.21       160\n",
      "          12       0.47      0.69      0.56       700\n",
      "          13       0.39      0.54      0.46       160\n",
      "          14       0.44      0.59      0.51       160\n",
      "          15       0.71      0.42      0.53      3327\n",
      "          16       0.68      0.76      0.71      1252\n",
      "          17       0.72      0.92      0.81       921\n",
      "          18       0.28      0.69      0.40       160\n",
      "          19       0.37      0.62      0.46       640\n",
      "\n",
      "    accuracy                           0.67     21736\n",
      "   macro avg       0.53      0.66      0.57     21736\n",
      "weighted avg       0.74      0.67      0.69     21736\n",
      "\n",
      "==================================================\n",
      "Loading CNN_FastText_Pre_300.h5 ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m680/680\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 9ms/step\n",
      "Loaded model CNN_FastText_Pre_300.h5 accuracy: 0.6658538829591462\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.87      0.83      0.85      1117\n",
      "           1       0.65      0.68      0.67      1439\n",
      "           2       0.84      0.64      0.73      5578\n",
      "           3       0.16      0.44      0.24        86\n",
      "           4       0.26      0.62      0.36       599\n",
      "           5       0.50      0.59      0.54       160\n",
      "           6       0.78      0.61      0.69       160\n",
      "           7       0.76      0.87      0.81       160\n",
      "           8       0.48      0.71      0.57       160\n",
      "           9       0.59      0.49      0.54       160\n",
      "          10       0.90      0.75      0.82      4637\n",
      "          11       0.11      0.44      0.18       160\n",
      "          12       0.46      0.70      0.56       700\n",
      "          13       0.38      0.82      0.52       160\n",
      "          14       0.39      0.51      0.44       160\n",
      "          15       0.77      0.45      0.57      3327\n",
      "          16       0.64      0.81      0.71      1252\n",
      "          17       0.86      0.84      0.85       921\n",
      "          18       0.22      0.68      0.33       160\n",
      "          19       0.31      0.65      0.42       640\n",
      "\n",
      "    accuracy                           0.67     21736\n",
      "   macro avg       0.55      0.66      0.57     21736\n",
      "weighted avg       0.75      0.67      0.69     21736\n",
      "\n",
      "==================================================\n",
      "Loading CNN_GloVe_100.h5 ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m680/680\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 6ms/step\n",
      "Loaded model CNN_GloVe_100.h5 accuracy: 0.6561924917188076\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.73      0.89      0.80      1117\n",
      "           1       0.75      0.54      0.63      1439\n",
      "           2       0.82      0.68      0.74      5578\n",
      "           3       0.12      0.37      0.18        86\n",
      "           4       0.26      0.61      0.37       599\n",
      "           5       0.37      0.78      0.50       160\n",
      "           6       0.74      0.68      0.71       160\n",
      "           7       0.73      0.81      0.77       160\n",
      "           8       0.53      0.61      0.56       160\n",
      "           9       0.40      0.65      0.50       160\n",
      "          10       0.92      0.74      0.82      4637\n",
      "          11       0.13      0.28      0.18       160\n",
      "          12       0.48      0.65      0.56       700\n",
      "          13       0.40      0.78      0.53       160\n",
      "          14       0.40      0.56      0.47       160\n",
      "          15       0.75      0.37      0.49      3327\n",
      "          16       0.60      0.78      0.68      1252\n",
      "          17       0.81      0.86      0.84       921\n",
      "          18       0.32      0.58      0.41       160\n",
      "          19       0.26      0.76      0.39       640\n",
      "\n",
      "    accuracy                           0.66     21736\n",
      "   macro avg       0.53      0.65      0.56     21736\n",
      "weighted avg       0.74      0.66      0.67     21736\n",
      "\n",
      "==================================================\n",
      "Loading CNN_Word2Vec_300.h5 ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m680/680\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 9ms/step\n",
      "Loaded model CNN_Word2Vec_300.h5 accuracy: 0.6799779168200221\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.79      0.87      0.83      1117\n",
      "           1       0.63      0.74      0.68      1439\n",
      "           2       0.85      0.69      0.76      5578\n",
      "           3       0.22      0.38      0.28        86\n",
      "           4       0.31      0.62      0.41       599\n",
      "           5       0.42      0.69      0.52       160\n",
      "           6       0.51      0.81      0.63       160\n",
      "           7       0.80      0.82      0.81       160\n",
      "           8       0.55      0.62      0.58       160\n",
      "           9       0.45      0.69      0.54       160\n",
      "          10       0.88      0.77      0.82      4637\n",
      "          11       0.10      0.41      0.17       160\n",
      "          12       0.41      0.76      0.53       700\n",
      "          13       0.46      0.59      0.52       160\n",
      "          14       0.54      0.64      0.58       160\n",
      "          15       0.78      0.37      0.50      3327\n",
      "          16       0.68      0.78      0.73      1252\n",
      "          17       0.88      0.89      0.89       921\n",
      "          18       0.34      0.62      0.44       160\n",
      "          19       0.34      0.64      0.44       640\n",
      "\n",
      "    accuracy                           0.68     21736\n",
      "   macro avg       0.55      0.67      0.58     21736\n",
      "weighted avg       0.75      0.68      0.69     21736\n",
      "\n",
      "==================================================\n",
      "Loading LSTM_FastText_Custom_300.h5 ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m680/680\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m29s\u001b[0m 42ms/step\n",
      "Loaded model LSTM_FastText_Custom_300.h5 accuracy: 0.6886271623113729\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.84      0.84      0.84      1117\n",
      "           1       0.66      0.72      0.69      1439\n",
      "           2       0.85      0.68      0.76      5578\n",
      "           3       0.16      0.38      0.22        86\n",
      "           4       0.31      0.65      0.42       599\n",
      "           5       0.43      0.74      0.55       160\n",
      "           6       0.69      0.77      0.73       160\n",
      "           7       0.70      0.90      0.79       160\n",
      "           8       0.69      0.52      0.59       160\n",
      "           9       0.58      0.64      0.61       160\n",
      "          10       0.93      0.73      0.82      4637\n",
      "          11       0.12      0.51      0.19       160\n",
      "          12       0.48      0.66      0.56       700\n",
      "          13       0.43      0.62      0.51       160\n",
      "          14       0.47      0.71      0.56       160\n",
      "          15       0.73      0.50      0.60      3327\n",
      "          16       0.65      0.80      0.72      1252\n",
      "          17       0.84      0.88      0.86       921\n",
      "          18       0.32      0.63      0.42       160\n",
      "          19       0.36      0.69      0.47       640\n",
      "\n",
      "    accuracy                           0.69     21736\n",
      "   macro avg       0.56      0.68      0.59     21736\n",
      "weighted avg       0.76      0.69      0.71     21736\n",
      "\n",
      "==================================================\n",
      "Loading LSTM_FastText_Pre_300.h5 ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m680/680\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 40ms/step\n",
      "Loaded model LSTM_FastText_Pre_300.h5 accuracy: 0.6619433198380567\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.83      0.86      0.85      1117\n",
      "           1       0.61      0.72      0.66      1439\n",
      "           2       0.85      0.67      0.75      5578\n",
      "           3       0.14      0.64      0.23        86\n",
      "           4       0.26      0.67      0.37       599\n",
      "           5       0.41      0.68      0.52       160\n",
      "           6       0.60      0.84      0.70       160\n",
      "           7       0.80      0.79      0.80       160\n",
      "           8       0.52      0.50      0.51       160\n",
      "           9       0.56      0.60      0.58       160\n",
      "          10       0.93      0.68      0.78      4637\n",
      "          11       0.08      0.49      0.14       160\n",
      "          12       0.47      0.68      0.55       700\n",
      "          13       0.41      0.69      0.52       160\n",
      "          14       0.52      0.53      0.52       160\n",
      "          15       0.77      0.46      0.58      3327\n",
      "          16       0.67      0.79      0.72      1252\n",
      "          17       0.91      0.79      0.85       921\n",
      "          18       0.25      0.66      0.37       160\n",
      "          19       0.36      0.57      0.44       640\n",
      "\n",
      "    accuracy                           0.66     21736\n",
      "   macro avg       0.55      0.67      0.57     21736\n",
      "weighted avg       0.76      0.66      0.69     21736\n",
      "\n",
      "==================================================\n",
      "Loading LSTM_GloVe_100.h5 ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m680/680\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 26ms/step\n",
      "Loaded model LSTM_GloVe_100.h5 accuracy: 0.6851766654398234\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.82      0.85      0.83      1117\n",
      "           1       0.68      0.67      0.68      1439\n",
      "           2       0.85      0.68      0.75      5578\n",
      "           3       0.09      0.73      0.15        86\n",
      "           4       0.33      0.66      0.44       599\n",
      "           5       0.41      0.71      0.52       160\n",
      "           6       0.66      0.75      0.70       160\n",
      "           7       0.71      0.81      0.76       160\n",
      "           8       0.53      0.61      0.57       160\n",
      "           9       0.51      0.65      0.57       160\n",
      "          10       0.91      0.80      0.85      4637\n",
      "          11       0.15      0.48      0.23       160\n",
      "          12       0.48      0.72      0.58       700\n",
      "          13       0.55      0.55      0.55       160\n",
      "          14       0.51      0.59      0.55       160\n",
      "          15       0.82      0.42      0.55      3327\n",
      "          16       0.67      0.81      0.74      1252\n",
      "          17       0.86      0.88      0.87       921\n",
      "          18       0.22      0.74      0.34       160\n",
      "          19       0.34      0.55      0.42       640\n",
      "\n",
      "    accuracy                           0.69     21736\n",
      "   macro avg       0.55      0.68      0.58     21736\n",
      "weighted avg       0.76      0.69      0.71     21736\n",
      "\n",
      "==================================================\n",
      "Loading LSTM_Word2Vec_300.h5 ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m680/680\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 38ms/step\n",
      "Loaded model LSTM_Word2Vec_300.h5 accuracy: 0.7046374677953625\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.85      0.87      0.86      1117\n",
      "           1       0.65      0.77      0.70      1439\n",
      "           2       0.87      0.68      0.76      5578\n",
      "           3       0.13      0.56      0.21        86\n",
      "           4       0.34      0.68      0.46       599\n",
      "           5       0.45      0.71      0.55       160\n",
      "           6       0.67      0.86      0.75       160\n",
      "           7       0.86      0.81      0.83       160\n",
      "           8       0.64      0.64      0.64       160\n",
      "           9       0.51      0.77      0.61       160\n",
      "          10       0.92      0.77      0.84      4637\n",
      "          11       0.14      0.45      0.21       160\n",
      "          12       0.51      0.72      0.60       700\n",
      "          13       0.47      0.68      0.56       160\n",
      "          14       0.54      0.65      0.59       160\n",
      "          15       0.76      0.52      0.62      3327\n",
      "          16       0.66      0.83      0.73      1252\n",
      "          17       0.86      0.81      0.83       921\n",
      "          18       0.29      0.69      0.41       160\n",
      "          19       0.38      0.63      0.48       640\n",
      "\n",
      "    accuracy                           0.70     21736\n",
      "   macro avg       0.57      0.70      0.61     21736\n",
      "weighted avg       0.77      0.70      0.72     21736\n",
      "\n",
      "==================================================\n",
      "Loading RNN_FastText_Custom_300.h5 ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m680/680\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 26ms/step\n",
      "Loaded model RNN_FastText_Custom_300.h5 accuracy: 0.37067537725432465\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.66      0.65      0.65      1117\n",
      "           1       0.45      0.28      0.35      1439\n",
      "           2       0.61      0.11      0.18      5578\n",
      "           3       0.05      0.29      0.09        86\n",
      "           4       0.11      0.53      0.18       599\n",
      "           5       0.19      0.41      0.26       160\n",
      "           6       0.20      0.64      0.30       160\n",
      "           7       0.23      0.41      0.30       160\n",
      "           8       0.14      0.08      0.10       160\n",
      "           9       0.15      0.41      0.22       160\n",
      "          10       0.76      0.69      0.72      4637\n",
      "          11       0.07      0.22      0.11       160\n",
      "          12       0.19      0.30      0.23       700\n",
      "          13       0.13      0.22      0.17       160\n",
      "          14       0.17      0.24      0.20       160\n",
      "          15       0.56      0.16      0.25      3327\n",
      "          16       0.33      0.65      0.44      1252\n",
      "          17       0.61      0.56      0.58       921\n",
      "          18       0.06      0.50      0.11       160\n",
      "          19       0.12      0.37      0.18       640\n",
      "\n",
      "    accuracy                           0.37     21736\n",
      "   macro avg       0.29      0.39      0.28     21736\n",
      "weighted avg       0.54      0.37      0.38     21736\n",
      "\n",
      "==================================================\n",
      "Loading RNN_FastText_Pre_300.h5 ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m680/680\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 25ms/step\n",
      "Loaded model RNN_FastText_Pre_300.h5 accuracy: 0.14869341185130658\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00      1117\n",
      "           1       0.07      0.00      0.00      1439\n",
      "           2       1.00      0.00      0.00      5578\n",
      "           3       0.00      0.00      0.00        86\n",
      "           4       0.00      0.00      0.00       599\n",
      "           5       0.03      0.17      0.05       160\n",
      "           6       0.00      0.00      0.00       160\n",
      "           7       0.00      0.00      0.00       160\n",
      "           8       0.00      0.00      0.00       160\n",
      "           9       0.00      0.00      0.00       160\n",
      "          10       0.43      0.53      0.48      4637\n",
      "          11       0.02      0.10      0.03       160\n",
      "          12       0.06      0.04      0.05       700\n",
      "          13       0.06      0.83      0.11       160\n",
      "          14       0.00      0.00      0.00       160\n",
      "          15       0.00      0.00      0.00      3327\n",
      "          16       0.00      0.00      0.00      1252\n",
      "          17       0.00      0.00      0.00       921\n",
      "          18       0.00      0.00      0.00       160\n",
      "          19       0.05      0.86      0.09       640\n",
      "\n",
      "    accuracy                           0.15     21736\n",
      "   macro avg       0.09      0.13      0.04     21736\n",
      "weighted avg       0.36      0.15      0.11     21736\n",
      "\n",
      "==================================================\n",
      "Loading RNN_GloVe_100.h5 ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m680/680\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 16ms/step\n",
      "Loaded model RNN_GloVe_100.h5 accuracy: 0.24682554287817446\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.17      0.11      0.13      1117\n",
      "           1       0.23      0.03      0.05      1439\n",
      "           2       0.52      0.04      0.08      5578\n",
      "           3       0.01      0.38      0.02        86\n",
      "           4       0.11      0.23      0.15       599\n",
      "           5       0.04      0.03      0.04       160\n",
      "           6       0.09      0.19      0.12       160\n",
      "           7       0.10      0.12      0.11       160\n",
      "           8       0.03      0.11      0.05       160\n",
      "           9       0.08      0.17      0.11       160\n",
      "          10       0.68      0.73      0.70      4637\n",
      "          11       0.07      0.04      0.05       160\n",
      "          12       0.17      0.04      0.06       700\n",
      "          13       0.04      0.64      0.07       160\n",
      "          14       0.05      0.09      0.06       160\n",
      "          15       0.32      0.03      0.06      3327\n",
      "          16       0.36      0.32      0.34      1252\n",
      "          17       0.28      0.47      0.35       921\n",
      "          18       0.06      0.21      0.09       160\n",
      "          19       0.08      0.32      0.13       640\n",
      "\n",
      "    accuracy                           0.25     21736\n",
      "   macro avg       0.17      0.22      0.14     21736\n",
      "weighted avg       0.40      0.25      0.24     21736\n",
      "\n",
      "==================================================\n",
      "Loading RNN_Word2Vec_300.h5 ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m680/680\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 25ms/step\n",
      "Loaded model RNN_Word2Vec_300.h5 accuracy: 0.3590357011409643\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.58      0.39      0.47      1117\n",
      "           1       0.48      0.35      0.40      1439\n",
      "           2       0.67      0.17      0.27      5578\n",
      "           3       0.05      0.44      0.09        86\n",
      "           4       0.10      0.61      0.17       599\n",
      "           5       0.09      0.68      0.16       160\n",
      "           6       0.15      0.56      0.24       160\n",
      "           7       0.15      0.16      0.15       160\n",
      "           8       0.07      0.23      0.10       160\n",
      "           9       0.13      0.11      0.12       160\n",
      "          10       0.69      0.78      0.74      4637\n",
      "          11       0.06      0.11      0.08       160\n",
      "          12       0.24      0.24      0.24       700\n",
      "          13       0.17      0.16      0.16       160\n",
      "          14       0.12      0.10      0.11       160\n",
      "          15       0.47      0.01      0.01      3327\n",
      "          16       0.30      0.64      0.41      1252\n",
      "          17       0.72      0.40      0.51       921\n",
      "          18       0.09      0.29      0.13       160\n",
      "          19       0.15      0.23      0.18       640\n",
      "\n",
      "    accuracy                           0.36     21736\n",
      "   macro avg       0.27      0.33      0.24     21736\n",
      "weighted avg       0.52      0.36      0.35     21736\n",
      "\n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pickle\n",
    "import numpy as np\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import load_model\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "\n",
    "with open(\"tokenizer_sl.pkl\", \"rb\") as f:\n",
    "    tokenizer = pickle.load(f)\n",
    "\n",
    "print(\"Testing saved models...\")\n",
    "for model_file in os.listdir(\"models_baru\"):\n",
    "    if model_file.endswith(\".h5\"):\n",
    "        model_path = os.path.join(\"models_baru\", model_file)\n",
    "        print(f\"Loading {model_file} ...\")\n",
    "        loaded_model = load_model(model_path)\n",
    "        \n",
    "        y_pred_prob = loaded_model.predict(X_test_pad)\n",
    "        y_pred = np.argmax(y_pred_prob, axis=1)\n",
    "        \n",
    "        acc = accuracy_score(y_test, y_pred)\n",
    "        report = classification_report(y_test, y_pred, labels=range(len(target_names)), target_names=target_names, zero_division=0)\n",
    "        print(f\"Loaded model {model_file} accuracy: {acc}\")\n",
    "        print(report)\n",
    "        print(\"=\"*50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5a55e15",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================== MULAI TRAINING SHALLOW LEARNING (PKL) ====================\n",
      "\n",
      "Training LogisticRegression_SVD...\n",
      "Model saved to models_baru_tf_idf/LogisticRegression_SVD.pkl\n",
      "Accuracy: 0.7304\n",
      "\n",
      "Training SVM_SVD...\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import joblib\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "\n",
    "folder_path = \"models_baru_tf_idf\"\n",
    "if not os.path.exists(\"models_baru_tf_idf\"):\n",
    "    os.makedirs(\"models_baru_tf_idf\")\n",
    "\n",
    "print(f\"\\n{'='*20} MULAI TRAINING SHALLOW LEARNING (PKL) {'='*20}\")\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "X_train_svd_scaled = scaler.fit_transform(X_train_svd)\n",
    "X_val_svd_scaled = scaler.transform(X_val_svd)\n",
    "\n",
    "joblib.dump(tfidf, f\"{folder_path}/tfidf_vectorizer.pkl\")\n",
    "joblib.dump(svd, f\"{folder_path}/svd_model.pkl\")\n",
    "joblib.dump(scaler, f\"{folder_path}/tfidf_svd_scaler.pkl\")\n",
    "\n",
    "class_weights = compute_class_weight('balanced', classes=np.unique(y_train), y=y_train)\n",
    "class_weights_dict = dict(enumerate(class_weights))\n",
    "\n",
    "try:\n",
    "    target_names = dataset_train_final.features['label'].names\n",
    "except:\n",
    "    target_names = [str(i) for i in range(len(np.unique(y_train)))]\n",
    "\n",
    "shallow_models = {\n",
    "    \"LogisticRegression_SVD\": LogisticRegression(max_iter=2000, class_weight='balanced', solver='lbfgs'),\n",
    "    # \"SVM_SVD\": SVC(kernel='rbf', class_weight='balanced', cache_size=1000)\n",
    "}\n",
    "\n",
    "all_results = {}\n",
    "\n",
    "for name, model in shallow_models.items():\n",
    "    print(f\"\\nTraining {name}...\")\n",
    "    \n",
    "    model.fit(X_train_svd_scaled, y_train)\n",
    "    \n",
    "    save_path = f\"{folder_path}/{name}.pkl\"\n",
    "    joblib.dump(model, save_path)\n",
    "    print(f\"Model saved to {save_path}\")\n",
    "    \n",
    "    y_pred = model.predict(X_val_svd_scaled)\n",
    "    acc = accuracy_score(y_val, y_pred)\n",
    "    report = classification_report(y_val, y_pred, target_names=target_names, zero_division=0)\n",
    "    \n",
    "    all_results[name] = {\"accuracy\": acc, \"report\": report}\n",
    "    print(f\"Accuracy: {acc:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "71bad258",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================== TRAINING DEEP LEARNING (H5) ====================\n",
      "\n",
      "Training ANN_TFIDF...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Iskandar\\Documents\\GitHub\\Tubes-NLP-PaperMatch\\venv\\Lib\\site-packages\\keras\\src\\layers\\core\\dense.py:95: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "\u001b[1m1087/1087\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 11ms/step - accuracy: 0.4952 - loss: 1.4287 - val_accuracy: 0.6343 - val_loss: 1.1284\n",
      "Epoch 2/30\n",
      "\u001b[1m1087/1087\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 11ms/step - accuracy: 0.6660 - loss: 0.7774 - val_accuracy: 0.6884 - val_loss: 0.9329\n",
      "Epoch 3/30\n",
      "\u001b[1m1087/1087\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 11ms/step - accuracy: 0.7061 - loss: 0.6141 - val_accuracy: 0.7105 - val_loss: 0.8835\n",
      "Epoch 4/30\n",
      "\u001b[1m1087/1087\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 10ms/step - accuracy: 0.7300 - loss: 0.5050 - val_accuracy: 0.7048 - val_loss: 0.9007\n",
      "Epoch 5/30\n",
      "\u001b[1m1087/1087\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 11ms/step - accuracy: 0.7519 - loss: 0.4283 - val_accuracy: 0.7159 - val_loss: 0.8770\n",
      "Epoch 6/30\n",
      "\u001b[1m1087/1087\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 11ms/step - accuracy: 0.7693 - loss: 0.3723 - val_accuracy: 0.7245 - val_loss: 0.8646\n",
      "Epoch 7/30\n",
      "\u001b[1m1087/1087\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 11ms/step - accuracy: 0.7833 - loss: 0.3376 - val_accuracy: 0.7270 - val_loss: 0.8713\n",
      "Epoch 8/30\n",
      "\u001b[1m1087/1087\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 11ms/step - accuracy: 0.7974 - loss: 0.3004 - val_accuracy: 0.7255 - val_loss: 0.8930\n",
      "Epoch 9/30\n",
      "\u001b[1m1087/1087\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 10ms/step - accuracy: 0.8121 - loss: 0.2600 - val_accuracy: 0.7260 - val_loss: 0.8965\n",
      "Epoch 10/30\n",
      "\u001b[1m1087/1087\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 11ms/step - accuracy: 0.8198 - loss: 0.2558 - val_accuracy: 0.7275 - val_loss: 0.9201\n",
      "Epoch 11/30\n",
      "\u001b[1m1087/1087\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 11ms/step - accuracy: 0.8307 - loss: 0.2300 - val_accuracy: 0.7458 - val_loss: 0.8931\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved to models_baru_tf_idf/ANN_TFIDF.h5\n",
      "\u001b[1m544/544\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step\n",
      "Accuracy: 0.7245\n",
      "\n",
      "Training CNN_TFIDF...\n",
      "Epoch 1/30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Iskandar\\Documents\\GitHub\\Tubes-NLP-PaperMatch\\venv\\Lib\\site-packages\\keras\\src\\layers\\reshaping\\reshape.py:38: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1087/1087\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m98s\u001b[0m 89ms/step - accuracy: 0.2130 - loss: 2.6826 - val_accuracy: 0.3251 - val_loss: 2.1431\n",
      "Epoch 2/30\n",
      "\u001b[1m1087/1087\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m97s\u001b[0m 89ms/step - accuracy: 0.3343 - loss: 2.2111 - val_accuracy: 0.4188 - val_loss: 1.8726\n",
      "Epoch 3/30\n",
      "\u001b[1m1087/1087\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m93s\u001b[0m 85ms/step - accuracy: 0.3695 - loss: 2.0568 - val_accuracy: 0.4470 - val_loss: 1.7663\n",
      "Epoch 4/30\n",
      "\u001b[1m1087/1087\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m94s\u001b[0m 86ms/step - accuracy: 0.3859 - loss: 1.9935 - val_accuracy: 0.4030 - val_loss: 1.8454\n",
      "Epoch 5/30\n",
      "\u001b[1m1087/1087\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m93s\u001b[0m 85ms/step - accuracy: 0.3885 - loss: 1.9561 - val_accuracy: 0.4574 - val_loss: 1.7498\n",
      "Epoch 6/30\n",
      "\u001b[1m1087/1087\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m93s\u001b[0m 85ms/step - accuracy: 0.3963 - loss: 1.9308 - val_accuracy: 0.4221 - val_loss: 1.7671\n",
      "Epoch 7/30\n",
      "\u001b[1m1087/1087\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m93s\u001b[0m 85ms/step - accuracy: 0.3961 - loss: 1.9102 - val_accuracy: 0.4743 - val_loss: 1.6776\n",
      "Epoch 8/30\n",
      "\u001b[1m1087/1087\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m94s\u001b[0m 87ms/step - accuracy: 0.4024 - loss: 1.8909 - val_accuracy: 0.4020 - val_loss: 1.7366\n",
      "Epoch 9/30\n",
      "\u001b[1m1087/1087\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m95s\u001b[0m 87ms/step - accuracy: 0.4006 - loss: 1.8807 - val_accuracy: 0.4693 - val_loss: 1.6822\n",
      "Epoch 10/30\n",
      "\u001b[1m1087/1087\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m94s\u001b[0m 86ms/step - accuracy: 0.4029 - loss: 1.8668 - val_accuracy: 0.3840 - val_loss: 1.7881\n",
      "Epoch 11/30\n",
      "\u001b[1m1087/1087\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m94s\u001b[0m 86ms/step - accuracy: 0.4078 - loss: 1.8492 - val_accuracy: 0.4383 - val_loss: 1.6970\n",
      "Epoch 12/30\n",
      "\u001b[1m1087/1087\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m96s\u001b[0m 88ms/step - accuracy: 0.4022 - loss: 1.8406 - val_accuracy: 0.4778 - val_loss: 1.6237\n",
      "Epoch 13/30\n",
      "\u001b[1m1087/1087\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m94s\u001b[0m 86ms/step - accuracy: 0.4097 - loss: 1.8289 - val_accuracy: 0.4550 - val_loss: 1.6417\n",
      "Epoch 14/30\n",
      "\u001b[1m1087/1087\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m94s\u001b[0m 87ms/step - accuracy: 0.4113 - loss: 1.8216 - val_accuracy: 0.4312 - val_loss: 1.6483\n",
      "Epoch 15/30\n",
      "\u001b[1m1087/1087\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m94s\u001b[0m 86ms/step - accuracy: 0.4057 - loss: 1.8131 - val_accuracy: 0.4444 - val_loss: 1.6140\n",
      "Epoch 16/30\n",
      "\u001b[1m1087/1087\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m94s\u001b[0m 87ms/step - accuracy: 0.4104 - loss: 1.8091 - val_accuracy: 0.4292 - val_loss: 1.6627\n",
      "Epoch 17/30\n",
      "\u001b[1m1087/1087\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m95s\u001b[0m 88ms/step - accuracy: 0.4102 - loss: 1.7966 - val_accuracy: 0.4619 - val_loss: 1.6311\n",
      "Epoch 18/30\n",
      "\u001b[1m1087/1087\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m93s\u001b[0m 86ms/step - accuracy: 0.4105 - loss: 1.7860 - val_accuracy: 0.4673 - val_loss: 1.6005\n",
      "Epoch 19/30\n",
      "\u001b[1m1087/1087\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m94s\u001b[0m 86ms/step - accuracy: 0.4114 - loss: 1.7845 - val_accuracy: 0.4410 - val_loss: 1.6325\n",
      "Epoch 20/30\n",
      "\u001b[1m1087/1087\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m93s\u001b[0m 86ms/step - accuracy: 0.4157 - loss: 1.7736 - val_accuracy: 0.4689 - val_loss: 1.6000\n",
      "Epoch 21/30\n",
      "\u001b[1m1087/1087\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m96s\u001b[0m 88ms/step - accuracy: 0.4117 - loss: 1.7686 - val_accuracy: 0.4626 - val_loss: 1.6063\n",
      "Epoch 22/30\n",
      "\u001b[1m1087/1087\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m95s\u001b[0m 87ms/step - accuracy: 0.4168 - loss: 1.7601 - val_accuracy: 0.4425 - val_loss: 1.6526\n",
      "Epoch 23/30\n",
      "\u001b[1m1087/1087\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m94s\u001b[0m 87ms/step - accuracy: 0.4151 - loss: 1.7563 - val_accuracy: 0.4545 - val_loss: 1.6224\n",
      "Epoch 24/30\n",
      "\u001b[1m1087/1087\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m95s\u001b[0m 88ms/step - accuracy: 0.4201 - loss: 1.7504 - val_accuracy: 0.4428 - val_loss: 1.6033\n",
      "Epoch 25/30\n",
      "\u001b[1m1087/1087\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m94s\u001b[0m 87ms/step - accuracy: 0.4196 - loss: 1.7448 - val_accuracy: 0.4319 - val_loss: 1.6373\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved to models_baru_tf_idf/CNN_TFIDF.h5\n",
      "\u001b[1m544/544\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 9ms/step\n",
      "Accuracy: 0.4689\n",
      "\n",
      "Training RNN_TFIDF...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Iskandar\\Documents\\GitHub\\Tubes-NLP-PaperMatch\\venv\\Lib\\site-packages\\keras\\src\\layers\\reshaping\\reshape.py:38: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "\u001b[1m1087/1087\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 5ms/step - accuracy: 0.4915 - loss: 1.4866 - val_accuracy: 0.6720 - val_loss: 1.0650\n",
      "Epoch 2/30\n",
      "\u001b[1m1087/1087\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 5ms/step - accuracy: 0.6639 - loss: 0.8206 - val_accuracy: 0.6973 - val_loss: 0.9584\n",
      "Epoch 3/30\n",
      "\u001b[1m1087/1087\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 5ms/step - accuracy: 0.6905 - loss: 0.6780 - val_accuracy: 0.6968 - val_loss: 0.9284\n",
      "Epoch 4/30\n",
      "\u001b[1m1087/1087\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 5ms/step - accuracy: 0.7008 - loss: 0.6067 - val_accuracy: 0.6982 - val_loss: 0.9291\n",
      "Epoch 5/30\n",
      "\u001b[1m1087/1087\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 5ms/step - accuracy: 0.7105 - loss: 0.5539 - val_accuracy: 0.7067 - val_loss: 0.8970\n",
      "Epoch 6/30\n",
      "\u001b[1m1087/1087\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 5ms/step - accuracy: 0.7207 - loss: 0.5128 - val_accuracy: 0.6882 - val_loss: 0.9446\n",
      "Epoch 7/30\n",
      "\u001b[1m1087/1087\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 5ms/step - accuracy: 0.7237 - loss: 0.4784 - val_accuracy: 0.7081 - val_loss: 0.8917\n",
      "Epoch 8/30\n",
      "\u001b[1m1087/1087\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 5ms/step - accuracy: 0.7288 - loss: 0.4501 - val_accuracy: 0.7063 - val_loss: 0.9008\n",
      "Epoch 9/30\n",
      "\u001b[1m1087/1087\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 5ms/step - accuracy: 0.7342 - loss: 0.4341 - val_accuracy: 0.7164 - val_loss: 0.8871\n",
      "Epoch 10/30\n",
      "\u001b[1m1087/1087\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 5ms/step - accuracy: 0.7404 - loss: 0.4085 - val_accuracy: 0.7115 - val_loss: 0.8825\n",
      "Epoch 11/30\n",
      "\u001b[1m1087/1087\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 5ms/step - accuracy: 0.7440 - loss: 0.3907 - val_accuracy: 0.7135 - val_loss: 0.9000\n",
      "Epoch 12/30\n",
      "\u001b[1m1087/1087\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 5ms/step - accuracy: 0.7496 - loss: 0.3775 - val_accuracy: 0.7080 - val_loss: 0.9208\n",
      "Epoch 13/30\n",
      "\u001b[1m1087/1087\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 5ms/step - accuracy: 0.7548 - loss: 0.3564 - val_accuracy: 0.7187 - val_loss: 0.9034\n",
      "Epoch 14/30\n",
      "\u001b[1m1087/1087\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 5ms/step - accuracy: 0.7590 - loss: 0.3524 - val_accuracy: 0.7168 - val_loss: 0.9206\n",
      "Epoch 15/30\n",
      "\u001b[1m1087/1087\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 5ms/step - accuracy: 0.7621 - loss: 0.3352 - val_accuracy: 0.7232 - val_loss: 0.9231\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved to models_baru_tf_idf/RNN_TFIDF.h5\n",
      "\u001b[1m544/544\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step\n",
      "Accuracy: 0.7115\n",
      "\n",
      "Training LSTM_TFIDF...\n",
      "Epoch 1/30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Iskandar\\Documents\\GitHub\\Tubes-NLP-PaperMatch\\venv\\Lib\\site-packages\\keras\\src\\layers\\reshaping\\reshape.py:38: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1087/1087\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 10ms/step - accuracy: 0.3375 - loss: 1.8608 - val_accuracy: 0.5612 - val_loss: 1.3462\n",
      "Epoch 2/30\n",
      "\u001b[1m1087/1087\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 10ms/step - accuracy: 0.5448 - loss: 1.0386 - val_accuracy: 0.6248 - val_loss: 1.1103\n",
      "Epoch 3/30\n",
      "\u001b[1m1087/1087\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 10ms/step - accuracy: 0.5978 - loss: 0.8817 - val_accuracy: 0.6400 - val_loss: 1.0520\n",
      "Epoch 4/30\n",
      "\u001b[1m1087/1087\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 10ms/step - accuracy: 0.6235 - loss: 0.7937 - val_accuracy: 0.6754 - val_loss: 0.9720\n",
      "Epoch 5/30\n",
      "\u001b[1m1087/1087\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 10ms/step - accuracy: 0.6428 - loss: 0.7276 - val_accuracy: 0.6721 - val_loss: 0.9683\n",
      "Epoch 6/30\n",
      "\u001b[1m1087/1087\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 10ms/step - accuracy: 0.6518 - loss: 0.6785 - val_accuracy: 0.6621 - val_loss: 0.9810\n",
      "Epoch 7/30\n",
      "\u001b[1m1087/1087\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 11ms/step - accuracy: 0.6604 - loss: 0.6383 - val_accuracy: 0.6779 - val_loss: 0.9495\n",
      "Epoch 8/30\n",
      "\u001b[1m1087/1087\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 10ms/step - accuracy: 0.6686 - loss: 0.6091 - val_accuracy: 0.6846 - val_loss: 0.9127\n",
      "Epoch 9/30\n",
      "\u001b[1m1087/1087\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 10ms/step - accuracy: 0.6776 - loss: 0.5726 - val_accuracy: 0.6799 - val_loss: 0.9347\n",
      "Epoch 10/30\n",
      "\u001b[1m1087/1087\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 10ms/step - accuracy: 0.6856 - loss: 0.5416 - val_accuracy: 0.6838 - val_loss: 0.9174\n",
      "Epoch 11/30\n",
      "\u001b[1m1087/1087\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 10ms/step - accuracy: 0.6899 - loss: 0.5179 - val_accuracy: 0.6937 - val_loss: 0.9068\n",
      "Epoch 12/30\n",
      "\u001b[1m1087/1087\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 10ms/step - accuracy: 0.6955 - loss: 0.4959 - val_accuracy: 0.6996 - val_loss: 0.8880\n",
      "Epoch 13/30\n",
      "\u001b[1m1087/1087\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 10ms/step - accuracy: 0.7008 - loss: 0.4819 - val_accuracy: 0.6980 - val_loss: 0.9020\n",
      "Epoch 14/30\n",
      "\u001b[1m1087/1087\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 10ms/step - accuracy: 0.7048 - loss: 0.4574 - val_accuracy: 0.6995 - val_loss: 0.8943\n",
      "Epoch 15/30\n",
      "\u001b[1m1087/1087\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 10ms/step - accuracy: 0.7133 - loss: 0.4373 - val_accuracy: 0.7138 - val_loss: 0.8732\n",
      "Epoch 16/30\n",
      "\u001b[1m1087/1087\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 10ms/step - accuracy: 0.7180 - loss: 0.4185 - val_accuracy: 0.7175 - val_loss: 0.8620\n",
      "Epoch 17/30\n",
      "\u001b[1m1087/1087\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 10ms/step - accuracy: 0.7231 - loss: 0.4005 - val_accuracy: 0.7159 - val_loss: 0.8771\n",
      "Epoch 18/30\n",
      "\u001b[1m1087/1087\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 10ms/step - accuracy: 0.7294 - loss: 0.3878 - val_accuracy: 0.7153 - val_loss: 0.8822\n",
      "Epoch 19/30\n",
      "\u001b[1m1087/1087\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 10ms/step - accuracy: 0.7371 - loss: 0.3679 - val_accuracy: 0.7178 - val_loss: 0.8897\n",
      "Epoch 20/30\n",
      "\u001b[1m1087/1087\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 10ms/step - accuracy: 0.7417 - loss: 0.3537 - val_accuracy: 0.7094 - val_loss: 0.9154\n",
      "Epoch 21/30\n",
      "\u001b[1m1087/1087\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 10ms/step - accuracy: 0.7460 - loss: 0.3463 - val_accuracy: 0.7199 - val_loss: 0.8912\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved to models_baru_tf_idf/LSTM_TFIDF.h5\n",
      "\u001b[1m544/544\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step\n",
      "Accuracy: 0.7175\n",
      "\n",
      "========================================\n",
      "FINAL LEADERBOARD (TF-IDF MODELS) 🏆\n",
      "========================================\n",
      "1. ANN_TFIDF                      | Accuracy: 0.7245\n",
      "2. LSTM_TFIDF                     | Accuracy: 0.7175\n",
      "3. RNN_TFIDF                      | Accuracy: 0.7115\n",
      "4. CNN_TFIDF                      | Accuracy: 0.4689\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import joblib\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, Flatten, Conv1D, GlobalMaxPooling1D, LSTM, SimpleRNN, Bidirectional, Reshape\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "\n",
    "folder_path = \"models_baru_tf_idf\"\n",
    "if not os.path.exists(\"models_baru_tf_idf\"):\n",
    "    os.makedirs(\"models_baru_tf_idf\")\n",
    "\n",
    "class_weights = compute_class_weight('balanced', classes=np.unique(y_train), y=y_train)\n",
    "class_weights_dict = dict(enumerate(class_weights))\n",
    "\n",
    "try:\n",
    "    target_names = dataset_train_final.features['label'].names\n",
    "except:\n",
    "    target_names = [str(i) for i in range(len(np.unique(y_train)))]\n",
    "\n",
    "all_results = {}\n",
    "\n",
    "print(f\"\\n{'='*20} TRAINING DEEP LEARNING (H5) {'='*20}\")\n",
    "\n",
    "input_dim = X_train_svd.shape[1]\n",
    "num_classes = len(np.unique(y_train))\n",
    "\n",
    "def build_tfidf_ann(input_dim, num_classes):\n",
    "    model = Sequential()\n",
    "    model.add(Dense(512, input_shape=(input_dim,), activation='relu'))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(256, activation='relu'))\n",
    "    model.add(Dropout(0.3))\n",
    "    model.add(Dense(num_classes, activation='softmax'))\n",
    "    model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "def build_tfidf_cnn(input_dim, num_classes):\n",
    "    model = Sequential()\n",
    "    model.add(Reshape((input_dim, 1), input_shape=(input_dim,)))\n",
    "    model.add(Conv1D(64, 5, activation='relu', padding='same'))\n",
    "    model.add(Conv1D(32, 3, activation='relu', padding='same'))\n",
    "    model.add(GlobalMaxPooling1D())\n",
    "    model.add(Dense(64, activation='relu'))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(num_classes, activation='softmax'))\n",
    "    model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "def build_tfidf_rnn(input_dim, num_classes):\n",
    "    model = Sequential()\n",
    "    model.add(Reshape((1, input_dim), input_shape=(input_dim,)))\n",
    "    model.add(Bidirectional(SimpleRNN(128, return_sequences=False)))\n",
    "    model.add(Dense(64, activation='relu'))\n",
    "    model.add(Dropout(0.3))\n",
    "    model.add(Dense(num_classes, activation='softmax'))\n",
    "    model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "def build_tfidf_lstm(input_dim, num_classes):\n",
    "    model = Sequential()\n",
    "    model.add(Reshape((1, input_dim), input_shape=(input_dim,)))\n",
    "    model.add(Bidirectional(LSTM(64, return_sequences=False)))\n",
    "    model.add(Dense(64, activation='relu'))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(num_classes, activation='softmax'))\n",
    "    model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "tfidf_dl_models = {\n",
    "    \"ANN_TFIDF\": build_tfidf_ann,\n",
    "    \"CNN_TFIDF\": build_tfidf_cnn,\n",
    "    \"RNN_TFIDF\": build_tfidf_rnn,\n",
    "    \"LSTM_TFIDF\": build_tfidf_lstm\n",
    "}\n",
    "\n",
    "for model_name, model_func in tfidf_dl_models.items():\n",
    "    print(f\"\\nTraining {model_name}...\")\n",
    "    \n",
    "    early_stop = EarlyStopping(\n",
    "        monitor='val_loss', \n",
    "        patience=5, \n",
    "        restore_best_weights=True\n",
    "    )\n",
    "    \n",
    "    model = model_func(input_dim, num_classes)\n",
    "    \n",
    "    history = model.fit(\n",
    "        X_train_svd, y_train,\n",
    "        epochs=30, \n",
    "        batch_size=64,\n",
    "        validation_data=(X_val_svd, y_val),\n",
    "        class_weight=class_weights_dict,\n",
    "        callbacks=[early_stop],\n",
    "        verbose=1 \n",
    "    )\n",
    "    \n",
    "    save_path = f\"{folder_path}/{model_name}.h5\"\n",
    "    model.save(save_path)\n",
    "    print(f\"Model saved to {save_path}\")\n",
    "    \n",
    "    y_pred_prob = model.predict(X_val_svd)\n",
    "    y_pred = np.argmax(y_pred_prob, axis=1)\n",
    "    \n",
    "    acc = accuracy_score(y_val, y_pred)\n",
    "    report = classification_report(y_val, y_pred, target_names=target_names, zero_division=0)\n",
    "    \n",
    "    all_results[model_name] = {\"accuracy\": acc, \"report\": report}\n",
    "    print(f\"Accuracy: {acc:.4f}\")\n",
    "\n",
    "\n",
    "print(\"\\n\" + \"=\"*40)\n",
    "print(\"FINAL LEADERBOARD (TF-IDF MODELS) 🏆\")\n",
    "print(\"=\"*40)\n",
    "\n",
    "sorted_results = sorted(all_results.items(), key=lambda x: x[1]['accuracy'], reverse=True)\n",
    "\n",
    "for i, (name, res) in enumerate(sorted_results):\n",
    "    print(f\"{i+1}. {name:<30} | Accuracy: {res['accuracy']:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "48f95f59",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading preprocessing tools from models_baru_tf_idf...\n",
      "Tools (TFIDF, SVD, Scaler) loaded successfully.\n",
      "\n",
      "Transforming test data...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape Test Data: (21736, 2000)\n",
      "==================================================\n",
      "Testing saved models...\n",
      "\n",
      "Testing Model: ANN_TFIDF.h5 ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.7236\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.88      0.85      0.86      1117\n",
      "           1       0.70      0.75      0.72      1439\n",
      "           2       0.89      0.67      0.77      5578\n",
      "           3       0.20      0.45      0.28        86\n",
      "           4       0.32      0.74      0.45       599\n",
      "           5       0.49      0.79      0.61       160\n",
      "           6       0.64      0.86      0.73       160\n",
      "           7       0.77      0.91      0.83       160\n",
      "           8       0.77      0.46      0.58       160\n",
      "           9       0.54      0.72      0.62       160\n",
      "          10       0.88      0.83      0.86      4637\n",
      "          11       0.18      0.46      0.26       160\n",
      "          12       0.54      0.69      0.61       700\n",
      "          13       0.48      0.75      0.59       160\n",
      "          14       0.55      0.69      0.61       160\n",
      "          15       0.77      0.53      0.63      3327\n",
      "          16       0.67      0.82      0.74      1252\n",
      "          17       0.82      0.91      0.86       921\n",
      "          18       0.32      0.71      0.44       160\n",
      "          19       0.46      0.72      0.56       640\n",
      "\n",
      "    accuracy                           0.72     21736\n",
      "   macro avg       0.59      0.72      0.63     21736\n",
      "weighted avg       0.77      0.72      0.74     21736\n",
      "\n",
      "--------------------------------------------------\n",
      "\n",
      "Testing Model: CNN_TFIDF.h5 ...\n",
      "Accuracy: 0.4666\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.64      0.62      0.63      1117\n",
      "           1       0.50      0.55      0.53      1439\n",
      "           2       0.75      0.42      0.54      5578\n",
      "           3       0.04      0.35      0.08        86\n",
      "           4       0.14      0.55      0.23       599\n",
      "           5       0.15      0.29      0.20       160\n",
      "           6       0.37      0.71      0.48       160\n",
      "           7       0.28      0.50      0.36       160\n",
      "           8       0.20      0.18      0.19       160\n",
      "           9       0.18      0.53      0.26       160\n",
      "          10       0.87      0.69      0.77      4637\n",
      "          11       0.06      0.26      0.10       160\n",
      "          12       0.28      0.42      0.34       700\n",
      "          13       0.20      0.39      0.26       160\n",
      "          14       0.08      0.40      0.14       160\n",
      "          15       0.56      0.10      0.16      3327\n",
      "          16       0.35      0.49      0.41      1252\n",
      "          17       0.78      0.66      0.71       921\n",
      "          18       0.14      0.31      0.19       160\n",
      "          19       0.22      0.50      0.30       640\n",
      "\n",
      "    accuracy                           0.47     21736\n",
      "   macro avg       0.34      0.45      0.34     21736\n",
      "weighted avg       0.62      0.47      0.49     21736\n",
      "\n",
      "--------------------------------------------------\n",
      "\n",
      "Testing Model: LogisticRegression_SVD.pkl ...\n",
      "Accuracy: 0.7305\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.84      0.87      0.86      1117\n",
      "           1       0.72      0.75      0.73      1439\n",
      "           2       0.88      0.71      0.79      5578\n",
      "           3       0.19      0.37      0.25        86\n",
      "           4       0.34      0.68      0.45       599\n",
      "           5       0.54      0.69      0.61       160\n",
      "           6       0.72      0.78      0.75       160\n",
      "           7       0.80      0.87      0.83       160\n",
      "           8       0.59      0.61      0.60       160\n",
      "           9       0.54      0.69      0.60       160\n",
      "          10       0.90      0.81      0.85      4637\n",
      "          11       0.14      0.41      0.21       160\n",
      "          12       0.52      0.70      0.60       700\n",
      "          13       0.56      0.74      0.64       160\n",
      "          14       0.56      0.68      0.61       160\n",
      "          15       0.75      0.59      0.66      3327\n",
      "          16       0.69      0.78      0.73      1252\n",
      "          17       0.85      0.88      0.87       921\n",
      "          18       0.39      0.59      0.47       160\n",
      "          19       0.44      0.72      0.54       640\n",
      "\n",
      "    accuracy                           0.73     21736\n",
      "   macro avg       0.60      0.70      0.63     21736\n",
      "weighted avg       0.77      0.73      0.74     21736\n",
      "\n",
      "--------------------------------------------------\n",
      "\n",
      "Testing Model: LSTM_TFIDF.h5 ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.7132\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.83      0.87      0.85      1117\n",
      "           1       0.67      0.77      0.72      1439\n",
      "           2       0.86      0.67      0.76      5578\n",
      "           3       0.22      0.42      0.29        86\n",
      "           4       0.31      0.72      0.44       599\n",
      "           5       0.52      0.71      0.60       160\n",
      "           6       0.70      0.78      0.74       160\n",
      "           7       0.78      0.85      0.81       160\n",
      "           8       0.61      0.67      0.64       160\n",
      "           9       0.59      0.69      0.64       160\n",
      "          10       0.89      0.80      0.85      4637\n",
      "          11       0.17      0.47      0.25       160\n",
      "          12       0.50      0.73      0.59       700\n",
      "          13       0.58      0.71      0.64       160\n",
      "          14       0.60      0.70      0.65       160\n",
      "          15       0.76      0.50      0.61      3327\n",
      "          16       0.66      0.81      0.73      1252\n",
      "          17       0.85      0.88      0.87       921\n",
      "          18       0.41      0.62      0.49       160\n",
      "          19       0.42      0.72      0.53       640\n",
      "\n",
      "    accuracy                           0.71     21736\n",
      "   macro avg       0.60      0.71      0.63     21736\n",
      "weighted avg       0.76      0.71      0.72     21736\n",
      "\n",
      "--------------------------------------------------\n",
      "\n",
      "Testing Model: RNN_TFIDF.h5 ...\n",
      "Accuracy: 0.7107\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.82      0.86      0.84      1117\n",
      "           1       0.68      0.78      0.72      1439\n",
      "           2       0.88      0.66      0.76      5578\n",
      "           3       0.25      0.35      0.29        86\n",
      "           4       0.30      0.75      0.43       599\n",
      "           5       0.50      0.69      0.58       160\n",
      "           6       0.66      0.79      0.72       160\n",
      "           7       0.80      0.86      0.83       160\n",
      "           8       0.54      0.66      0.59       160\n",
      "           9       0.52      0.71      0.60       160\n",
      "          10       0.89      0.79      0.84      4637\n",
      "          11       0.15      0.43      0.22       160\n",
      "          12       0.48      0.71      0.57       700\n",
      "          13       0.57      0.72      0.63       160\n",
      "          14       0.53      0.71      0.60       160\n",
      "          15       0.76      0.52      0.62      3327\n",
      "          16       0.67      0.81      0.73      1252\n",
      "          17       0.83      0.89      0.86       921\n",
      "          18       0.38      0.61      0.47       160\n",
      "          19       0.43      0.72      0.54       640\n",
      "\n",
      "    accuracy                           0.71     21736\n",
      "   macro avg       0.58      0.70      0.62     21736\n",
      "weighted avg       0.77      0.71      0.72     21736\n",
      "\n",
      "--------------------------------------------------\n",
      "\n",
      "Testing Selesai.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import joblib\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tensorflow.keras.models import load_model\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "\n",
    "FOLDER_MODEL = \"models_baru_tf_idf\"\n",
    "TARGET_TEXT = df_test['final_text'] \n",
    "TARGET_LABEL = y_test\n",
    "\n",
    "try:\n",
    "    target_names = dataset_train_final.features['label'].names\n",
    "except:\n",
    "    target_names = [str(i) for i in range(len(np.unique(TARGET_LABEL)))]\n",
    "\n",
    "print(f\"Loading preprocessing tools from {FOLDER_MODEL}...\")\n",
    "\n",
    "try:\n",
    "    tfidf_vectorizer = joblib.load(os.path.join(FOLDER_MODEL, \"tfidf_vectorizer.pkl\"))\n",
    "    svd_model = joblib.load(os.path.join(FOLDER_MODEL, \"svd_model.pkl\"))\n",
    "    scaler = joblib.load(os.path.join(FOLDER_MODEL, \"tfidf_svd_scaler.pkl\"))\n",
    "    print(\"Tools (TFIDF, SVD, Scaler) loaded successfully.\")\n",
    "except FileNotFoundError as e:\n",
    "    print(f\"Error: {e}\")\n",
    "    print(\"Pastikan file .pkl tools ada di folder models_baru_tf_idf\")\n",
    "    exit()\n",
    "\n",
    "print(\"\\nTransforming test data...\")\n",
    "\n",
    "X_test_tfidf = tfidf_vectorizer.transform(TARGET_TEXT)\n",
    "X_test_svd = svd_model.transform(X_test_tfidf)\n",
    "X_test_svd_scaled = scaler.transform(X_test_svd)\n",
    "\n",
    "print(f\"Shape Test Data: {X_test_svd.shape}\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "print(\"Testing saved models...\")\n",
    "\n",
    "for model_file in os.listdir(FOLDER_MODEL):\n",
    "    model_path = os.path.join(FOLDER_MODEL, model_file)\n",
    "    \n",
    "    if \"scaler\" in model_file or \"vectorizer\" in model_file or \"svd\" in model_file:\n",
    "        continue\n",
    "\n",
    "    print(f\"\\nTesting Model: {model_file} ...\")\n",
    "    \n",
    "    try:\n",
    "        y_pred = None\n",
    "        \n",
    "        if model_file.endswith(\".h5\"):\n",
    "            loaded_model = load_model(model_path)\n",
    "            input_data = X_test_svd \n",
    "            y_pred_prob = loaded_model.predict(input_data, verbose=0)\n",
    "            y_pred = np.argmax(y_pred_prob, axis=1)\n",
    "\n",
    "        elif model_file.endswith(\".pkl\"):\n",
    "            loaded_model = joblib.load(model_path)\n",
    "            y_pred = loaded_model.predict(X_test_svd_scaled)\n",
    "        \n",
    "        else:\n",
    "            continue\n",
    "\n",
    "        if y_pred is not None:\n",
    "            acc = accuracy_score(TARGET_LABEL, y_pred)\n",
    "            report = classification_report(\n",
    "                TARGET_LABEL, \n",
    "                y_pred, \n",
    "                labels=range(len(target_names)), \n",
    "                target_names=target_names, \n",
    "                zero_division=0\n",
    "            )\n",
    "            \n",
    "            print(f\"Accuracy: {acc:.4f}\")\n",
    "            print(report)\n",
    "            print(\"-\" * 50)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Gagal load/test {model_file}: {e}\")\n",
    "        if model_file.endswith(\".h5\"):\n",
    "            print(f\"   Expected Input Shape: {loaded_model.input_shape}\")\n",
    "            print(f\"   Actual Data Shape:    {input_data.shape}\")\n",
    "\n",
    "print(\"\\nTesting Selesai.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "489dc4cc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
