{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9599fd48",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Folder database ditemukan di './chroma_download\\chroma_db'.\n",
      "   Melewati proses download dan menggunakan data lokal.\n",
      "\n",
      "Memulai tes query...\n",
      "Mencari di Vector DB...\n",
      "\n",
      "Query: 'tiongkok debt trap for developing countries'\n",
      "\n",
      "SCORE      | JUDUL PAPER\n",
      "--------------------------------------------------------------------------------\n",
      "1.6227     | Physics of eta-prime with rooted staggered quarks | 7 | The quark-mass dependence of the eta in the Schwinger model, which -- like the eta-prime in QCD -- becomes massive through the axial anomaly, is studied on the lattice with N_f=0,1,2. Staggered quarks are used, with a rooted determinant for N_f=1. In the chiral limit the Schwinger mass is reproduced, which suggests that the anomaly is being treated correctly.\n",
      "Abstract   : quark mass dependence eta schwinger like eta prime qcd becomes massive axial anomaly studied lattice staggered quark used rooted determinant chiral limit schwinger mass reproduced suggests anomaly tre...\n",
      "--------------------------------------------------------------------------------\n",
      "1.6373     | Confining potential of Y-string on the lattice at finite T | 7 | The potential due to a system of three static quark (3Q) is studied using SU(3) lattice QCD at finite temperature with Polyakov loops operators. We focused our analysis on the large distance properties of the 3Q potential and found a good fit behavior to the Y-string model formula. In addition to the linearly confining term proportional to the minimal length of the Y-string, we observed that the subleading logarithmic term, which is proportional to Dedekind eta function and accounts for the Y-string's quantum fluctuations, is necessary to reproduce the quark anti-quark string tension of the corresponding mesonic system at finite temperature.\n",
      "Abstract   : potential due system three static quark studied lattice qcd finite temperature polyakov loop operator focused large distance property potential found good fit behavior string formula addition linearly...\n",
      "--------------------------------------------------------------------------------\n",
      "1.6560     | Hunting the static energy renormalon | 7 | We employ Numerical Stochastic Perturbation Theory (NSPT) together with twisted boundary conditions (TBC) to search for the leading renormalon in the perturbative expansion of the static energy. This renormalon is expected to emerge four times faster than the one for the gluon conden- sate in the plaquette. We extract the static energy from Polyakov loop calculations up to 12 loops and present preliminary results, indicating a significant step towards confirming the theoretical expectation.\n",
      "Abstract   : employ numerical stochastic perturbation theory nspt together twisted boundary condition tbc search leading renormalon perturbative expansion static energy renormalon expected emerge four time faster ...\n",
      "--------------------------------------------------------------------------------\n",
      "1.6674     | Confinement in large N gauge theories | 7 | We report some recent results obtained for large N gauge theories which support the idea of volume reduction. Results for the string tension of the Twisted Eguchi-Kawai model match with those obtained from extrapolation from finite N. Determination of other observables is currently under way. Application of the twisted reduction idea to the case of 1 or 2 flavours of quarks in the adjoint representation of the group offers promising results. Preliminary results for the string tension point towards a very different behaviour for 1 and 2 flavours. While the string tension remains finite for 1 flavour at the critical massless quark limit, it seems to vanish with a large anomalous dimension for the $N_f=2$ case. This is consistent with the predicted Infrared fixed point expected in the latter case.\n",
      "Abstract   : report recent obtained large gauge theory support idea volume reduction string tension twisted eguchi kawai match obtained extrapolation finite determination observables currently way application twis...\n",
      "--------------------------------------------------------------------------------\n",
      "1.6821     | Improved convergence of Complex Langevin simulations | 7 | The sign problem appears in lattice QCD as soon as a non-zero chemical potential is introduced. This prevents direct simulations to determine the phase structure of the strongly interacting matter. Complex Langevin methods have been successfully used for various models or approximations of QCD. However, in some scenarios it converges to incorrect results. We present developments of our new method that helps to improve the convergence by keeping the system closer to the SU(3) manifold and discuss preliminary tests and results.\n",
      "Abstract   : sign problem appears lattice qcd soon non zero chemical potential introduced prevents direct simulation determine phase structure strongly interacting matter complex langevin method successfully used ...\n",
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import shutil\n",
    "from huggingface_hub import snapshot_download\n",
    "from langchain_chroma import Chroma\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "\n",
    "REPO_ID = \"iskandarmrp/nlp-papermatch-dataset\"\n",
    "REPO_TYPE = \"dataset\"\n",
    "TEST_DIR = \"./chroma_download\"\n",
    "\n",
    "def test_chroma_from_hf():\n",
    "    chroma_path = os.path.join(TEST_DIR, \"chroma_db\")\n",
    "    \n",
    "    if os.path.exists(chroma_path) and len(os.listdir(chroma_path)) > 0:\n",
    "        print(f\"Folder database ditemukan di '{chroma_path}'.\")\n",
    "        print(\"   Melewati proses download dan menggunakan data lokal.\\n\")\n",
    "    else:\n",
    "        print(f\"Folder tidak ditemukan. Sedang mendownload 'chroma_db' dari {REPO_ID}...\")\n",
    "        try:\n",
    "            snapshot_download(\n",
    "                repo_id=REPO_ID,\n",
    "                repo_type=REPO_TYPE,\n",
    "                local_dir=TEST_DIR,\n",
    "                allow_patterns=\"chroma_db/*\",\n",
    "                local_dir_use_symlinks=False\n",
    "            )\n",
    "            print(\"Download selesai!\\n\")\n",
    "        except Exception as e:\n",
    "            print(f\"Gagal download: {e}\")\n",
    "            return\n",
    "\n",
    "    print(\"Memulai tes query...\")\n",
    "\n",
    "    if not os.path.exists(chroma_path):\n",
    "        print(f\"Error: Folder {chroma_path} tetap tidak ditemukan.\")\n",
    "        return\n",
    "\n",
    "    EMBEDDING_MODEL = \"sentence-transformers/all-MiniLM-L6-v2\"\n",
    "    embedding_function = HuggingFaceEmbeddings(\n",
    "        model_name=EMBEDDING_MODEL,\n",
    "        encode_kwargs={\"normalize_embeddings\": True} \n",
    "    )\n",
    "\n",
    "    vector_db = Chroma(\n",
    "        persist_directory=chroma_path,\n",
    "        embedding_function=embedding_function,\n",
    "        collection_name=\"paper_abstracts\"\n",
    "    )\n",
    "\n",
    "    my_query = \"tiongkok debt trap for developing countries\"\n",
    "    top_k = 5\n",
    "\n",
    "    print(f\"Mencari di Vector DB...\")\n",
    "    results = vector_db.similarity_search_with_score(my_query, k=top_k, filter={\"label\": 7})\n",
    "\n",
    "    print(f\"\\nQuery: '{my_query}'\\n\")\n",
    "    print(f\"{'SCORE':<10} | {'JUDUL PAPER'}\")\n",
    "    print(\"-\" * 80)\n",
    "\n",
    "    for doc, score in results:\n",
    "        title = doc.metadata.get(\"title\", \"No Title\")\n",
    "        label = doc.metadata.get(\"label\", \"-\")\n",
    "        raw_abstract = doc.metadata.get(\"raw_abstract\", \"-\")\n",
    "        \n",
    "        print(f\"{score:.4f}     | {title} | {label} | {raw_abstract}\")\n",
    "        print(f\"Abstract   : {doc.page_content[:200]}...\") \n",
    "        print(\"-\" * 80)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    test_chroma_from_hf()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "69fbd3bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sedang meload model dari Hugging Face: iskandarmrp/distilbert-lora-paper-topic-classification...\n",
      "Label mapping ditemukan (20 kelas).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model sukses dimuat di cpu!\n",
      "\n",
      "==================================================\n",
      "TEST QUERY:\n",
      "This study investigates the impact of fiscal policy on economic growth in developing nations \n",
      "    during periods of high inflation. Using a panel data approach covering 50 countries from 2000 to 2020, \n",
      "    we analyze the relationship between government spending, tax revenues, and GDP per capita. \n",
      "    Our results indicate that while fiscal stimulus can boost short-term growth, excessive public debt \n",
      "    negatively correlates with long-term economic stability.\n",
      "==================================================\n",
      "\n",
      "HASIL PREDIKSI:\n",
      "Label ID   : 3\n",
      "Kategori   : econ\n",
      "Confidence : 0.4891 (48.91%)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import json\n",
    "import os\n",
    "from peft import PeftModel, PeftConfig\n",
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer\n",
    "import torch.nn.functional as F\n",
    "\n",
    "MODEL_ID = \"iskandarmrp/distilbert-lora-paper-topic-classification\"\n",
    "\n",
    "LABEL_MAPPING_FILE = \"label_mapping.json\"\n",
    "\n",
    "def test_inference():\n",
    "    print(f\"Sedang meload model dari Hugging Face: {MODEL_ID}...\")\n",
    "    \n",
    "    if os.path.exists(LABEL_MAPPING_FILE):\n",
    "        with open(LABEL_MAPPING_FILE, \"r\") as f:\n",
    "            label_mapping = json.load(f)\n",
    "        id2label = {v: k for k, v in label_mapping.items()}\n",
    "        num_labels = len(label_mapping)\n",
    "        print(f\"Label mapping ditemukan ({num_labels} kelas).\")\n",
    "    else:\n",
    "        print(\"Warning: 'label_mapping.json' tidak ditemukan. Hasil akan berupa angka.\")\n",
    "        id2label = {}\n",
    "        num_labels = 20\n",
    "\n",
    "    peft_config = PeftConfig.from_pretrained(MODEL_ID)\n",
    "\n",
    "    tokenizer = AutoTokenizer.from_pretrained(peft_config.base_model_name_or_path)\n",
    "\n",
    "    base_model = AutoModelForSequenceClassification.from_pretrained(\n",
    "        peft_config.base_model_name_or_path,\n",
    "        num_labels=num_labels,\n",
    "        ignore_mismatched_sizes=True \n",
    "    )\n",
    "\n",
    "    model = PeftModel.from_pretrained(base_model, MODEL_ID)\n",
    "    \n",
    "    # device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    device = \"cpu\"\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "    print(f\"Model sukses dimuat di {device}!\")\n",
    "\n",
    "    test_text = \"\"\"\n",
    "    This study investigates the impact of fiscal policy on economic growth in developing nations \n",
    "    during periods of high inflation. Using a panel data approach covering 50 countries from 2000 to 2020, \n",
    "    we analyze the relationship between government spending, tax revenues, and GDP per capita. \n",
    "    Our results indicate that while fiscal stimulus can boost short-term growth, excessive public debt \n",
    "    negatively correlates with long-term economic stability.\n",
    "    \"\"\"\n",
    "\n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(\"TEST QUERY:\")\n",
    "    print(test_text.strip())\n",
    "    print(\"=\"*50)\n",
    "\n",
    "    inputs = tokenizer(\n",
    "        test_text, \n",
    "        return_tensors=\"pt\", \n",
    "        truncation=True, \n",
    "        max_length=256,\n",
    "        padding=\"max_length\"\n",
    "    ).to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "        logits = outputs.logits\n",
    "        probabilities = F.softmax(logits, dim=1)\n",
    "\n",
    "        predicted_class_id = torch.argmax(probabilities, dim=1).item()\n",
    "        confidence = probabilities[0][predicted_class_id].item()\n",
    "\n",
    "    predicted_label = id2label.get(predicted_class_id, str(predicted_class_id))\n",
    "    \n",
    "    print(f\"\\nHASIL PREDIKSI:\")\n",
    "    print(f\"Label ID   : {predicted_class_id}\")\n",
    "    print(f\"Kategori   : {predicted_label}\")\n",
    "    print(f\"Confidence : {confidence:.4f} ({confidence*100:.2f}%)\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    test_inference()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd49fcf6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Iskandar\\Documents\\GitHub\\Tubes-NLP-PaperMatch\\venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ GPU Terdeteksi: NVIDIA GeForce RTX 3060 Laptop GPU\n",
      "   VRAM Tersedia: 6.00 GB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Iskandar\\Documents\\GitHub\\Tubes-NLP-PaperMatch\\venv\\Lib\\site-packages\\huggingface_hub\\file_download.py:942: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Iskandar\\Documents\\GitHub\\Tubes-NLP-PaperMatch\\venv\\Lib\\site-packages\\huggingface_hub\\file_download.py:979: UserWarning: `local_dir_use_symlinks` parameter is deprecated and will be ignored. The process to download files to a local folder has been updated and do not rely on symlinks anymore. You only need to pass a destination folder as`local_dir`.\n",
      "For more details, check out https://huggingface.co/docs/huggingface_hub/main/en/guides/download#download-files-to-local-folder.\n",
      "  warnings.warn(\n",
      "Fetching 9 files: 100%|██████████| 9/9 [00:00<00:00, 1381.78it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Memuat Model ke GPU (Full VRAM) ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Iskandar\\Documents\\GitHub\\Tubes-NLP-PaperMatch\\venv\\Lib\\site-packages\\transformers\\quantizers\\auto.py:239: UserWarning: You passed `quantization_config` or equivalent parameters to `from_pretrained` but the model you're loading already has a `quantization_config` attribute. The `quantization_config` from the model will be used.\n",
      "  warnings.warn(warning_msg)\n",
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:08<00:00,  4.24s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Model berhasil masuk ke VRAM GPU!\n",
      "\n",
      "=== RUNNING ON GPU ===\n",
      "\n",
      "Generating (Cepat)...\n",
      "The work presented in @cite_13 is one such example where they have used Twitter data collected over 3 months period using hashtags related to #BlackLivesMatter movement. Their findings show that there exists a strong correlation between racial slurs and other forms of offensive words like swearwords, insults etc. This shows how important it is to consider context while classifying tweets into different categories. In addition, they also found that most of the users who use racist terms do so repeatedly which makes them easy targets for detection algorithms. Another interesting finding was that people tend to be more tolerant towards racism when compared to sexism. It can be attributed to the fact that women face discrimination at all levels whereas men don't suffer much because of their gender. Hence, we need to take care about what kind of training examples our models see during development phase otherwise they might end up being biased against certain groups. Finally, they conclude saying that although machine learning methods perform well but still human intervention plays crucial role especially when dealing with sensitive issues like hate speech. Erkut et al., proposed a framework called \"Hate Speech Detection Framework\" (@cite_15) which uses supervised learning approach alongwith rule-based filtering technique to identify hateful comments posted online. The main idea behind this method lies in identifying patterns present within each comment rather than relying solely upon keywords matching strategy employed earlier days. This helps us avoid false positives caused due to misspellings made intentionally or unintentionally by spammers trying hard enough to fool detectors designed\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
    "from huggingface_hub import snapshot_download\n",
    "import os\n",
    "import shutil\n",
    "\n",
    "MODEL_ID = \"Alan43/related_works_generation_model\"\n",
    "LOCAL_DIR = \"./model_local_final\"\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "if not torch.cuda.is_available():\n",
    "    print(\"ERROR: Tidak ada GPU NVIDIA yang terdeteksi! Script ini butuh GPU.\")\n",
    "    exit()\n",
    "\n",
    "print(f\"GPU Terdeteksi: {torch.cuda.get_device_name(0)}\")\n",
    "print(f\"   VRAM Tersedia: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.2f} GB\")\n",
    "\n",
    "try:\n",
    "    snapshot_download(repo_id=MODEL_ID, local_dir=LOCAL_DIR, local_dir_use_symlinks=False, resume_download=True)\n",
    "except:\n",
    "    pass\n",
    "\n",
    "print(\"\\n--- Memuat Model ke GPU (Full VRAM) ---\")\n",
    "\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.float16,\n",
    "    llm_int8_enable_fp32_cpu_offload=False\n",
    ")\n",
    "\n",
    "try:\n",
    "    tokenizer = AutoTokenizer.from_pretrained(LOCAL_DIR)\n",
    "    if tokenizer.pad_token_id is None:\n",
    "        tokenizer.pad_token_id = tokenizer.eos_token_id\n",
    "\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        LOCAL_DIR, \n",
    "        quantization_config=bnb_config,\n",
    "        device_map=\"cuda:0\",\n",
    "        trust_remote_code=True,\n",
    "        local_files_only=True\n",
    "    )\n",
    "    \n",
    "    print(\"Model berhasil masuk ke VRAM GPU!\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"\\nGAGAL LOAD (Kemungkinan VRAM Kurang): {e}\")\n",
    "    print(\"Solusi: Gunakan script sebelumnya dengan CPU Offload.\")\n",
    "    exit()\n",
    "\n",
    "def generate_related_work(input_text):\n",
    "    system_msg = \"You are an academic writing assistant. Write a 'Related Work' section based on the provided text. The input contains the Current Abstract followed by References (marked with @cite_n). Synthesize these references and highlight the novelty of the Current Abstract.\"\n",
    "    \n",
    "    prompt = f\"\"\"<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
    "\n",
    "{system_msg}<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
    "\n",
    "{input_text}<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
    "\"\"\"\n",
    "    \n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(\"cuda\")\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=300,\n",
    "            do_sample=True,\n",
    "            temperature=0.1,\n",
    "            top_p=0.9,\n",
    "            repetition_penalty=1.2,\n",
    "            pad_token_id=tokenizer.eos_token_id\n",
    "        )\n",
    "    \n",
    "    return tokenizer.decode(outputs[0][inputs.input_ids.shape[1]:], skip_special_tokens=True).strip()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"\\n=== RUNNING ON GPU ===\")\n",
    "    while True:\n",
    "        text = input(\"\\nMasukkan Text:\\n\")\n",
    "        if text.lower() in ['exit', 'quit']: break\n",
    "        if not text.strip(): continue\n",
    "        \n",
    "        try:\n",
    "            print(\"\\nGenerating (Cepat)...\")\n",
    "            print(generate_related_work(text))\n",
    "        except Exception as e:\n",
    "            print(f\"Error: {e}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
